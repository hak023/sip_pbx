"""
Text Embedder

í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
"""

import os
import ssl
import warnings

# ===== 1ë‹¨ê³„: í™˜ê²½ë³€ìˆ˜ ë¨¼ì € ì„¤ì • (ê°€ì¥ ë¨¼ì €!) =====
os.environ['CURL_CA_BUNDLE'] = ''
os.environ['REQUESTS_CA_BUNDLE'] = ''
os.environ['SSL_CERT_FILE'] = ''
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['PYTHONHTTPSVERIFY'] = '0'  # Python HTTPS ê²€ì¦ ë¹„í™œì„±í™”

# HuggingFace/Transformers ì§„í–‰ í‘œì‹œì¤„ ë° ë¡œê·¸ ë¹„í™œì„±í™”
os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

# ===== 2ë‹¨ê³„: SSL ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ë³€ê²½ =====
ssl._create_default_https_context = ssl._create_unverified_context

# ===== 3ë‹¨ê³„: urllib3 ê²½ê³  ë¹„í™œì„±í™” =====
try:
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except ImportError:
    pass

# ===== 4ë‹¨ê³„: requests ì„¸ì…˜ì—ë„ SSL ê²€ì¦ ë¹„í™œì„±í™” =====
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    
    # requests ê¸°ë³¸ ì„¸ì…˜ SSL ê²€ì¦ ë¹„í™œì„±í™”
    original_request = requests.Session.request
    def patched_request(self, *args, **kwargs):
        kwargs.setdefault('verify', False)
        return original_request(self, *args, **kwargs)
    requests.Session.request = patched_request
except ImportError:
    pass

# ===== 5ë‹¨ê³„: ë¡œê¹… ì„¤ì • =====
import logging
logging.getLogger("sentence_transformers").setLevel(logging.ERROR)
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)
logging.getLogger("requests").setLevel(logging.ERROR)

# ===== ì´ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import =====
from sentence_transformers import SentenceTransformer
import asyncio
from typing import List, Union
import structlog

logger = structlog.get_logger(__name__)


class TextEmbedder:
    """
    í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±ê¸°
    
    Sentence Transformersë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    GPUê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        model_name: str = "paraphrase-multilingual-mpnet-base-v2",
        dimension: int = 768,
        batch_size: int = 32,
        device: str = None  # 'cuda', 'cpu', or None (auto)
    ):
        """
        Args:
            model_name: Sentence Transformers ëª¨ë¸ ì´ë¦„
            dimension: ì„ë² ë”© ì°¨ì›
            batch_size: ë°°ì¹˜ í¬ê¸°
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=auto)
        """
        self.model_name = model_name
        self.dimension = dimension
        self.batch_size = batch_size
        
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        if device is None:
            import torch
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            if self.device == 'cuda':
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info("ğŸ® [GPU] CUDA available! Using GPU acceleration",
                           device=self.device,
                           gpu_name=gpu_name,
                           gpu_memory_gb=f"{gpu_memory:.2f}GB")
            else:
                logger.info("ğŸ’» [CPU] CUDA not available, using CPU",
                           device=self.device)
        else:
            self.device = device
            logger.info(f"Device specified: {device}")
        
        # ëª¨ë¸ ë¡œë“œ
        logger.info("Loading embedding model", model=model_name, device=self.device)
        
        # HuggingFace Hub SSL ê²€ì¦ ë¹„í™œì„±í™”
        try:
            logger.info("ğŸ”§ [DEBUG] Disabling HuggingFace Hub transfer...")
            from huggingface_hub import constants
            constants.HF_HUB_ENABLE_HF_TRANSFER = False
            logger.info("âœ… [DEBUG] HuggingFace Hub transfer disabled")
        except Exception as e:
            logger.warning("âš ï¸ [DEBUG] Failed to disable HF transfer", error=str(e))
        
        # ëª¨ë¸ ë¡œë“œ ì‹œì‘ (SSL ê²€ì¦ ë¹„í™œì„±í™” ì ìš©ë¨)
        logger.info("ğŸ”„ [DEBUG] Starting SentenceTransformer initialization...")
        logger.info("ğŸ”„ [DEBUG] Model name: {}, Device: {}, Auth: False".format(model_name, self.device))
        
        import time
        start_time = time.time()
        
        try:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì • (5ë¶„)
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("Model loading timed out after 5 minutes")
            
            # Windowsì—ì„œëŠ” signal.SIGALRMì´ ì—†ìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš©
            logger.info("ğŸ”„ [DEBUG] Calling SentenceTransformer()...")
            # âœ… use_auth_token deprecated, ì œê±° (v3ì—ì„œ ì‚­ì œ ì˜ˆì •)
            self.model = SentenceTransformer(model_name, device=self.device)
            
            elapsed = time.time() - start_time
            logger.info("âœ… [DEBUG] SentenceTransformer loaded successfully in {:.2f} seconds".format(elapsed))
            
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error("âŒ [DEBUG] Model loading failed after {:.2f} seconds".format(elapsed), 
                        error=str(e), 
                        error_type=type(e).__name__)
            raise
        
        # GPU ì •ë³´ ì¶œë ¥
        if self.device == 'cuda':
            import torch
            logger.info("âœ… [GPU] Model loaded on GPU",
                       model=model_name,
                       device=self.device,
                       cuda_device=torch.cuda.current_device())
        
        # í†µê³„
        self.total_embeddings = 0
        self.total_texts = 0
        
        logger.info("TextEmbedder initialized", 
                   model=model_name,
                   dimension=dimension,
                   device=self.device)
    
    async def embed(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        try:
            # CPU ë°”ìš´ë“œ ì‘ì—…ì´ë¯€ë¡œ executorì—ì„œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(
                None,
                lambda: self.model.encode(text, convert_to_numpy=True)
            )
            
            self.total_embeddings += 1
            self.total_texts += len(text)
            
            return embedding.tolist()
            
        except Exception as e:
            logger.error("Embedding failed", text_length=len(text), error=str(e))
            # ì˜¤ë¥˜ ì‹œ ì œë¡œ ë²¡í„° ë°˜í™˜
            return [0.0] * self.dimension
    
    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """
        ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë°°ì¹˜ ì²˜ë¦¬
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(
                None,
                lambda: self.model.encode(
                    texts, 
                    batch_size=self.batch_size,
                    convert_to_numpy=True
                )
            )
            
            self.total_embeddings += len(texts)
            self.total_texts += sum(len(t) for t in texts)
            
            return [emb.tolist() for emb in embeddings]
            
        except Exception as e:
            logger.error("Batch embedding failed", 
                        batch_size=len(texts), 
                        error=str(e))
            # ì˜¤ë¥˜ ì‹œ ì œë¡œ ë²¡í„° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return [[0.0] * self.dimension for _ in texts]
    
    def embed_sync(self, text: str) -> List[float]:
        """
        ë™ê¸° ì„ë² ë”© (í•„ìš”í•œ ê²½ìš°)
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        try:
            embedding = self.model.encode(text, convert_to_numpy=True)
            self.total_embeddings += 1
            self.total_texts += len(text)
            return embedding.tolist()
        except Exception as e:
            logger.error("Sync embedding failed", error=str(e))
            return [0.0] * self.dimension
    
    def get_stats(self) -> dict:
        """ì„ë² ë”© í†µê³„ ë°˜í™˜"""
        stats = {
            "total_embeddings": self.total_embeddings,
            "total_texts": self.total_texts,
            "model_name": self.model_name,
            "dimension": self.dimension,
            "device": self.device,
            "avg_text_length": (
                self.total_texts / self.total_embeddings 
                if self.total_embeddings > 0 else 0
            ),
        }
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ê°€
        if self.device == 'cuda':
            try:
                import torch
                stats.update({
                    "gpu_name": torch.cuda.get_device_name(0),
                    "gpu_memory_allocated_mb": torch.cuda.memory_allocated(0) / 1024**2,
                    "gpu_memory_reserved_mb": torch.cuda.memory_reserved(0) / 1024**2,
                    "gpu_memory_total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3,
                })
            except Exception as e:
                logger.error("Failed to get GPU stats", error=str(e))
        
        return stats


class SimpleEmbedder:
    """
    ê°„ë‹¨í•œ ì„ë² ë” (í…ŒìŠ¤íŠ¸ìš©)
    
    ì‹¤ì œ ëª¨ë¸ ì—†ì´ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, dimension: int = 768):
        """
        Args:
            dimension: ì„ë² ë”© ì°¨ì›
        """
        self.dimension = dimension
        logger.info("SimpleEmbedder initialized", dimension=dimension)
    
    async def embed(self, text: str) -> List[float]:
        """
        í•´ì‹œ ê¸°ë°˜ ì„ë² ë”©
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        import hashlib
        
        # í…ìŠ¤íŠ¸ í•´ì‹œ
        hash_obj = hashlib.sha256(text.encode())
        hash_bytes = hash_obj.digest()
        
        # í•´ì‹œë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        embedding = []
        for i in range(0, min(len(hash_bytes), self.dimension), 2):
            if i + 1 < len(hash_bytes):
                value = (hash_bytes[i] * 256 + hash_bytes[i + 1]) / 65535.0
            else:
                value = hash_bytes[i] / 255.0
            embedding.append(value)
        
        # ì°¨ì› ë§ì¶”ê¸°
        while len(embedding) < self.dimension:
            embedding.append(0.0)
        
        return embedding[:self.dimension]
    
    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """ë°°ì¹˜ ì„ë² ë”©"""
        return [await self.embed(t) for t in texts]
    
    def get_stats(self) -> dict:
        """í†µê³„ ë°˜í™˜"""
        return {
            "embedder_type": "simple",
            "dimension": self.dimension,
        }

