"""
Knowledge Extractor

í†µí™” ë…¹ìŒì—ì„œ ì§€ì‹ì •ë³´ë¥¼ ì •ì œ(ì¶”ì¶œÂ·ë¶„ë¥˜)í•˜ì—¬ Vector DBì— ì €ì¥.
Â§7 PII íŒŒì´í”„ë¼ì¸: contains_piiì¸ í•­ëª©ì€ ê²€í†  ëŒ€ê¸°ì—´ì—ë§Œ ì ì¬(ì„ íƒ).
"""

from typing import List, Dict, Optional
import asyncio
from pathlib import Path
import json
import structlog

logger = structlog.get_logger(__name__)


class KnowledgeExtractor:
    """
    í†µí™” ë…¹ìŒì—ì„œ ì§€ì‹ì •ë³´ë¥¼ ì •ì œ(ì¶”ì¶œÂ·ë¶„ë¥˜)í•˜ì—¬ Vector DBì— ì €ì¥.

    ì›Œí¬í”Œë¡œìš°:
    1. ë…¹ìŒ íŒŒì¼ ë¡œë“œ
    2. ì „ì‚¬ í…ìŠ¤íŠ¸ ë¡œë“œ
    3. LLM ì§€ì‹ ì •ì œ (í†µí™”ì—ì„œ ì €ì¥í•  ì§€ì‹ ë‹¨ìœ„ ì¶”ì¶œÂ·ë¶„ë¥˜)
    4. í…ìŠ¤íŠ¸ ì²­í‚¹
    5. ì„ë² ë”© ìƒì„±
    6. Vector DB ì €ì¥ (contains_piiì´ê³  pii_review_queue_enabledì´ë©´ ê²€í†  ëŒ€ê¸°ì—´ì—ë§Œ ì €ì¥)
    """
    
    def __init__(
        self,
        llm_client,      # LLMClient ì¸ìŠ¤í„´ìŠ¤
        embedder,        # TextEmbedder ì¸ìŠ¤í„´ìŠ¤
        vector_db,       # VectorDB ì¸ìŠ¤í„´ìŠ¤
        min_confidence: float = 0.7,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        min_text_length: int = 10,  # âœ… 50 â†’ 10 (ì§§ì€ ëŒ€í™”ë„ ì €ì¥)
        pii_review_queue_enabled: bool = False,
        extraction_pending_file: Optional[str] = None,
    ):
        """
        Args:
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸
            embedder: í…ìŠ¤íŠ¸ ì„ë² ë”
            vector_db: Vector DB í´ë¼ì´ì–¸íŠ¸
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ (ìœ ìš©ì„± íŒë‹¨)
            chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì)
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë© (ë¬¸ì)
            min_text_length: ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´
        """
        self.llm = llm_client
        self.embedder = embedder
        self.vector_db = vector_db
        self.min_confidence = min_confidence
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_text_length = min_text_length
        self.pii_review_queue_enabled = pii_review_queue_enabled
        self._pending_store = None
        if pii_review_queue_enabled:
            from src.services.extraction_review_store import get_extraction_review_store
            self._pending_store = get_extraction_review_store(extraction_pending_file)
        
        # í†µê³„
        self.total_extractions = 0
        self.total_chunks_stored = 0
        
        logger.info("KnowledgeExtractor initialized",
                   min_confidence=min_confidence,
                   chunk_size=chunk_size,
                   pii_review_queue_enabled=pii_review_queue_enabled)
    
    async def extract_from_call(
        self, 
        call_id: str,
        transcript_path: str,
        owner_id: str,
        speaker: str = "callee"
    ) -> Dict:
        """
        í†µí™”ì—ì„œ ì§€ì‹ ì¶”ì¶œ
        
        Args:
            call_id: í†µí™” ID
            transcript_path: ì „ì‚¬ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            owner_id: ì†Œìœ ì ID (ì°©ì‹ ì ID)
            speaker: ì¶”ì¶œ ëŒ€ìƒ í™”ì (caller/callee)
            
        Returns:
            {
                "success": bool,
                "extracted_count": int,
                "confidence": float
            }
        """
        try:
            logger.info("ğŸ”„ [VectorDB Flow] Step 1/6: Knowledge extraction started",
                       call_id=call_id,
                       owner_id=owner_id,
                       speaker=speaker,
                       transcript_path=transcript_path)
            
            # 1. ì „ì‚¬ í…ìŠ¤íŠ¸ ë¡œë“œ
            logger.info("ğŸ”„ [VectorDB Flow] Step 2/6: Loading transcript", 
                       call_id=call_id,
                       path=transcript_path)
            
            transcript = await self._load_transcript(transcript_path)
            if not transcript:
                logger.warning("âŒ [VectorDB Flow] Empty transcript - Aborting", call_id=call_id)
                return {"success": False, "extracted_count": 0, "confidence": 0.0}
            
            logger.info("âœ… [VectorDB Flow] Transcript loaded", 
                       call_id=call_id,
                       transcript_length=len(transcript),
                       preview=transcript[:100] + "..." if len(transcript) > 100 else transcript)
            
            # 2. í™”ì í•„í„°ë§ (ë˜ëŠ” ì „ì²´ ëŒ€í™” ì‚¬ìš©)
            logger.info("ğŸ”„ [VectorDB Flow] Step 3/6: Filtering by speaker",
                       call_id=call_id,
                       target_speaker=speaker)
            
            if speaker == "both" or speaker == "all":
                # ë°œì‹ ì+ì°©ì‹ ì ëª¨ë‘ ì‚¬ìš©
                speaker_text = transcript
                logger.info("âœ… [VectorDB Flow] Using full conversation (both speakers)",
                           call_id=call_id,
                           text_length=len(speaker_text))
            else:
                # íŠ¹ì • í™”ìë§Œ í•„í„°ë§
                speaker_text = self._filter_by_speaker(transcript, speaker)
            if not speaker_text or len(speaker_text) < self.min_text_length:
                logger.warning("âŒ [VectorDB Flow] Insufficient text from target speaker - Aborting", 
                          call_id=call_id, 
                          speaker=speaker,
                          text_length=len(speaker_text) if speaker_text else 0,
                          min_required=self.min_text_length)
                return {"success": False, "extracted_count": 0, "confidence": 0.0}
            
            logger.info("âœ… [VectorDB Flow] Speaker text filtered",
                       call_id=call_id,
                       filtered_length=len(speaker_text),
                       preview=speaker_text[:100] + "..." if len(speaker_text) > 100 else speaker_text)
            
            # 3. LLM ì§€ì‹ ì •ì œ â€” ì„¤ê³„ì„œ: ë§¥ë½ íŒŒì•…ì„ ìœ„í•´ ì „ì²´ ì „ì‚¬(ë°œì‹ ì+ì°©ì‹ ì) ì „ë‹¬, ì €ì¥ í›„ë³´ëŠ” ì°©ì‹ ìë§Œ
            logger.info("ğŸ”„ [VectorDB Flow] Step 4/6: LLM refining knowledge (full transcript for context)",
                       call_id=call_id)
            
            judgment = await self.llm.judge_usefulness(
                transcript=transcript,
                speaker=speaker,
                call_id=call_id,
            )
            
            logger.info("âœ… [VectorDB Flow] LLM knowledge refinement completed",
                       call_id=call_id,
                       is_useful=judgment["is_useful"],
                       confidence=judgment.get("confidence", 0.0),
                       reason=judgment.get("reason", "N/A"))
            
            if not judgment["is_useful"]:
                logger.info("âŒ [VectorDB Flow] Content not useful - Skipping storage", 
                          call_id=call_id,
                          reason=judgment.get("reason", "N/A"))
                return {
                    "success": True, 
                    "extracted_count": 0, 
                    "confidence": judgment.get("confidence", 0.0)
                }
            
            if judgment["confidence"] < self.min_confidence:
                logger.info("âŒ [VectorDB Flow] Low confidence - Skipping storage", 
                          call_id=call_id,
                          confidence=judgment["confidence"],
                          min_required=self.min_confidence)
                return {
                    "success": True, 
                    "extracted_count": 0, 
                    "confidence": judgment["confidence"]
                }
            
            # 4. ìœ ìš©í•œ ì •ë³´ ì¶”ì¶œ
            extracted_info = judgment.get("extracted_info", [])
            if not extracted_info:
                # LLMì´ êµ¬ì²´ì  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•œ ê²½ìš°, ì „ì²´ í…ìŠ¤íŠ¸ ì²­í‚¹
                logger.info("ğŸ”„ [VectorDB Flow] No specific info extracted, using full text",
                           call_id=call_id)
                extracted_info = [
                    {
                        "text": speaker_text,
                        "category": "ê¸°íƒ€",
                        "keywords": []
                    }
                ]
            else:
                logger.info("ğŸ”„ [VectorDB Flow] Extracted specific info",
                           call_id=call_id,
                           info_count=len(extracted_info))
            
            # 5. ì²­í‚¹ ë° ì„ë² ë”©
            logger.info("ğŸ”„ [VectorDB Flow] Step 5/6: Chunking and embedding",
                       call_id=call_id,
                       chunk_size=self.chunk_size,
                       chunk_overlap=self.chunk_overlap)
            
            stored_count = 0
            for idx, info in enumerate(extracted_info):
                text = info["text"]
                chunks = self._chunk_text(text)
                contains_pii = info.get("contains_pii", False)
                category = info.get("category", "ê¸°íƒ€")
                keywords = info.get("keywords", [])
                
                logger.info(f"  ğŸ“„ Processing info block {idx + 1}/{len(extracted_info)}",
                           call_id=call_id,
                           chunks_count=len(chunks),
                           category=category,
                           contains_pii=contains_pii)
                
                # Â§7 PII íŒŒì´í”„ë¼ì¸: contains_piiì´ê³  ê²€í†  ëŒ€ê¸°ì—´ ì‚¬ìš© ì‹œ VectorDB ê±´ë„ˆë›°ê³  ëŒ€ê¸°ì—´ì—ë§Œ ì ì¬
                if contains_pii and self.pii_review_queue_enabled and self._pending_store:
                    for chunk_idx, chunk in enumerate(chunks):
                        await self._pending_store.add(
                            call_id=call_id,
                            owner=owner_id,
                            speaker=speaker,
                            text=chunk,
                            category=category,
                            keywords=keywords if isinstance(keywords, list) else (keywords.split(",") if isinstance(keywords, str) else []),
                            contains_pii=True,
                            confidence=float(judgment.get("confidence", 0)),
                            chunk_index=chunk_idx,
                        )
                        stored_count += 1
                    continue
                
                for chunk_idx, chunk in enumerate(chunks):
                    # ì„ë² ë”© ìƒì„±
                    logger.debug(f"    ğŸ”¢ Generating embedding for chunk {chunk_idx + 1}/{len(chunks)}",
                                call_id=call_id,
                                chunk_preview=chunk[:50] + "...")
                    
                    embedding = await self.embedder.embed(chunk)
                    
                    # Vector DB ì €ì¥
                    doc_id = f"{call_id}_chunk_{idx}_{chunk_idx}"
                    metadata = {
                        "call_id": call_id,
                        "owner": owner_id,
                        "speaker": speaker,
                        "category": category,
                        "keywords": keywords,
                        "chunk_index": chunk_idx,
                        "confidence": judgment["confidence"],
                        "contains_pii": contains_pii,
                        "extraction_source": "call",
                    }
                    
                    logger.info(f"ğŸ”„ [VectorDB Flow] Step 6/6: Storing chunk {stored_count + 1} to VectorDB",
                               call_id=call_id,
                               doc_id=doc_id,
                               embedding_dim=len(embedding) if embedding else 0,
                               metadata_keys=list(metadata.keys()))
                    
                    await self.vector_db.upsert(
                        doc_id=doc_id,
                        embedding=embedding,
                        text=chunk,
                        metadata=metadata
                    )
                    
                    stored_count += 1
                    
                    logger.info(f"  âœ… Chunk {stored_count} stored successfully",
                               call_id=call_id,
                               doc_id=doc_id)
            
            self.total_extractions += 1
            self.total_chunks_stored += stored_count
            
            logger.info("ğŸ‰ [VectorDB Flow] âœ… Knowledge extraction COMPLETED!",
                       call_id=call_id,
                       total_chunks_stored=stored_count,
                       confidence=judgment["confidence"],
                       owner_id=owner_id)
            
            return {
                "success": True,
                "extracted_count": stored_count,
                "confidence": judgment["confidence"]
            }
            
        except Exception as e:
            logger.error("Knowledge extraction error", 
                        call_id=call_id, 
                        error=str(e),
                        exc_info=True)
            return {"success": False, "extracted_count": 0, "confidence": 0.0}
    
    async def _load_transcript(self, path: str) -> str:
        """
        ì „ì‚¬ í…ìŠ¤íŠ¸ ë¡œë“œ
        
        Args:
            path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì „ì‚¬ í…ìŠ¤íŠ¸
        """
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            logger.warning("Transcript file not found", path=path)
            return ""
        except Exception as e:
            logger.error("Transcript load error", path=path, error=str(e))
            return ""
    
    def _filter_by_speaker(self, transcript: str, speaker: str) -> str:
        """
        í™”ìë³„ ë°œí™” í•„í„°ë§
        
        í˜•ì‹ ì˜ˆì‹œ:
        ë°œì‹ ì: ì•ˆë…•í•˜ì„¸ìš”
        ì°©ì‹ ì: ë„¤, ì•ˆë…•í•˜ì„¸ìš”
        
        Args:
            transcript: ì „ì‚¬ í…ìŠ¤íŠ¸
            speaker: í™”ì (caller/callee)
            
        Returns:
            í•„í„°ë§ëœ í…ìŠ¤íŠ¸
        """
        lines = transcript.split('\n')
        speaker_lines = []
        
        speaker_label = "ì°©ì‹ ì" if speaker == "callee" else "ë°œì‹ ì"
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if ':' in line:
                parts = line.split(':', 1)
                if len(parts) == 2 and parts[0].strip() == speaker_label:
                    text = parts[1].strip()
                    if text:
                        speaker_lines.append(text)
        
        return ' '.join(speaker_lines)
    
    def _chunk_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ ì²­í‚¹ (ì˜¤ë²„ë© í¬í•¨)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if len(text) <= self.chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end]
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ)
            if end < len(text):
                last_period = max(
                    chunk.rfind('.'),
                    chunk.rfind('!'),
                    chunk.rfind('?'),
                    chunk.rfind('ã€‚')  # í•œêµ­ì–´ ë§ˆì¹¨í‘œ
                )
                if last_period > 0:
                    chunk = chunk[:last_period + 1]
                    end = start + last_period + 1
            
            chunk = chunk.strip()
            if chunk:
                chunks.append(chunk)
            
            # ë‹¤ìŒ ì‹œì‘ì  (ì˜¤ë²„ë© ì ìš©)
            start = end - self.chunk_overlap
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if start <= 0 or start >= len(text):
                break
        
        return chunks
    
    def get_stats(self) -> dict:
        """ì§€ì‹ ì¶”ì¶œ í†µê³„ ë°˜í™˜"""
        avg_chunks = (
            self.total_chunks_stored / self.total_extractions 
            if self.total_extractions > 0 else 0
        )
        
        return {
            "total_extractions": self.total_extractions,
            "total_chunks_stored": self.total_chunks_stored,
            "avg_chunks_per_extraction": avg_chunks,
            "min_confidence": self.min_confidence,
            "chunk_size": self.chunk_size,
        }

