"""
Google Gemini LLM Client

ëŒ€í™” ìƒì„± ë° ì§€ì‹ ìœ ìš©ì„± íŒë‹¨
"""

import time

# Gemini import ì¶”ì 
_import_logger_available = False
try:
    import structlog
    _logger = structlog.get_logger(__name__)
    _import_logger_available = True
    _logger.info("ðŸ”„ [LLM Module] Importing google.generativeai...")
    _genai_import_start = time.time()
except:
    pass

import google.generativeai as genai

if _import_logger_available:
    _genai_import_time = time.time() - _genai_import_start
    _logger.info(f"âœ… [LLM Module] google.generativeai imported", elapsed=f"{_genai_import_time:.3f}s")

import asyncio
import re
from typing import List, Dict, Optional, Any
import json

logger = structlog.get_logger(__name__)


class LLMClient:
    """
    Google Gemini LLM Client
    
    ëŒ€í™” ìƒì„± ë° ì§€ì‹ ìœ ìš©ì„± íŒë‹¨ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: dict, api_key: str):
        """
        Args:
            config: LLM ì„¤ì •
                - model: "gemini-pro"
                - temperature: 0.7
                - max_tokens: 200
                - top_p: 1.0
                - top_k: 1
            api_key: Google API í‚¤
        """
        self.config = config
        
        # Gemini ì„¤ì •
        genai.configure(api_key=api_key)
        
        model_name = config.get("model", "gemini-pro")
        self.model = genai.GenerativeModel(model_name=model_name)
        
        # Generation ì„¤ì • (max_output_tokens: config.yaml í‚¤, max_tokens: êµ¬ ì„¤ì • í˜¸í™˜)
        max_tokens = config.get("max_output_tokens") or config.get("max_tokens", 200)
        self.generation_config = genai.types.GenerationConfig(
            temperature=config.get("temperature", 0.7),
            top_p=config.get("top_p", 1.0),
            top_k=config.get("top_k", 1),
            max_output_tokens=max_tokens,
        )
        
        # ëŒ€í™” ížˆìŠ¤í† ë¦¬
        self.conversation_history: List[Dict[str, str]] = []
        self.max_history_length = config.get("max_history_length", 20)
        
        # í†µê³„
        self.total_requests = 0
        self.total_tokens = 0
        
        logger.info("LLMClient initialized", 
                   model=model_name,
                   temperature=config.get("temperature"))
    
    async def generate_response(
        self, 
        user_text: str, 
        context_docs: List[str],
        system_prompt: Optional[str] = None,
        call_id: Optional[str] = None  # DB ë¡œê¹…ìš©
    ) -> str:
        """
        ì‚¬ìš©ìž ìž…ë ¥ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            user_text: ì‚¬ìš©ìž ì§ˆë¬¸
            context_docs: RAG ê²€ìƒ‰ ê²°ê³¼ (ê´€ë ¨ ë¬¸ì„œ)
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            call_id: í†µí™” ID (DB ë¡œê¹…ìš©, ì„ íƒ)
            
        Returns:
            ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
        """
        import time
        start_time = time.time()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
            prompt = self._build_conversation_prompt(
                user_text, 
                context_docs, 
                system_prompt
            )
            
            # Gemini API í˜¸ì¶œ (ë¹„ë™ê¸°)
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content(
                    prompt,
                    generation_config=self.generation_config
                )
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            answer = response.text.strip()
            
            # ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.conversation_history.append({
                "role": "user",
                "content": user_text
            })
            self.conversation_history.append({
                "role": "assistant",
                "content": answer
            })
            
            # ížˆìŠ¤í† ë¦¬ ì œí•œ
            if len(self.conversation_history) > self.max_history_length:
                self.conversation_history = self.conversation_history[-self.max_history_length:]
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.total_requests += 1
            # í† í° ìˆ˜ ì¶”ì • (ì‹¤ì œ APIì—ì„œ ì œê³µí•˜ëŠ” ê²½ìš° í•´ë‹¹ ê°’ ì‚¬ìš©)
            tokens_used = len(prompt.split()) + len(answer.split())
            self.total_tokens += tokens_used
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            latency_ms = int((time.time() - start_time) * 1000)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            confidence = self._calculate_confidence(answer, context_docs)
            
            logger.info("LLM response generated",
                       call=True,
                       progress="llm",
                       user_text_length=len(user_text),
                       response_length=len(answer),
                       context_docs_count=len(context_docs),
                       latency_ms=latency_ms,
                       confidence=confidence)
            
            # DB ë¡œê¹… (ì‹ ê·œ)
            if call_id:
                try:
                    from ..logging.ai_logger import log_llm_process_sync
                    
                    log_llm_process_sync(
                        call_id=call_id,
                        input_prompt=prompt,
                        output_text=answer,
                        confidence=confidence,
                        latency_ms=latency_ms,
                        tokens_used=tokens_used,
                        model_name=self.config.get("model", "gemini-pro"),
                        temperature=self.config.get("temperature", 0.7)
                    )
                except ImportError:
                    logger.debug("AI logger not available, skipping DB logging")
                except Exception as e:
                    logger.error("Failed to log LLM process to DB", error=str(e))
            
            return answer
            
        except Exception as e:
            logger.error("LLM generation error", error=str(e), exc_info=True)
            return "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def format_for_customer(self, raw_text: str) -> str:
        """
        HITL ë‹´ë‹¹ìž ë‹µë³€ì„ ê³ ê°ì—ê²Œ ì „ë‹¬í•  í•œ ë¬¸ìž¥ìœ¼ë¡œ ì •ë¦¬ (ì„¤ê³„ TTS_RTP_AND_HITL_DESIGN.md).
        ëŒ€í™” ížˆìŠ¤í† ë¦¬ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ.
        """
        if not raw_text or not raw_text.strip():
            return raw_text
        prompt = (
            "ë‹¤ìŒì€ ìƒë‹´ ë‹´ë‹¹ìžê°€ ê³ ê°ì—ê²Œ ì „ë‹¬í•  ë‚´ìš©ìž…ë‹ˆë‹¤. "
            "ê³ ê°ì—ê²Œ ìžì—°ìŠ¤ëŸ½ê²Œ ë§í•  í•œ ë¬¸ìž¥ìœ¼ë¡œë§Œ ì •ë¦¬í•´ ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶™ì´ì§€ ë§ˆì„¸ìš”.\n\n"
            f"ë‹´ë‹¹ìž ì›ë¬¸:\n{raw_text.strip()}"
        )
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=256,
                        temperature=0.3,
                    ),
                ),
            )
            return (response.text or raw_text).strip()
        except Exception as e:
            logger.warning("format_for_customer_failed", error=str(e))
            return raw_text

    def _calculate_confidence(self, answer: str, context_docs: List[str]) -> float:
        """
        LLM ì‘ë‹µì˜ ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        
        Args:
            answer: LLM ì‘ë‹µ
            context_docs: ì°¸ê³ í•œ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ
            
        Returns:
            ì‹ ë¢°ë„ (0.0 ~ 1.0)
        """
        confidence = 0.5  # ê¸°ë³¸ê°’
        
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ ìžˆìœ¼ë©´ ì‹ ë¢°ë„ ìƒìŠ¹
        if context_docs:
            confidence += 0.3
        
        # ë‹µë³€ì´ ê¸¸ë©´ ì‹ ë¢°ë„ ìƒìŠ¹ (êµ¬ì²´ì ì¸ ë‹µë³€)
        if len(answer) > 50:
            confidence += 0.1
        
        # "ëª¨ë¥´"ë‚˜ "í™•ì¸"ì´ ìžˆìœ¼ë©´ ì‹ ë¢°ë„ í•˜ë½
        if "ëª¨ë¥´" in answer or "í™•ì¸" in answer:
            confidence -= 0.2
        
        return max(0.0, min(1.0, confidence))
    
    def _build_conversation_prompt(
        self, 
        user_text: str, 
        context_docs: List[str],
        system_prompt: Optional[str] = None
    ) -> str:
        """ëŒ€í™” í”„ë¡¬í”„íŠ¸ ì¡°ë¦½"""
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        if not system_prompt:
            system_prompt = (
                "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì •í™•í•œ AI ë¹„ì„œìž…ë‹ˆë‹¤. "
                "ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , "
                "ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì†”ì§ížˆ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”. "
                "ë‹µë³€ì€ 1-2ë¬¸ìž¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”."
            )
        
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ
        context_str = ""
        if context_docs:
            context_str = "\n\n**ì°¸ê³  ì •ë³´:**\n" + "\n".join([
                f"- {doc}" for doc in context_docs[:3]  # ìµœëŒ€ 3ê°œ
            ])
        
        # ëŒ€í™” ížˆìŠ¤í† ë¦¬
        history_str = ""
        if self.conversation_history:
            recent_history = self.conversation_history[-10:]  # ìµœê·¼ 5í„´ (10ê°œ ë©”ì‹œì§€)
            history_str = "\n\n**ì´ì „ ëŒ€í™”:**\n" + "\n".join([
                f"{'ì‚¬ìš©ìž' if msg['role'] == 'user' else 'AI'}: {msg['content']}"
                for msg in recent_history
            ])
        
        # ì „ì²´ í”„ë¡¬í”„íŠ¸
        prompt = f"""{system_prompt}
{context_str}
{history_str}

**í˜„ìž¬ ì§ˆë¬¸:**
ì‚¬ìš©ìž: {user_text}

AI:"""
        
        return prompt
    
    async def judge_barge_in(
        self,
        user_text: str,
        ai_current_text: str = "",
    ) -> str:
        """
        Barge-in íŒë‹¨ (Phase 3): ì‚¬ìš©ìž ë°œí™”ê°€ ë§žìž¥êµ¬ì¸ì§€ interruptì¸ì§€ LLMì´ íŒë‹¨.
        
        Args:
            user_text: ì‚¬ìš©ìžê°€ ë§í•œ ë‚´ìš©
            ai_current_text: AIê°€ í˜„ìž¬ ë§í•˜ê³  ìžˆëŠ” ë‚´ìš©
            
        Returns:
            "ë§žìž¥êµ¬" ë˜ëŠ” "interruption"
        """
        try:
            prompt = (
                'AIê°€ ê³ ê°ì—ê²Œ ì„¤ëª…ì„ í•˜ê³  ìžˆëŠ” ì¤‘ì— ê³ ê°ì´ ì•„ëž˜ì™€ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤.\n\n'
                f'AIê°€ ë§í•˜ê³  ìžˆëŠ” ë‚´ìš©: "{ai_current_text[:200]}"\n'
                f'ê³ ê°ì´ ë§í•œ ë‚´ìš©: "{user_text}"\n\n'
                'ê³ ê°ì˜ ë§ì´ ë‹¤ìŒ ì¤‘ ì–´ë””ì— í•´ë‹¹í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”:\n'
                '1. "ë§žìž¥êµ¬" - ë“£ê³  ìžˆë‹¤ëŠ” í‘œì‹œ (ì˜ˆ: "ë„¤", "ìŒ", "ê·¸ë ‡êµ°ìš”", "ì•„~")\n'
                '2. "interruption" - ë§ì„ ëŠê³  ìƒˆë¡œìš´ ìš”ì²­/ì§ˆë¬¸ì„ í•˜ë ¤ëŠ” ì˜ë„\n\n'
                'ë‹µë³€: "ë§žìž¥êµ¬" ë˜ëŠ” "interruption" ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.'
            )

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content(
                    prompt,
                    generation_config={
                        "temperature": 0.1,
                        "max_output_tokens": 10,
                    },
                ),
            )

            result = response.text.strip().lower().replace('"', '').replace("'", "")

            if "interruption" in result:
                return "interruption"
            return "ë§žìž¥êµ¬"

        except Exception as e:
            logger.warning("judge_barge_in_error", error=str(e))
            # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ interruptë¡œ íŒë‹¨ (ì‚¬ìš©ìž ì˜ë„ ìš°ì„ )
            return "interruption"

    async def judge_usefulness(
        self,
        transcript: str,
        speaker: str = "callee",
        call_id: str = "",
    ) -> Dict[str, Any]:
        """
        í†µí™”ì •ë³´ ì¤‘ ì§€ì‹ì •ë³´ ì •ì œ (ì§€ì‹ ì¶”ì¶œìš©).
        í†µí™” ì „ì‚¬ì—ì„œ ì €ìž¥í•  ì§€ì‹ ë‹¨ìœ„ë¥¼ ì¶”ì¶œÂ·ë¶„ë¥˜í•œë‹¤. (êµ¬ ëª…ì¹­: ìœ ìš©ì„± íŒë‹¨)

        Args:
            transcript: í†µí™” ì „ì²´ ì „ì‚¬ (ë°œì‹ ìž+ì°©ì‹ ìž). ë§¥ë½ íŒŒì•…ìš©ìœ¼ë¡œ ì „ì²´ë¥¼ ë„˜ê¸°ê³ , ì €ìž¥ì€ ì°©ì‹ ìž ë°œí™”ë§Œ ì¶”ì¶œ.
            speaker: ì €ìž¥ ëŒ€ìƒ í™”ìž (caller/callee). extracted_infoì—ëŠ” ì´ í™”ìž ë°œí™”ë§Œ ë„£ìœ¼ë¼ê³  í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜.
            call_id: í†µí™” ID (ë¡œê·¸ call í‚¤ìš©)

        Returns:
            {
                "is_useful": bool,
                "confidence": float,
                "reason": str,
                "extracted_info": List[Dict]
            }
        """
        result_text = ""
        json_text = ""
        judgment_max_tokens = self.config.get("judgment_max_output_tokens") or self.config.get("max_output_tokens") or self.config.get("max_tokens") or 2048
        # ì„¤ê³„ì„œ 2.2a: ê¸´ í†µí™” í† í°/ê¸¸ì´ ì²˜ë¦¬ â€” ì„¤ì • ê°€ëŠ¥ ë¬¸ìž ìƒí•œ (ê¸°ë³¸ 6000)
        max_input_chars = self.config.get("judgment_max_input_chars", 6000)
        transcript_for_prompt = transcript[:max_input_chars]
        if len(transcript) > max_input_chars:
            transcript_for_prompt += "\n\n[ì´í•˜ ìƒëžµ: í†µí™”ê°€ ê¸¸ì–´ ì•žë¶€ë¶„ë§Œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.]"
        try:
            prompt = f"""ë‹¹ì‹ ì€ í†µí™” ê¸°ë¡ì—ì„œ ì§€ì‹ ë² ì´ìŠ¤(VectorDB)ì— ì €ìž¥í•  ì§€ì‹ ì •ë³´ë¥¼ ì •ì œ(ì¶”ì¶œÂ·ë¶„ë¥˜)í•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ëª©ì : AI ë¹„ì„œê°€ ì´í›„ í†µí™”ì—ì„œ ìž¬ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ì§€ì‹ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

**ìž…ë ¥ í˜•ì‹:** ì•„ëž˜ëŠ” "ë°œì‹ ìž:", "ì°©ì‹ ìž:"ë¡œ êµ¬ë¶„ëœ **ì „ì²´ ëŒ€í™”** ì „ì‚¬ìž…ë‹ˆë‹¤. ë°œì‹ ìž ì§ˆë¬¸/ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ì°©ì‹ ìž ë‹µë³€ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ì„¸ìš”.

**ì €ìž¥ ëŒ€ìƒ:** ì €ìž¥í•  ì§€ì‹ì€ ë°˜ë“œì‹œ **ì°©ì‹ ìž(callee)ê°€ ë§í•œ ë‚´ìš©**ì—ì„œë§Œ ì¶”ì¶œí•˜ì„¸ìš”. extracted_infoì˜ textì—ëŠ” ì°©ì‹ ìž ë°œí™” ì›ë¬¸ë§Œ ë„£ìœ¼ì„¸ìš”. ë°œì‹ ìž ë°œí™”ëŠ” ì €ìž¥í•˜ì§€ ë§ˆì„¸ìš”.

**ìœ ìš©í•˜ë‹¤ê³  íŒë‹¨í•  ê²½ìš° (is_useful = true):**
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì§ˆë¬¸Â·ë‹µë³€ (êµ¬ì²´ì  ì‚¬ì‹¤, ì ˆì°¨, ì¡°ê±´ì´ í¬í•¨ëœ ê²½ìš°)
- ë‹¤ë¥¸ í†µí™”ì—ì„œë„ ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ FAQ ì„±ê²©ì˜ ëŒ€í™”
- ë¬¸ì˜/ì´ìŠˆì— ëŒ€í•œ í•´ê²° ë°©ë²•Â·ë‹¤ìŒ ë‹¨ê³„ê°€ ëª…í™•í•œ ê²½ìš°
- ì•½ì†Â·ì¼ì •Â·ì—°ë½ì²˜Â·ì—…ë¬´ ì§€ì‹œÂ·ì„ í˜¸ë„ ë“± ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ (ê°œì¸ì •ë³´ëŠ” ì €ìž¥ ì‹œ ë³„ë„ ì •ì±… ì ìš©)

**ìœ ìš©í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨í•  ê²½ìš° (is_useful = false):**
- ê°œì¸ì„ íŠ¹ì •í•  ìˆ˜ ìžˆëŠ” ì •ë³´ë§Œ ìžˆëŠ” ê²½ìš° (ì´ë¦„, ì „í™”ë²ˆí˜¸, ì£¼ì†Œ ë“±)
- ì¸ì‚¬, ë§žìž¥êµ¬, "ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”" ë“± ì§€ì‹ìœ¼ë¡œ ì“¸ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°
- "í™•ì¸ í›„ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ìž˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤" ë“± ë¯¸í•´ê²°Â·ìœ ë³´ë§Œ ìžˆëŠ” ê²½ìš°
- ì‚¬ì‹¤Â·ì ˆì°¨ ì—†ì´ ë¶ˆë§ŒÂ·ì¹­ì°¬ ë“± ê°ì • í‘œí˜„ë§Œ ìžˆëŠ” ê²½ìš°
- ì›ë¬¸ì— ì—†ëŠ” ì§ˆë¬¸/ë‹µë³€ì„ ë§Œë“¤ì–´ ë‚´ì§€ ë§ ê²ƒ (ì›ë¬¸ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì¶”ì¶œ)

**í†µí™” ë‚´ìš© (ì „ì²´ ëŒ€í™”, ì €ìž¥ì€ ì°©ì‹ ìž ë°œí™”ë§Œ):**
{transcript_for_prompt}

**ì¶œë ¥ í˜•ì‹ (JSONë§Œ ì¶œë ¥):**
{{
  "is_useful": true ë˜ëŠ” false,
  "confidence": 0.0~1.0,
  "reason": "íŒë‹¨ ì´ìœ  (50ìž ì´ë‚´)",
  "extracted_info": [
    {{
      "text": "ì›ë¬¸ì— ë‚˜ì˜¨ ë¬¸ìž¥ ê·¸ëŒ€ë¡œ ë˜ëŠ” í•œ ë‹¨ìœ„ë¡œ ì •ë¦¬í•œ í…ìŠ¤íŠ¸",
      "category": "FAQ|ì´ìŠˆí•´ê²°|ì•½ì†|ì •ë³´|ì§€ì‹œ|ì„ í˜¸ë„|ê¸°íƒ€",
      "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
      "contains_pii": false
    }}
  ]
}}

**category ê·œì¹™ (ë°˜ë“œì‹œ ì•„ëž˜ ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©):**
- FAQ: ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ ì§ˆë¬¸Â·ë‹µë³€ ìŒ
- ì´ìŠˆí•´ê²°: ë¬¸ì˜/ë¶ˆë§Œì— ëŒ€í•œ í•´ê²° ë°©ë²•Â·ë‹¤ìŒ ë‹¨ê³„ê°€ ëª…í™•í•œ ê²½ìš°
- ì•½ì†: ì¼ì‹œÂ·ìž¥ì†ŒÂ·ë‹´ë‹¹ìž ë“± êµ¬ì²´ì  ì•½ì†
- ì •ë³´: ì˜ì—…ì‹œê°„, ì ˆì°¨, ì¡°ê±´ ë“± ì‚¬ì‹¤ ì •ë³´
- ì§€ì‹œ: ì—…ë¬´ ì§€ì‹œ, "í•­ìƒ Aë¡œ í•´ì£¼ì„¸ìš”" ë“±
- ì„ í˜¸ë„: "BëŠ” ì‹«ì–´í•©ë‹ˆë‹¤" ë“± ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ ì„ í˜¸
- ê¸°íƒ€: ìœ„ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë‚˜ ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´

**í•„ìˆ˜ ì§€ì¹¨:**
- reasonì€ 50ìž ì´ë‚´ë¡œ ìž‘ì„±í•˜ì„¸ìš”.
- extracted_infoì˜ textëŠ” **ì°©ì‹ ìžê°€ ë§í•œ ë¬¸ìž¥ë§Œ** ë„£ìœ¼ì„¸ìš”. ë°œì‹ ìž ë°œí™”ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ì›ë¬¸ì— ë‚˜ì˜¨ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ìž„ì˜ë¡œ ìš”ì•½í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”. í•œ í•­ëª©ì€ í•˜ë‚˜ì˜ ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ ë‹¨ìœ„(ì˜ˆ: í•˜ë‚˜ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒ, í•˜ë‚˜ì˜ ì•½ì†)ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
- ê°œì¸ì„ íŠ¹ì •í•  ìˆ˜ ìžˆëŠ” ì •ë³´(ì´ë¦„Â·ì „í™”ë²ˆí˜¸Â·ì£¼ì†Œ ë“±)ê°€ í¬í•¨ë˜ë©´ í•´ë‹¹ í•­ëª©ì— "contains_pii": true, ì—†ìœ¼ë©´ falseë¡œ í‘œì‹œí•˜ì„¸ìš”.
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.

JSON:"""

            logger.info("llm_judgment_request",
                        call=True,
                        call_id=call_id or "",
                        category="llm",
                        progress="extraction",
                        transcript_length=len(transcript),
                        max_input_chars=max_input_chars,
                        transcript_truncated=len(transcript) > max_input_chars,
                        speaker=speaker,
                        max_tokens=judgment_max_tokens,
                        prompt_length=len(prompt),
                        prompt_preview=prompt[:200].replace("\n", " ") + "..." if len(prompt) > 200 else prompt[:200].replace("\n", " "),
                        note="ì§€ì‹ ì •ì œ ìš”ì²­ (ì „ì²´ ëŒ€í™” ë§¥ë½, ì €ìž¥ì€ ì°©ì‹ ìž ë°œí™”ë§Œ)")

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.3,
                        max_output_tokens=judgment_max_tokens,
                    )
                )
            )

            result_text = (response.text or "").strip()

            # Gemini ì¢…ë£Œ ì‚¬ìœ  ë¡œê¹… (1=STOP, 2=MAX_TOKENS, 3=SAFETY, 4=RECITATION ë“± â€” ìž˜ë¦¼ ì‹œ 2)
            finish_reason = None
            finish_reason_desc = None
            try:
                if getattr(response, "candidates", None) and len(response.candidates) > 0:
                    finish_reason = getattr(response.candidates[0], "finish_reason", None)
                    if finish_reason is not None:
                        fr_map = {1: "STOP", 2: "MAX_TOKENS", 3: "SAFETY", 4: "RECITATION"}
                        finish_reason_desc = fr_map.get(int(finish_reason), str(finish_reason))
            except Exception:
                pass
            logger.info("llm_judgment_response",
                        call=True,
                        call_id=call_id or "",
                        category="llm",
                        progress="extraction",
                        response_length=len(result_text),
                        finish_reason=finish_reason_desc or str(finish_reason),
                        response_full=result_text[:2000] if len(result_text) <= 2000 else result_text[:2000] + "...",
                        note="ìœ ìš©ì„± íŒë‹¨ ì‘ë‹µ (call í‚¤ë¡œ í•„í„°)")
            if finish_reason_desc == "MAX_TOKENS":
                logger.warning("llm_judgment_truncated",
                              call=True,
                              call_id=call_id or "",
                              progress="extraction",
                              note="ì‘ë‹µì´ max_output_tokensì—ì„œ ìž˜ë¦¼, JSON ë³µêµ¬ ì‹œë„ (judgment_max_output_tokens ìƒí–¥ ê¶Œìž¥)")
            
            # 1) JSON ì¶”ì¶œ: ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì œê±° í›„ ë³¸ë¬¸ë§Œ ì‚¬ìš©
            json_text = None
            if "```json" in result_text:
                try:
                    json_text = result_text.split("```json")[1].split("```")[0].strip()
                except IndexError:
                    pass
            if not json_text and "```" in result_text:
                try:
                    json_text = result_text.split("```")[1].split("```")[0].strip()
                except IndexError:
                    pass
            if not json_text and "{" in result_text and "}" in result_text:
                try:
                    start = result_text.index("{")
                    end = result_text.rindex("}") + 1
                    json_text = result_text[start:end]
                except (ValueError, IndexError):
                    pass
            if not json_text:
                json_text = result_text

            # 2) íŒŒì‹± ì‹œë„: json.loads â†’ ì‹¤íŒ¨ ì‹œ ì •ë¦¬ í›„ ìž¬ì‹œë„
            result = None
            try:
                result = json.loads(json_text)
            except json.JSONDecodeError as parse_error:
                logger.warning("JSON parse failed, attempting cleanup",
                             call=True,
                             call_id=call_id or "",
                             error=str(parse_error),
                             json_preview=json_text[:200] if json_text else "None")
                if json_text:
                    fixed = json_text.rstrip()
                    # ì£¼ì„/í›„í–‰ ì‰¼í‘œ ì •ë¦¬
                    lines = []
                    for line in fixed.split("\n"):
                        if "//" in line:
                            line = line.split("//")[0]
                        lines.append(line)
                    fixed = "\n".join(lines).replace(",}", "}").replace(",]", "]")

                    parse_err_str = str(parse_error).lower()
                    is_truncated = (
                        "unterminated string" in parse_err_str
                        or "expecting value" in parse_err_str
                        or "expecting" in parse_err_str
                    )

                    # ìž˜ë¦° ì‘ë‹µ ë³µêµ¬: Unterminated string ìš°ì„ , ê·¸ ë‹¤ìŒ incomplete field (confidence:, reason: ë“±)
                    if fixed and is_truncated:
                        fixed_clean = fixed.rstrip()
                        if fixed_clean.endswith(","):
                            fixed_clean = fixed_clean[:-1]
                        # 1) Unterminated string: "reason": "â€¦ ì—ì„œ ëŠê¸´ ê²½ìš° â€” ë‹«ëŠ” " í›„ extracted_infoÂ·ê´„í˜¸ ë‹«ê¸°
                        if "unterminated string" in parse_err_str:
                            nq = fixed_clean.count('"') - fixed_clean.count('\\"')
                            if nq % 2 != 0:
                                fixed_clean += '"'
                            # reason ê°’ë§Œ ë‹«í˜€ ìžˆê³  extracted_infoê°€ ì—†ìœ¼ë©´ ë³´ê°• (MAX_TOKENS ìž˜ë¦¼ ì‹œ í”í•œ íŒ¨í„´)
                            if '"reason"' in fixed_clean and '"extracted_info"' not in fixed_clean:
                                fixed_clean += ', "extracted_info": []'
                            open_braces = fixed_clean.count("{") - fixed_clean.count("}")
                            open_brackets = fixed_clean.count("[") - fixed_clean.count("]")
                            fixed_clean += "]" * max(0, open_brackets) + "}" * max(0, open_braces)
                            try:
                                result = json.loads(fixed_clean)
                            except json.JSONDecodeError:
                                pass
                        # 1-2) Unterminated stringì¸ë° ìœ„ì—ì„œ ë³µêµ¬ ì‹¤íŒ¨ ì‹œ: reason ê°’ ë‹«ëŠ” " í›„ extracted_infoÂ·ê´„í˜¸ ì¶”ê°€
                        if result is None and "unterminated string" in parse_err_str and fixed_clean:
                            match = re.search(r'"reason"\s*:\s*"', fixed_clean)
                            if match:
                                try_clean = fixed_clean.rstrip()
                                if (try_clean.count('"') - try_clean.count('\\"')) % 2 != 0:
                                    try_clean += '"'
                                try_clean += ', "extracted_info": []}'
                                open_braces = try_clean.count("{") - try_clean.count("}")
                                open_brackets = try_clean.count("[") - try_clean.count("]")
                                try_clean += "]" * max(0, open_brackets) + "}" * max(0, open_braces)
                                try:
                                    result = json.loads(try_clean)
                                except json.JSONDecodeError:
                                    pass
                        # 2) ë¯¸ë³µêµ¬ ì‹œ ê°’ ì—†ì´ ëŠê¸´ í•„ë“œ ë³µêµ¬ (ì›ë³¸ fixed ê¸°ì¤€ìœ¼ë¡œ ìž¬ì‹œë„)
                        if result is None:
                            fixed_clean = fixed.rstrip()
                            if fixed_clean.endswith(","):
                                fixed_clean = fixed_clean[:-1]
                            if re.search(r'"confidence"\s*$', fixed_clean):
                                fixed_clean += ': 0.0, "reason": "", "extracted_info": []}'
                            elif re.search(r'"confidence"\s*:\s*$', fixed_clean):
                                fixed_clean += '0.0, "reason": "", "extracted_info": []}'
                            elif re.search(r'"confidence"\s*:\s*$', fixed_clean, re.MULTILINE):
                                fixed_clean += '0.0, "reason": "", "extracted_info": []}'
                            elif re.search(r'"reason"\s*:\s*$', fixed_clean):
                                fixed_clean += '"", "extracted_info": []}'
                            elif re.search(r'"extracted_info"\s*:\s*$', fixed_clean):
                                fixed_clean += '[]}'
                            elif re.search(r'"is_useful"\s*:\s*$', fixed_clean):
                                fixed_clean += 'false, "confidence": 0.0, "reason": "", "extracted_info": []}'
                            else:
                                fixed_clean = re.sub(r'(:\s*)(\s*)$', r'\g<1>null\2', fixed_clean, count=1)
                            open_braces = fixed_clean.count("{") - fixed_clean.count("}")
                            open_brackets = fixed_clean.count("[") - fixed_clean.count("]")
                            fixed_clean += "]" * max(0, open_brackets) + "}" * max(0, open_braces)
                            try:
                                result = json.loads(fixed_clean)
                            except json.JSONDecodeError:
                                pass
                        fixed = fixed_clean
                    if result is None and fixed and re.search(r"\d\s*$", fixed):
                        fixed += "}"
                        open_braces = fixed.count("{") - fixed.count("}")
                        open_brackets = fixed.count("[") - fixed.count("]")
                        fixed += "]" * open_brackets + "}" * open_braces
                        try:
                            result = json.loads(fixed)
                        except json.JSONDecodeError:
                            pass
                    if result is None and fixed and not fixed.endswith("}"):
                        fixed = re.sub(r":\s*$", ": null", fixed)
                        fixed = re.sub(r",\s*$", "", fixed)
                        if (fixed.count('"') - fixed.count('\\"')) % 2 != 0:
                            fixed += '"'
                        open_braces = fixed.count("{") - fixed.count("}")
                        open_brackets = fixed.count("[") - fixed.count("]")
                        fixed += "]" * max(0, open_brackets) + "}" * max(0, open_braces)
                        try:
                            result = json.loads(fixed)
                        except json.JSONDecodeError:
                            pass
                    if result is None:
                        try:
                            result = json.loads(fixed)
                        except json.JSONDecodeError:
                            pass
                    # Final fallback: do not re-raise; return default so caller always gets valid dict
                    if result is None:
                        result = {
                            "is_useful": False,
                            "confidence": 0.0,
                            "reason": "Response truncated or invalid JSON; default applied" if is_truncated else "JSON parse failed after cleanup",
                            "extracted_info": [],
                        }
            
            if result is None:
                result = {
                    "is_useful": False,
                    "confidence": 0.0,
                    "reason": "JSON parse failed",
                    "extracted_info": [],
                }
            
            # 3) í•˜ìœ„ í˜¸í™˜: confidence/is_useful ì ˆëŒ€ None ê¸ˆì§€ (ê¸°ë³¸ê°’ ì ìš©)
            is_useful = result.get("is_useful")
            if is_useful is None:
                is_useful = False
            else:
                is_useful = bool(is_useful)
            confidence = result.get("confidence")
            if confidence is None:
                confidence = 0.0
            else:
                try:
                    confidence = float(confidence)
                except (TypeError, ValueError):
                    confidence = 0.0
                confidence = max(0.0, min(1.0, confidence))
            result["is_useful"] = is_useful
            result["confidence"] = confidence
            result.setdefault("reason", "")
            result.setdefault("extracted_info", [])
            # ì„¤ê³„ì„œ Â§2.3 ì¹´í…Œê³ ë¦¬ Enum ì •ê·œí™”: í—ˆìš©ê°’ ì™¸ëŠ” "ê¸°íƒ€"ë¡œ ë§¤í•‘
            JUDGMENT_CATEGORIES = {"FAQ", "ì´ìŠˆí•´ê²°", "ì•½ì†", "ì •ë³´", "ì§€ì‹œ", "ì„ í˜¸ë„", "ê¸°íƒ€"}
            for item in result.get("extracted_info", []):
                if isinstance(item, dict):
                    cat = item.get("category") or "ê¸°íƒ€"
                    if isinstance(cat, str):
                        cat = cat.strip()
                    item["category"] = cat if cat in JUDGMENT_CATEGORIES else "ê¸°íƒ€"
            # optional keys (e.g. contains_pii) preserved for downstream

            logger.info("llm_judgment_completed",
                        call=True,
                        call_id=call_id or "",
                        category="llm",
                        progress="extraction",
                        is_useful=result["is_useful"],
                        confidence=result["confidence"])

            return result

        except json.JSONDecodeError as e:
            logger.error("llm_judgment_json_failed",
                        call=True,
                        call_id=call_id or "",
                        category="llm",
                        progress="extraction",
                        error=str(e),
                        raw_response_full=result_text[:2000] if result_text else "N/A",
                        json_attempt_full=json_text[:2000] if json_text else "N/A")
            return {
                "is_useful": False,
                "confidence": 0.0,
                "reason": f"JSON parse error: {str(e)}",
                "extracted_info": []
            }
        except Exception as e:
            logger.error("llm_judgment_error",
                        call=True,
                        call_id=call_id or "",
                        category="llm",
                        progress="extraction",
                        error=str(e),
                        exc_info=True)
            return {
                "is_useful": False,
                "confidence": 0.0,
                "reason": f"Error: {str(e)}",
                "extracted_info": []
            }
    
    def clear_history(self):
        """ëŒ€í™” ížˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history.clear()
        logger.info("LLM conversation history cleared")
    
    def get_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” ížˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.conversation_history.copy()
    
    def get_stats(self) -> dict:
        """LLM í†µê³„ ë°˜í™˜"""
        return {
            "total_requests": self.total_requests,
            "total_tokens": self.total_tokens,
            "history_length": len(self.conversation_history),
            "avg_tokens_per_request": (
                self.total_tokens / self.total_requests 
                if self.total_requests > 0 else 0
            ),
        }

