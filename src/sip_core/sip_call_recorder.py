"""
SIP Call Recorder

SIP PBX ì¼ë°˜ í†µí™” (ì‚¬ëŒ-ì‚¬ëŒ) ë…¹ìŒ
RTP Relay ë ˆë²¨ì—ì„œ íŒ¨í‚· ìº¡ì²˜ ë° WAV íŒŒì¼ ì €ì¥
í›„ì²˜ë¦¬ STTë¡œ transcript ìƒì„±
"""

import asyncio
import wave
import struct
from pathlib import Path
from typing import Optional, Dict, List
from datetime import datetime
import json
import structlog

logger = structlog.get_logger(__name__)


class SIPCallRecorder:
    """
    SIP í†µí™” ë…¹ìŒ (RTP Relay ë ˆë²¨)
    
    - RTP íŒ¨í‚· ìº¡ì²˜
    - G.711 â†’ PCM ë³€í™˜
    - WAV íŒŒì¼ ì €ì¥
    - í™”ì ë¶„ë¦¬ (caller/callee)
    - í›„ì²˜ë¦¬ STT (Google Speech-to-Text)
    """
    
    def __init__(
        self, 
        output_dir: str = "./recordings", 
        sample_rate: int = 8000,
        enable_post_stt: bool = True,
        enable_diarization: bool = True,
        stt_language: str = "ko-KR",
        gcp_credentials_path: Optional[str] = None
    ):
        """
        Args:
            output_dir: ë…¹ìŒ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
            sample_rate: ìƒ˜í”Œë ˆì´íŠ¸ (Hz) - ì¼ë°˜ì ìœ¼ë¡œ 8000 (telephony)
            enable_post_stt: í›„ì²˜ë¦¬ STT í™œì„±í™” ì—¬ë¶€
            enable_diarization: í™”ì ë¶„ë¦¬(diarization) í™œì„±í™” ì—¬ë¶€
            stt_language: STT ì–¸ì–´ ì½”ë“œ
            gcp_credentials_path: GCP ì¸ì¦ íŒŒì¼ ê²½ë¡œ
        """
        self.output_dir = Path(output_dir)
        self.sample_rate = sample_rate
        self.channels = 1  # Mono
        self.sample_width = 2  # 16-bit
        
        # í›„ì²˜ë¦¬ STT ì„¤ì •
        self.enable_post_stt = enable_post_stt
        self.enable_diarization = enable_diarization
        self.stt_language = stt_language
        self.gcp_credentials_path = gcp_credentials_path
        
        # í™œì„± ë…¹ìŒ ì„¸ì…˜
        self.active_recordings: Dict[str, dict] = {}
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Google Speech-to-Text í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì„ íƒì )
        self.stt_client = None
        if self.enable_post_stt:
            self._init_stt_client()
        
        logger.info("SIPCallRecorder initialized", 
                   output_dir=str(self.output_dir),
                   sample_rate=sample_rate,
                   enable_post_stt=enable_post_stt,
                   enable_diarization=enable_diarization)
    
    async def start_recording(
        self, 
        call_id: str,
        caller_id: str,
        callee_id: str
    ):
        """
        í†µí™” ë…¹ìŒ ì‹œì‘
        
        Args:
            call_id: í†µí™” ID
            caller_id: ë°œì‹ ì ID
            callee_id: ìˆ˜ì‹ ì ID
        """
        if call_id in self.active_recordings:
            logger.warning("Recording already active", call_id=call_id)
            return
        
        logger.info("SIP call recording started", 
                   call_id=call_id,
                   caller=caller_id,
                   callee=callee_id)
        
        # ë…¹ìŒ ë””ë ‰í† ë¦¬ ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dir_name = f"{timestamp}_{caller_id}_to_{callee_id}"
        call_dir = self.output_dir / dir_name
        call_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("Recording directory created",
                   call_id=call_id,
                   directory=str(call_dir))
        
        # ë…¹ìŒ ì„¸ì…˜ ì´ˆê¸°í™”
        self.active_recordings[call_id] = {
            "start_time": datetime.now(),
            "caller_id": caller_id,
            "callee_id": callee_id,
            "caller_buffer": [],
            "callee_buffer": [],
            "caller_frames": 0,
            "callee_frames": 0,
            "call_dir": call_dir,
            "dir_name": dir_name  # ë””ë ‰í† ë¦¬ ì´ë¦„ ì €ì¥ (metadataìš©)
        }
    
    async def add_rtp_packet(
        self,
        call_id: str,
        audio_data: bytes,
        direction: str,
        codec: str = "PCMU"
    ):
        """
        RTP íŒ¨í‚· ì¶”ê°€ (RTP Relayì—ì„œ í˜¸ì¶œ)
        
        Args:
            call_id: í†µí™” ID
            audio_data: RTP í˜ì´ë¡œë“œ (ì˜¤ë””ì˜¤ ë°ì´í„°)
            direction: "caller" | "callee"
            codec: "PCMU" | "PCMA" | "opus"
        """
        recording = self.active_recordings.get(call_id)
        if not recording:
            # í†µí™” ì¢…ë£Œ í›„ ì”ì—¬ RTP íŒ¨í‚·ì€ ì •ìƒì ìœ¼ë¡œ ë¬´ì‹œ (debug ë ˆë²¨)
            # BYE ì²˜ë¦¬ í›„ Pipecat TTSê°€ ì”ì—¬ íŒ¨í‚·ì„ ë³´ë‚¼ ìˆ˜ ìˆìŒ
            if not hasattr(self, '_no_recording_warned'):
                self._no_recording_warned = set()
            if call_id not in self._no_recording_warned:
                self._no_recording_warned.add(call_id)
                logger.debug("RTP packet after recording stopped (expected after BYE)",
                            call_id=call_id,
                            direction=direction)
            return
        
        # âœ… ë¹ˆ ë°ì´í„° ì²´í¬
        if not audio_data or len(audio_data) == 0:
            logger.warning("Empty RTP payload received",
                          call_id=call_id,
                          direction=direction)
            return
        
        # ì½”ë± ë””ì½”ë”© (G.711 â†’ PCM)
        if codec == "PCMU":
            pcm_data = self._decode_g711_ulaw(audio_data)
        elif codec == "PCMA":
            pcm_data = self._decode_g711_alaw(audio_data)
        else:
            # Opus ë“± ë‹¤ë¥¸ ì½”ë±ì€ ì¶”í›„ êµ¬í˜„
            logger.warning("Unsupported codec, using raw data",
                          codec=codec,
                          call_id=call_id)
            pcm_data = audio_data
        
        # âœ… ë””ì½”ë”© ê²°ê³¼ ì²´í¬
        if not pcm_data or len(pcm_data) == 0:
            logger.warning("Decoding resulted in empty PCM data",
                          call_id=call_id,
                          direction=direction,
                          codec=codec,
                          input_size=len(audio_data))
            return
        
        # ë²„í¼ì— ì¶”ê°€
        if direction == "caller":
            recording["caller_buffer"].append(pcm_data)
            recording["caller_frames"] += 1
            
            # ì²« 10ê°œ íŒ¨í‚·ë§Œ ë””ë²„ê·¸ ë¡œê·¸
            if recording["caller_frames"] <= 10:
                logger.debug("Caller RTP packet added",
                            call_id=call_id,
                            frame=recording["caller_frames"],
                            pcm_size=len(pcm_data))
        elif direction == "callee":
            recording["callee_buffer"].append(pcm_data)
            recording["callee_frames"] += 1
            
            # ì²« 10ê°œ íŒ¨í‚·ë§Œ ë””ë²„ê·¸ ë¡œê·¸
            if recording["callee_frames"] <= 10:
                logger.debug("Callee RTP packet added",
                            call_id=call_id,
                            frame=recording["callee_frames"],
                            pcm_size=len(pcm_data))
    
    def _decode_g711_ulaw(self, ulaw_data: bytes) -> bytes:
        """
        G.711 Î¼-law â†’ PCM ë³€í™˜
        
        Args:
            ulaw_data: Î¼-law ì¸ì½”ë”©ëœ ë°ì´í„°
            
        Returns:
            PCM 16-bit ë°ì´í„°
        """
        import audioop
        try:
            return audioop.ulaw2lin(ulaw_data, 2)  # 2 = 16-bit
        except Exception as e:
            logger.error("G.711 Î¼-law decode error", error=str(e))
            return b''
    
    def _decode_g711_alaw(self, alaw_data: bytes) -> bytes:
        """
        G.711 A-law â†’ PCM ë³€í™˜
        
        Args:
            alaw_data: A-law ì¸ì½”ë”©ëœ ë°ì´í„°
            
        Returns:
            PCM 16-bit ë°ì´í„°
        """
        import audioop
        try:
            return audioop.alaw2lin(alaw_data, 2)  # 2 = 16-bit
        except Exception as e:
            logger.error("G.711 A-law decode error", error=str(e))
            return b''
    
    async def stop_recording(self, call_id: str) -> dict:
        """
        ë…¹ìŒ ì¤‘ì§€ ë° íŒŒì¼ ì €ì¥
        
        Args:
            call_id: í†µí™” ID
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ì •ë³´ dict
        """
        recording = self.active_recordings.pop(call_id, None)
        if not recording:
            logger.warning("No active recording", call_id=call_id)
            return {}
        
        end_time = datetime.now()
        duration = (end_time - recording["start_time"]).total_seconds()
        
        call_dir = recording["call_dir"]
        
        # íŒŒì¼ ê²½ë¡œ
        caller_path = call_dir / "caller.wav"
        callee_path = call_dir / "callee.wav"
        mixed_path = call_dir / "mixed.wav"
        metadata_path = call_dir / "metadata.json"
        transcript_path = call_dir / "transcript.txt"
        
        # WAV íŒŒì¼ ì €ì¥ (ë³‘ë ¬)
        await asyncio.gather(
            self._save_wav(caller_path, recording["caller_buffer"]),
            self._save_wav(callee_path, recording["callee_buffer"]),
            self._save_mixed_wav(mixed_path, recording)
        )
        
        # í›„ì²˜ë¦¬ STT ì‹¤í–‰ (ì„ íƒì , ë¹„ë™ê¸°)
        transcript_text = ""
        if self.enable_post_stt and mixed_path.exists():
            try:
                logger.info("stt_post_process_start",
                           category="stt",
                           progress="stt",
                           call_id=call_id,
                           audio_file=str(mixed_path),
                           diarization_enabled=self.enable_diarization)
                
                stt_result = await self._transcribe_audio(
                    mixed_path,
                    enable_diarization=self.enable_diarization
                )
                
                logger.info("stt_post_process_completed",
                           category="stt",
                           progress="stt",
                           call_id=call_id,
                           has_words=bool(stt_result.get("words")),
                           has_speakers=bool(stt_result.get("speakers")),
                           word_count=len(stt_result.get("words", [])))
                
                # í™”ìë³„ í¬ë§·íŒ…
                if self.enable_diarization and stt_result.get("words"):
                    logger.info("stt_diarization_format",
                               category="stt",
                               call_id=call_id)
                    
                    transcript_text = self._format_transcript_with_speakers(
                        stt_result["words"],
                        stt_result["speakers"]
                    )
                else:
                    transcript_text = stt_result.get("transcript", "")
                
                # transcript.txt ì €ì¥
                if transcript_text:
                    with open(transcript_path, 'w', encoding='utf-8') as f:
                        f.write(transcript_text)
                    
                    logger.info("stt_transcript_saved",
                               call=True,
                               category="stt",
                               call_id=call_id,
                               file_path=str(transcript_path),
                               transcript_length=len(transcript_text),
                               preview=transcript_text[:100] + "..." if len(transcript_text) > 100 else transcript_text)
                else:
                    logger.warning("stt_empty_transcript", call=True, category="stt", call_id=call_id)
                    
            except Exception as e:
                logger.error("âŒ [STT Flow] Post-processing STT error",
                            call_id=call_id,
                            error=str(e),
                            exc_info=True)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            "call_id": call_id,
            "directory": recording["dir_name"],  # âœ… ë””ë ‰í† ë¦¬ ì´ë¦„ ì¶”ê°€
            "caller_id": recording["caller_id"],
            "callee_id": recording["callee_id"],
            "start_time": recording["start_time"].isoformat(),
            "end_time": end_time.isoformat(),
            "duration": duration,
            "type": "sip_call",  # vs "ai_call"
            "sample_rate": self.sample_rate,
            "channels": self.channels,
            "caller_frames": recording["caller_frames"],
            "callee_frames": recording["callee_frames"],
            "has_transcript": transcript_path.exists(),
            "files": {
                "caller": str(caller_path.relative_to(self.output_dir)),
                "callee": str(callee_path.relative_to(self.output_dir)),
                "mixed": str(mixed_path.relative_to(self.output_dir)),
                "transcript": str(transcript_path.relative_to(self.output_dir)) if transcript_path.exists() else None
            }
        }
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)
        
        logger.info("SIP call recording stopped", 
                   call_id=call_id,
                   duration=duration,
                   caller_frames=recording["caller_frames"],
                   callee_frames=recording["callee_frames"],
                   has_transcript=metadata["has_transcript"])
        
        return metadata
    
    async def _save_wav(self, path: Path, buffer: list):
        """
        WAV íŒŒì¼ ì €ì¥
        
        Args:
            path: ì €ì¥ ê²½ë¡œ
            buffer: PCM ë°ì´í„° ë²„í¼
        """
        if not buffer:
            logger.warning("Empty buffer, skipping WAV save", path=str(path))
            return
        
        try:
            # ë²„í¼ ê²°í•©
            pcm_data = b''.join(buffer)
            
            # WAV íŒŒì¼ ìƒì„±
            with wave.open(str(path), 'wb') as wav_file:
                wav_file.setnchannels(self.channels)
                wav_file.setsampwidth(self.sample_width)
                wav_file.setframerate(self.sample_rate)
                wav_file.writeframes(pcm_data)
            
            logger.debug("WAV file saved", 
                        path=str(path),
                        size=len(pcm_data))
        except Exception as e:
            logger.error("WAV save error", 
                        path=str(path),
                        error=str(e),
                        exc_info=True)
    
    async def _save_mixed_wav(self, path: Path, recording: dict):
        """
        ë¯¹ì‹±ëœ WAV íŒŒì¼ ì €ì¥ (caller + callee)
        
        Args:
            path: ì €ì¥ ê²½ë¡œ
            recording: ë…¹ìŒ ì„¸ì…˜ ì •ë³´
        """
        caller_buffer = recording["caller_buffer"]
        callee_buffer = recording["callee_buffer"]
        
        # âœ… ë‘˜ ë‹¤ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
        if not caller_buffer and not callee_buffer:
            logger.warning("Empty buffers, skipping mixed WAV", path=str(path))
            return
        
        try:
            # âœ… í•œìª½ë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not caller_buffer:
                logger.info("Only callee audio available, using callee only", path=str(path))
                mixed_data = b''.join(callee_buffer)
            elif not callee_buffer:
                logger.info("Only caller audio available, using caller only", path=str(path))
                mixed_data = b''.join(caller_buffer)
            else:
                # âœ… ë‘˜ ë‹¤ ìˆëŠ” ê²½ìš° ë¯¹ì‹±
                caller_data = b''.join(caller_buffer)
                callee_data = b''.join(callee_buffer)
                
                logger.info("Mixing caller and callee audio",
                           caller_size=len(caller_data),
                           callee_size=len(callee_data),
                           caller_frames=len(caller_buffer),
                           callee_frames=len(callee_buffer))
                
                mixed_data = self._mix_audio(caller_data, callee_data)
            
            # âœ… ë¯¹ì‹±ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬
            if not mixed_data:
                logger.error("Mixed data is empty after processing", path=str(path))
                return
            
            # WAV íŒŒì¼ ìƒì„±
            with wave.open(str(path), 'wb') as wav_file:
                wav_file.setnchannels(self.channels)
                wav_file.setsampwidth(self.sample_width)
                wav_file.setframerate(self.sample_rate)
                wav_file.writeframes(mixed_data)
            
            logger.info("Mixed WAV file saved", 
                        path=str(path),
                        size=len(mixed_data),
                        duration_sec=len(mixed_data) / (self.sample_rate * self.sample_width))
        except Exception as e:
            logger.error("Mixed WAV save error", 
                        path=str(path),
                        error=str(e),
                        exc_info=True)
    
    def _mix_audio(self, audio1: bytes, audio2: bytes) -> bytes:
        """
        ë‘ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë¯¹ì‹± (í‰ê· )
        
        Args:
            audio1: ì²« ë²ˆì§¸ PCM ë°ì´í„°
            audio2: ë‘ ë²ˆì§¸ PCM ë°ì´í„°
            
        Returns:
            ë¯¹ì‹±ëœ PCM ë°ì´í„°
        """
        # âœ… ê¸¸ì´ ë§ì¶”ê¸° (ê¸´ ìª½ì„ ì§§ì€ ìª½ì— ë§ì¶¤, ë˜ëŠ” ê¸´ ìª½ ìœ ì§€)
        max_len = max(len(audio1), len(audio2))
        
        # âœ… ì§§ì€ ìª½ì„ silence(0)ë¡œ íŒ¨ë”©
        if len(audio1) < max_len:
            audio1 = audio1 + (b'\x00' * (max_len - len(audio1)))
        if len(audio2) < max_len:
            audio2 = audio2 + (b'\x00' * (max_len - len(audio2)))
        
        # âœ… 16-bit PCMìœ¼ë¡œ ì–¸íŒ© (ê¸¸ì´ê°€ í™€ìˆ˜ë©´ ë§ˆì§€ë§‰ ë°”ì´íŠ¸ ì œê±°)
        if len(audio1) % 2 != 0:
            audio1 = audio1[:-1]
        if len(audio2) % 2 != 0:
            audio2 = audio2[:-1]
        
        try:
            samples1 = struct.unpack(f'{len(audio1)//2}h', audio1)
            samples2 = struct.unpack(f'{len(audio2)//2}h', audio2)
            
            # âœ… í‰ê·  ë¯¹ì‹± (clipping ë°©ì§€)
            mixed_samples = []
            for s1, s2 in zip(samples1, samples2):
                mixed = (s1 + s2) // 2
                # Clipping ë°©ì§€ (-32768 ~ 32767)
                mixed = max(-32768, min(32767, mixed))
                mixed_samples.append(mixed)
            
            # ë‹¤ì‹œ íŒ¨í‚¹
            mixed_data = struct.pack(f'{len(mixed_samples)}h', *mixed_samples)
            
            return mixed_data
            
        except struct.error as e:
            logger.error("Audio mixing struct error",
                        audio1_len=len(audio1),
                        audio2_len=len(audio2),
                        error=str(e))
            # ë¯¹ì‹± ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ë§Œ ë°˜í™˜
            return audio1
    
    def is_recording(self, call_id: str) -> bool:
        """
        í†µí™”ê°€ ë…¹ìŒ ì¤‘ì¸ì§€ í™•ì¸
        
        Args:
            call_id: í†µí™” ID
            
        Returns:
            ë…¹ìŒ ì¤‘ì´ë©´ True
        """
        return call_id in self.active_recordings
    
    def get_recording_duration(self, call_id: str) -> float:
        """
        í˜„ì¬ ë…¹ìŒ ì‹œê°„ ì¡°íšŒ
        
        Args:
            call_id: í†µí™” ID
            
        Returns:
            ë…¹ìŒ ì‹œê°„ (ì´ˆ)
        """
        recording = self.active_recordings.get(call_id)
        if not recording:
            return 0.0
        
        return (datetime.now() - recording["start_time"]).total_seconds()
    
    def _init_stt_client(self):
        """
        Google Speech-to-Text í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        """
        try:
            from google.cloud import speech
            import os
            
            # âœ… ë¹ ë¥¸ ì‹¤íŒ¨: ì¸ì¦ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
            if not self.gcp_credentials_path:
                logger.warning("Google Cloud credentials path not provided, post-processing STT disabled")
                self.enable_post_stt = False
                return
            
            if not os.path.exists(self.gcp_credentials_path):
                logger.warning("Google Cloud credentials file not found, post-processing STT disabled", 
                             path=self.gcp_credentials_path)
                self.enable_post_stt = False
                return
            
            # GCP ì¸ì¦ ì„¤ì •
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = self.gcp_credentials_path
            
            # âœ… íƒ€ì„ì•„ì›ƒ ì„¤ì • (5ì´ˆ)
            import socket
            old_timeout = socket.getdefaulttimeout()
            socket.setdefaulttimeout(5.0)
            
            try:
                self.stt_client = speech.SpeechClient()
                logger.info("Google Speech-to-Text client initialized")
            finally:
                socket.setdefaulttimeout(old_timeout)
                
        except ImportError:
            logger.warning("google-cloud-speech not installed, post-processing STT disabled")
            self.enable_post_stt = False
        except Exception as e:
            logger.error("Failed to initialize STT client", error=str(e))
            self.enable_post_stt = False
    
    async def _transcribe_audio(
        self, 
        audio_path: Path,
        enable_diarization: bool = True
    ) -> Dict[str, any]:
        """
        WAV íŒŒì¼ì„ STTë¡œ ì „ì‚¬ (í›„ì²˜ë¦¬)
        
        ì „í™” í†µí™”ì˜ ê²½ìš°, caller.wavì™€ callee.wavë¥¼ ê°ê° STT ì²˜ë¦¬í•˜ì—¬ ê²°í•©í•©ë‹ˆë‹¤.
        ì´ ë°©ë²•ì´ mixed.wavì˜ í™”ì ë¶„ë¦¬ë³´ë‹¤ í›¨ì”¬ ì •í™•í•©ë‹ˆë‹¤.
        
        Args:
            audio_path: WAV íŒŒì¼ ê²½ë¡œ (mixed.wav)
            enable_diarization: í™”ì ë¶„ë¦¬ í™œì„±í™” (caller/callee ê°œë³„ ì²˜ë¦¬ ì‹œ ë¬´ì‹œë¨)
            
        Returns:
            {
                "transcript": "ì „ì²´ ì „ì‚¬ í…ìŠ¤íŠ¸",
                "words": [{"word": "ë‹¨ì–´", "speaker_tag": 1, "start_time": 0.0, "end_time": 0.5}],
                "speakers": {1: "caller", 2: "callee"}
            }
        """
        if not self.stt_client:
            logger.warning("STT client not initialized")
            return {"transcript": "", "words": [], "speakers": {}}
        
        # â­ Caller/Callee ê°œë³„ íŒŒì¼ì´ ìˆìœ¼ë©´ ê°ê° STT ì²˜ë¦¬ (ë” ì •í™•í•¨)
        caller_path = audio_path.parent / "caller.wav"
        callee_path = audio_path.parent / "callee.wav"
        
        if caller_path.exists() and callee_path.exists():
            logger.info("ğŸ¯ [STT Flow] Using separate caller/callee STT (more accurate)",
                       caller_path=str(caller_path),
                       callee_path=str(callee_path))
            return await self._transcribe_separate_channels(caller_path, callee_path)
        
        # Mixed íŒŒì¼ë§Œ ìˆìœ¼ë©´ diarization ì‚¬ìš© (ëœ ì •í™•í•¨)
        logger.info("âš ï¸ [STT Flow] Using mixed audio with diarization (less accurate)",
                   audio_path=str(audio_path))
        
        try:
            from google.cloud import speech
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
            with open(audio_path, 'rb') as audio_file:
                audio_content = audio_file.read()
            
            # Speech-to-Text ì„¤ì •
            audio = speech.RecognitionAudio(content=audio_content)
            
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=self.sample_rate,
                language_code=self.stt_language,
                enable_automatic_punctuation=True,
                enable_word_time_offsets=True,
                # í™”ì ë¶„ë¦¬ ì„¤ì •
                diarization_config=speech.SpeakerDiarizationConfig(
                    enable_speaker_diarization=enable_diarization,
                    min_speaker_count=2,
                    max_speaker_count=2,
                ) if enable_diarization else None,
                model="telephony",  # ì „í™” í†µí™” ìµœì í™” ëª¨ë¸
            )
            
            logger.info("Starting STT transcription", 
                       audio_path=str(audio_path),
                       file_size=len(audio_content))
            
            # STT ì‹¤í–‰ (ë™ê¸° â†’ ë¹„ë™ê¸° ë˜í•‘)
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.stt_client.recognize(config=config, audio=audio)
            )
            
            # ê²°ê³¼ íŒŒì‹±
            transcript_parts = []
            words_with_speakers = []
            
            logger.info("ğŸ“Š [STT Debug] Parsing response", 
                       results_count=len(response.results))
            
            # â­ Diarization ê²°ê³¼ëŠ” ë§ˆì§€ë§‰ resultì— ìˆìŒ
            # Google STT APIëŠ” í™”ì ë¶„ë¦¬ ì •ë³´ë¥¼ ë§ˆì§€ë§‰ resultì— í†µí•©í•˜ì—¬ ë°˜í™˜
            for idx, result in enumerate(response.results):
                alternative = result.alternatives[0]
                transcript_parts.append(alternative.transcript)
            
            # â­ í™”ì ë¶„ë¦¬ê°€ í™œì„±í™”ëœ ê²½ìš°, ë§ˆì§€ë§‰ resultì—ì„œ ë‹¨ì–´ë³„ í™”ì ì •ë³´ ì¶”ì¶œ
            if enable_diarization and response.results:
                last_result = response.results[-1]
                last_alternative = last_result.alternatives[0]
                
                logger.info(f"ğŸ“Š [STT Debug] Last result analysis",
                           has_words=hasattr(last_alternative, 'words'),
                           words_count=len(last_alternative.words) if hasattr(last_alternative, 'words') else 0)
                
                if hasattr(last_alternative, 'words'):
                    # ì²˜ìŒ 10ê°œ ë‹¨ì–´ì˜ speaker_tag í™•ì¸
                    sample_words = []
                    for i, word_info in enumerate(last_alternative.words[:10]):
                        has_tag = hasattr(word_info, 'speaker_tag')
                        tag = word_info.speaker_tag if has_tag else None
                        sample_words.append({
                            "word": word_info.word,
                            "has_speaker_tag": has_tag,
                            "speaker_tag": tag
                        })
                    
                    logger.info(f"ğŸ“Š [STT Debug] Sample words (first 10)", 
                               sample_words=sample_words[:5])
                    
                    # ëª¨ë“  ë‹¨ì–´ ì¶”ì¶œ
                    for word_info in last_alternative.words:
                        words_with_speakers.append({
                            "word": word_info.word,
                            "speaker_tag": word_info.speaker_tag if hasattr(word_info, 'speaker_tag') else 1,
                            "start_time": word_info.start_time.total_seconds() if hasattr(word_info.start_time, 'total_seconds') else 0.0,
                            "end_time": word_info.end_time.total_seconds() if hasattr(word_info.end_time, 'total_seconds') else 0.0,
                        })
                    
                    # í™”ìë³„ ë‹¨ì–´ ìˆ˜ ì¹´ìš´íŠ¸
                    speaker_counts = {}
                    for w in words_with_speakers:
                        tag = w.get("speaker_tag", 1)
                        speaker_counts[tag] = speaker_counts.get(tag, 0) + 1
                    
                    logger.info("ğŸ“Š [STT Debug] Speaker distribution",
                               speaker_counts=speaker_counts)
            
            full_transcript = ' '.join(transcript_parts)
            
            # í™”ì ë§¤í•‘ (ê°€ì •: speaker_tag 1 = caller, 2 = callee)
            speakers = {1: "caller", 2: "callee"} if enable_diarization else {}
            
            logger.info("STT transcription completed",
                       progress="stt",
                       audio_path=str(audio_path),
                       transcript_length=len(full_transcript),
                       words_count=len(words_with_speakers),
                       speaker_tags_present=len(set(w.get("speaker_tag", 1) for w in words_with_speakers)) if words_with_speakers else 0,
                       first_10_speakers=[w.get("speaker_tag", 1) for w in words_with_speakers[:10]] if words_with_speakers else [])
            
            return {
                "transcript": full_transcript,
                "words": words_with_speakers,
                "speakers": speakers
            }
            
        except Exception as e:
            logger.error("STT transcription error",
                        progress="stt",
                        audio_path=str(audio_path),
                        error=str(e),
                        exc_info=True)
            return {"transcript": "", "words": [], "speakers": {}}
    
    def _format_transcript_with_speakers(
        self, 
        words: List[Dict],
        speakers: Dict[int, str]
    ) -> str:
        """
        í™”ìë³„ë¡œ ì „ì‚¬ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        
        Args:
            words: ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (speaker_tag í¬í•¨)
            speakers: í™”ì ë§¤í•‘
            
        Returns:
            í¬ë§·íŒ…ëœ ì „ì‚¬ í…ìŠ¤íŠ¸
            ì˜ˆ: "ë°œì‹ ì: ì•ˆë…•í•˜ì„¸ìš”\nì°©ì‹ ì: ë„¤ ì•ˆë…•í•˜ì„¸ìš”"
        """
        if not words:
            return ""
        
        # í™”ìë³„ë¡œ ê·¸ë£¹í™”
        current_speaker = None
        transcript_lines = []
        current_line = []
        
        # Google STTê°€ ë°˜í™˜í•˜ëŠ” ì„œë¸Œì›Œë“œ ë§ˆì»¤(â– U+2581) ì œê±° â€” transcript ê°€ë…ì„±
        def _norm(w: str) -> str:
            return (w or "").replace("\u2581", "").strip()

        for word_info in words:
            speaker_tag = word_info.get("speaker_tag", 1)
            word = _norm(word_info.get("word", ""))
            
            if speaker_tag != current_speaker:
                # í™”ì ë³€ê²½ ì‹œ ìƒˆë¡œìš´ ë¼ì¸ ì‹œì‘
                if current_line:
                    speaker_label = self._get_speaker_label(current_speaker, speakers)
                    transcript_lines.append(f"{speaker_label}: {' '.join(current_line)}")
                    current_line = []
                
                current_speaker = speaker_tag
            
            if word:
                current_line.append(word)
        
        # ë§ˆì§€ë§‰ ë¼ì¸ ì¶”ê°€
        if current_line:
            speaker_label = self._get_speaker_label(current_speaker, speakers)
            transcript_lines.append(f"{speaker_label}: {' '.join(current_line)}")
        
        return '\n'.join(transcript_lines)
    
    def _get_speaker_label(self, speaker_tag: int, speakers: Dict[int, str]) -> str:
        """
        í™”ì íƒœê·¸ë¥¼ í•œê¸€ ë ˆì´ë¸”ë¡œ ë³€í™˜
        
        Args:
            speaker_tag: í™”ì íƒœê·¸ (1, 2, ...)
            speakers: í™”ì ë§¤í•‘
            
        Returns:
            "ë°œì‹ ì" | "ì°©ì‹ ì"
        """
        speaker_role = speakers.get(speaker_tag, "caller")
        return "ë°œì‹ ì" if speaker_role == "caller" else "ì°©ì‹ ì"
    
    async def _transcribe_separate_channels(
        self,
        caller_path: Path,
        callee_path: Path
    ) -> Dict[str, any]:
        """
        Callerì™€ Calleeë¥¼ ê°ê° STT ì²˜ë¦¬í•˜ì—¬ ê²°í•©
        
        ì´ ë°©ë²•ì´ mixed audioì˜ diarizationë³´ë‹¤ í›¨ì”¬ ì •í™•í•©ë‹ˆë‹¤.
        
        Args:
            caller_path: caller.wav ê²½ë¡œ
            callee_path: callee.wav ê²½ë¡œ
            
        Returns:
            {
                "transcript": "ì „ì²´ ì „ì‚¬ í…ìŠ¤íŠ¸",
                "words": [{"word": "ë‹¨ì–´", "speaker_tag": 1 or 2, "start_time": 0.0, "end_time": 0.5}],
                "speakers": {1: "caller", 2: "callee"}
            }
        """
        try:
            from google.cloud import speech
            
            # Caller STT
            logger.info("ğŸ“ [STT Flow] Transcribing caller audio", path=str(caller_path))
            caller_words = await self._transcribe_single_channel(
                caller_path, 
                speaker_tag=1,  # Caller = speaker 1
                speaker_role="caller"
            )
            
            # Callee STT
            logger.info("ğŸ“ [STT Flow] Transcribing callee audio", path=str(callee_path))
            callee_words = await self._transcribe_single_channel(
                callee_path,
                speaker_tag=2,  # Callee = speaker 2
                speaker_role="callee"
            )
            
            # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            all_words = caller_words + callee_words
            all_words.sort(key=lambda w: w.get("start_time", 0.0))
            
            # ì „ì²´ ì „ì‚¬ í…ìŠ¤íŠ¸ ìƒì„±
            transcript = ' '.join([w.get("word", "") for w in all_words])
            
            # í™”ìë³„ ë‹¨ì–´ ìˆ˜ ì¹´ìš´íŠ¸
            speaker_counts = {}
            for w in all_words:
                tag = w.get("speaker_tag", 1)
                speaker_counts[tag] = speaker_counts.get(tag, 0) + 1
            
            logger.info("âœ… [STT Flow] Separate channel transcription completed",
                       total_words=len(all_words),
                       caller_words=len(caller_words),
                       callee_words=len(callee_words),
                       speaker_distribution=speaker_counts)
            
            return {
                "transcript": transcript,
                "words": all_words,
                "speakers": {1: "caller", 2: "callee"}
            }
            
        except Exception as e:
            logger.error("Separate channel transcription failed", error=str(e), exc_info=True)
            return {"transcript": "", "words": [], "speakers": {}}
    
    async def _transcribe_single_channel(
        self,
        audio_path: Path,
        speaker_tag: int,
        speaker_role: str
    ) -> List[Dict]:
        """
        ë‹¨ì¼ ì±„ë„(caller ë˜ëŠ” callee) STT ì²˜ë¦¬
        
        Args:
            audio_path: WAV íŒŒì¼ ê²½ë¡œ
            speaker_tag: í™”ì íƒœê·¸ (1=caller, 2=callee)
            speaker_role: í™”ì ì—­í•  ("caller" or "callee")
            
        Returns:
            ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ [{"word": "ë‹¨ì–´", "speaker_tag": 1, "start_time": 0.0, "end_time": 0.5}]
        """
        try:
            from google.cloud import speech
            import os
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´
            file_size = os.path.getsize(audio_path)
            duration_sec = file_size / (self.sample_rate * 2)  # 16-bit = 2 bytes
            
            logger.info(f"ğŸ“Š [STT] Audio file info",
                       audio_path=str(audio_path),
                       file_size=file_size,
                       duration_sec=round(duration_sec, 2))
            
            # 1ë¶„ ì´ìƒì´ë©´ long_running_recognize ì‚¬ìš©
            if duration_sec > 60:
                logger.info("ğŸ”„ [STT] Using LongRunningRecognize for audio > 60s",
                           duration=round(duration_sec, 2))
                return await self._transcribe_long_audio(audio_path, speaker_tag, speaker_role)
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
            with open(audio_path, 'rb') as audio_file:
                audio_content = audio_file.read()
            
            # Speech-to-Text ì„¤ì • (diarization ì—†ì´)
            audio = speech.RecognitionAudio(content=audio_content)
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=self.sample_rate,
                language_code=self.stt_language,
                enable_automatic_punctuation=True,
                enable_word_time_offsets=True,
                model="telephony",  # ì „í™” í†µí™” ìµœì í™” ëª¨ë¸
            )
            
            # STT ì‹¤í–‰
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.stt_client.recognize(config=config, audio=audio)
            )
            
            # ê²°ê³¼ íŒŒì‹±
            words = []
            for result in response.results:
                alternative = result.alternatives[0]
                if hasattr(alternative, 'words'):
                    for word_info in alternative.words:
                        words.append({
                            "word": word_info.word,
                            "speaker_tag": speaker_tag,  # Caller=1, Callee=2
                            "start_time": word_info.start_time.total_seconds() if hasattr(word_info.start_time, 'total_seconds') else 0.0,
                            "end_time": word_info.end_time.total_seconds() if hasattr(word_info.end_time, 'total_seconds') else 0.0,
                        })
            
            logger.info(f"âœ… [{speaker_role.upper()}] STT completed",
                       words_count=len(words),
                       audio_path=str(audio_path))
            
            return words
            
        except Exception as e:
            logger.error(f"Single channel transcription failed for {speaker_role}",
                        audio_path=str(audio_path),
                        error=str(e),
                        exc_info=True)
            return []
    
    async def _transcribe_long_audio(
        self,
        audio_path: Path,
        speaker_tag: int,
        speaker_role: str
    ) -> List[Dict]:
        """
        ê¸´ ì˜¤ë””ì˜¤(1ë¶„ ì´ìƒ)ë¥¼ 60ì´ˆ ë‹¨ìœ„ë¡œ chunk ì²˜ë¦¬
        
        Args:
            audio_path: WAV íŒŒì¼ ê²½ë¡œ
            speaker_tag: í™”ì íƒœê·¸ (1=caller, 2=callee)
            speaker_role: í™”ì ì—­í•  ("caller" or "callee")
            
        Returns:
            ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            from google.cloud import speech
            import wave
            
            all_words = []
            
            with wave.open(str(audio_path), 'rb') as wav_file:
                framerate = wav_file.getframerate()
                n_frames = wav_file.getnframes()
                
                # 60ì´ˆ ë‹¨ìœ„ë¡œ chunk ì²˜ë¦¬
                chunk_duration = 60  # seconds
                chunk_frames = int(framerate * chunk_duration)
                
                time_offset = 0.0
                chunk_num = 0
                
                while wav_file.tell() < n_frames:
                    chunk_num += 1
                    chunk_data = wav_file.readframes(min(chunk_frames, n_frames - wav_file.tell()))
                    
                    if not chunk_data:
                        break
                    
                    logger.info(f"ğŸ”„ [STT] Processing chunk {chunk_num}",
                               chunk_start=round(time_offset, 2),
                               speaker=speaker_role)
                    
                    # STT API í˜¸ì¶œ
                    audio = speech.RecognitionAudio(content=chunk_data)
                    config = speech.RecognitionConfig(
                        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                        sample_rate_hertz=framerate,
                        language_code=self.stt_language,
                        enable_automatic_punctuation=True,
                        enable_word_time_offsets=True,
                        model="telephony",
                    )
                    
                    loop = asyncio.get_event_loop()
                    response = await loop.run_in_executor(
                        None,
                        lambda c=chunk_data: self.stt_client.recognize(config=config, audio=speech.RecognitionAudio(content=c))
                    )
                    
                    # ê²°ê³¼ ìˆ˜ì§‘ (time offset ì¡°ì •)
                    for result in response.results:
                        alternative = result.alternatives[0]
                        if hasattr(alternative, 'words'):
                            for word_info in alternative.words:
                                all_words.append({
                                    "word": word_info.word,
                                    "speaker_tag": speaker_tag,
                                    "start_time": time_offset + (word_info.start_time.total_seconds() if hasattr(word_info.start_time, 'total_seconds') else 0.0),
                                    "end_time": time_offset + (word_info.end_time.total_seconds() if hasattr(word_info.end_time, 'total_seconds') else 0.0)
                                })
                    
                    time_offset += chunk_duration
                
                logger.info(f"âœ… [STT] Long audio completed",
                           speaker=speaker_role,
                           chunks=chunk_num,
                           total_words=len(all_words))
                
                return all_words
                
        except Exception as e:
            logger.error(f"Long audio transcription failed for {speaker_role}",
                        audio_path=str(audio_path),
                        error=str(e),
                        exc_info=True)
            return []

