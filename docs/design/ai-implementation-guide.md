# AI ì»´í¬ë„ŒíŠ¸ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ë¬¸ì„œ ì •ë³´

ì´ ë¬¸ì„œëŠ” AI ì•„í‚¤í…ì²˜ ë¬¸ì„œì—ì„œ ì¸í„°í˜ì´ìŠ¤ë§Œ ì •ì˜ëœ 8ê°œ ì»´í¬ë„ŒíŠ¸ì˜ ìƒì„¸í•œ êµ¬í˜„ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**ëŒ€ìƒ ì»´í¬ë„ŒíŠ¸:**
1. Audio Buffer & Jitter âœ…
2. VAD Detector âœ…
3. STT Client (Google gRPC)
4. TTS Client (Google gRPC)
5. LLM Client (Gemini)
6. RAG Engine
7. Call Recorder
8. Knowledge Extractor

---

## 1. Audio Buffer & Jitter âœ…

### 1.1 ì±…ì„ (Responsibility)
- RTP íŒ¨í‚· (UDP) â†’ gRPC ìŠ¤íŠ¸ë¦¼ (TCP) ë³€í™˜
- Jitter ë²„í¼ë§ (20-60ms)
- íŒ¨í‚· ìˆœì„œ ì¬ì •ë ¬
- ìƒ˜í”Œë ˆì´íŠ¸ ë³€í™˜ (8kHz â†’ 16kHz)

### 1.2 ì™„ì „í•œ êµ¬í˜„

íŒŒì¼ ìœ„ì¹˜: `src/ai_voicebot/audio_buffer.py`

```python
import asyncio
import audioop
from collections import deque
from dataclasses import dataclass
from typing import Optional
import structlog

logger = structlog.get_logger(__name__)


@dataclass
class AudioFrame:
    """ì˜¤ë””ì˜¤ í”„ë ˆì„ ë°ì´í„°"""
    sequence: int
    timestamp: int
    payload: bytes
    sample_rate: int = 8000


class AudioBuffer:
    """
    RTP íŒ¨í‚·ì„ ë²„í¼ë§í•˜ê³  gRPC ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Features:
    - Jitter buffering (íŒ¨í‚· ì§€ì—° ë³´ì •)
    - Packet reordering (ìˆœì„œ ì¬ì •ë ¬)
    - Sample rate conversion (8kHz â†’ 16kHz)
    - Packet loss detection
    """
    
    def __init__(
        self, 
        jitter_buffer_ms: int = 60,
        max_buffer_size: int = 100,
        target_sample_rate: int = 16000
    ):
        self.jitter_buffer_ms = jitter_buffer_ms
        self.max_buffer_size = max_buffer_size
        self.target_sample_rate = target_sample_rate
        
        self.buffer: deque[AudioFrame] = deque(maxlen=max_buffer_size)
        self.packets_received = 0
        self.packets_dropped = 0
        self.packets_reordered = 0
        self.last_sequence = -1
        
        self.output_queue: asyncio.Queue[bytes] = asyncio.Queue(maxsize=50)
        self._buffering_task: Optional[asyncio.Task] = None
        self._running = False
        
    async def start(self):
        """ë²„í¼ë§ íƒœìŠ¤í¬ ì‹œì‘"""
        if self._running:
            return
            
        self._running = True
        self._buffering_task = asyncio.create_task(self._buffer_worker())
        logger.info("AudioBuffer started")
    
    async def stop(self):
        """ë²„í¼ë§ íƒœìŠ¤í¬ ì¤‘ì§€"""
        self._running = False
        if self._buffering_task:
            self._buffering_task.cancel()
            try:
                await self._buffering_task
            except asyncio.CancelledError:
                pass
    
    async def add_packet(self, rtp_packet) -> None:
        """RTP íŒ¨í‚·ì„ ë²„í¼ì— ì¶”ê°€"""
        self.packets_received += 1
        
        frame = AudioFrame(
            sequence=rtp_packet.sequence,
            timestamp=rtp_packet.timestamp,
            payload=rtp_packet.payload,
            sample_rate=rtp_packet.sample_rate or 8000
        )
        
        # íŒ¨í‚· ì†ì‹¤ ê°ì§€
        if self.last_sequence >= 0:
            expected_seq = (self.last_sequence + 1) % 65536
            if frame.sequence != expected_seq:
                gap = (frame.sequence - expected_seq) % 65536
                self.packets_dropped += gap
                logger.warning("Packet loss", gap=gap)
        
        self._insert_sorted(frame)
        self.last_sequence = frame.sequence
    
    def _insert_sorted(self, frame: AudioFrame) -> None:
        """ë²„í¼ì— sequence number ìˆœì„œë¡œ ì‚½ì…"""
        if not self.buffer or frame.sequence > self.buffer[-1].sequence:
            self.buffer.append(frame)
            return
        
        for i, buffered_frame in enumerate(self.buffer):
            if frame.sequence < buffered_frame.sequence:
                self.buffer.insert(i, frame)
                self.packets_reordered += 1
                return
    
    async def _buffer_worker(self):
        """ë²„í¼ ì›Œì»¤ íƒœìŠ¤í¬"""
        while self._running:
            try:
                await asyncio.sleep(self.jitter_buffer_ms / 1000.0)
                
                if not self.buffer:
                    continue
                
                frame = self.buffer.popleft()
                converted = self._convert_sample_rate(
                    frame.payload,
                    frame.sample_rate,
                    self.target_sample_rate
                )
                
                try:
                    self.output_queue.put_nowait(converted)
                except asyncio.QueueFull:
                    logger.warning("Output queue full")
                    
            except Exception as e:
                logger.error("Buffer worker error", error=str(e))
    
    def _convert_sample_rate(
        self, 
        audio_data: bytes, 
        from_rate: int, 
        to_rate: int
    ) -> bytes:
        """ìƒ˜í”Œë ˆì´íŠ¸ ë³€í™˜"""
        if from_rate == to_rate:
            return audio_data
        
        try:
            converted, _ = audioop.ratecv(
                audio_data, 2, 1, from_rate, to_rate, None
            )
            return converted
        except Exception as e:
            logger.error("Sample rate conversion failed", error=str(e))
            return audio_data
    
    async def get_frame(self, timeout: float = 0.1) -> Optional[bytes]:
        """ë³€í™˜ëœ ì˜¤ë””ì˜¤ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return await asyncio.wait_for(
                self.output_queue.get(), 
                timeout=timeout
            )
        except asyncio.TimeoutError:
            return None
```

### 1.3 ì‚¬ìš© ì˜ˆì‹œ

```python
# ì´ˆê¸°í™”
buffer = AudioBuffer(
    jitter_buffer_ms=60,
    target_sample_rate=16000
)
await buffer.start()

# RTP íŒ¨í‚· ì¶”ê°€
await buffer.add_packet(rtp_packet)

# í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
frame = await buffer.get_frame()
if frame:
    # STTë¡œ ì „ì†¡
    await stt_client.send_audio(frame)

await buffer.stop()
```

---

## 2. VAD Detector âœ…

### 2.1 ì™„ì „í•œ êµ¬í˜„

íŒŒì¼ ìœ„ì¹˜: `src/ai_voicebot/vad_detector.py`

```python
import webrtcvad
from collections import deque
import structlog

logger = structlog.get_logger(__name__)


class VADDetector:
    """
    Voice Activity Detector (ìŒì„± í™œë™ ê°ì§€ê¸°)
    WebRTC VAD ê¸°ë°˜ Barge-in ì§€ì›
    """
    
    def __init__(
        self,
        mode: int = 3,  # 0-3, 3ì´ ê°€ì¥ ë¯¼ê°
        sample_rate: int = 16000,
        frame_duration_ms: int = 30,
        trigger_threshold: float = 0.5,
        speech_frame_count: int = 3
    ):
        if mode not in [0, 1, 2, 3]:
            raise ValueError("VAD mode must be 0-3")
        
        if sample_rate not in [8000, 16000, 32000, 48000]:
            raise ValueError("Invalid sample rate")
        
        self.vad = webrtcvad.Vad(mode)
        self.sample_rate = sample_rate
        self.frame_duration_ms = frame_duration_ms
        self.trigger_threshold = trigger_threshold
        self.speech_frame_count = speech_frame_count
        
        # í”„ë ˆì„ í¬ê¸° (bytes): sample_rate * duration / 1000 * 2
        self.frame_size = int(sample_rate * frame_duration_ms / 1000 * 2)
        
        self.recent_frames = deque(maxlen=10)
        self.consecutive_speech = 0
        
        logger.info("VADDetector initialized", mode=mode)
    
    def detect(self, audio_frame: bytes) -> bool:
        """ìŒì„± ê°ì§€"""
        # í”„ë ˆì„ í¬ê¸° ë§ì¶”ê¸°
        if len(audio_frame) != self.frame_size:
            if len(audio_frame) < self.frame_size:
                audio_frame = audio_frame + b'\x00' * (self.frame_size - len(audio_frame))
            else:
                audio_frame = audio_frame[:self.frame_size]
        
        try:
            is_speech = self.vad.is_speech(audio_frame, self.sample_rate)
            
            if is_speech:
                self.consecutive_speech += 1
            else:
                self.consecutive_speech = 0
            
            self.recent_frames.append(is_speech)
            return is_speech
            
        except Exception as e:
            logger.error("VAD detection failed", error=str(e))
            return False
    
    def is_speaking(self) -> bool:
        """í˜„ì¬ ë°œí™” ì¤‘ì¸ì§€ (Barge-in íŠ¸ë¦¬ê±°ìš©)"""
        return self.consecutive_speech >= self.speech_frame_count
    
    def get_speech_ratio(self) -> float:
        """ìµœê·¼ ìœˆë„ìš° ìŒì„± ë¹„ìœ¨"""
        if not self.recent_frames:
            return 0.0
        speech_count = sum(1 for is_speech in self.recent_frames if is_speech)
        return speech_count / len(self.recent_frames)
    
    def is_barge_in(self) -> bool:
        """Barge-in ì¡°ê±´ ë§Œì¡± ì—¬ë¶€"""
        if not self.is_speaking():
            return False
        
        speech_ratio = self.get_speech_ratio()
        return speech_ratio >= self.trigger_threshold
    
    def reset(self):
        """VAD ìƒíƒœ ì´ˆê¸°í™”"""
        self.recent_frames.clear()
        self.consecutive_speech = 0
```

### 2.2 ì‚¬ìš© ì˜ˆì‹œ

```python
# ì´ˆê¸°í™”
vad = VADDetector(
    mode=3,
    sample_rate=16000,
    trigger_threshold=0.6,
    speech_frame_count=3
)

# ì˜¤ë””ì˜¤ í”„ë ˆì„ ê°ì§€
is_speech = vad.detect(audio_frame)

# Barge-in í™•ì¸
if vad.is_barge_in():
    # TTS ì¬ìƒ ì¤‘ë‹¨
    await orchestrator.stop_speaking()
```

---

## 3. STT Client (Google gRPC) ğŸ†•

### 3.1 ì™„ì „í•œ êµ¬í˜„

íŒŒì¼ ìœ„ì¹˜: `src/ai_voicebot/ai_pipeline/stt_client.py`

```python
from google.cloud import speech
import asyncio
from typing import Optional, Callable
import structlog

logger = structlog.get_logger(__name__)


class STTClient:
    """
    Google Cloud Speech-to-Text gRPC Streaming Client
    
    ì‹¤ì‹œê°„ ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: dict):
        """
        Args:
            config: STT ì„¤ì •
                - model: "telephony" | "latest_long"
                - language_code: "ko-KR"
                - sample_rate: 16000
                - enable_enhanced: True
        """
        self.config = config
        self.client = speech.SpeechClient()
        
        # ì„¤ì •
        self.recognition_config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=config.get("sample_rate", 16000),
            language_code=config.get("language_code", "ko-KR"),
            model=config.get("model", "telephony"),
            use_enhanced=config.get("enable_enhanced", True),
            enable_automatic_punctuation=True,
            enable_word_time_offsets=False,
        )
        
        self.streaming_config = speech.StreamingRecognitionConfig(
            config=self.recognition_config,
            interim_results=True,  # ì¤‘ê°„ ê²°ê³¼
            single_utterance=False,  # ì—°ì† ì¸ì‹
        )
        
        # ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ
        self.audio_queue: Optional[asyncio.Queue] = None
        self.result_callback: Optional[Callable] = None
        self._streaming_task: Optional[asyncio.Task] = None
        self._running = False
        
        logger.info("STTClient initialized", 
                   model=config.get("model"),
                   language=config.get("language_code"))
    
    async def start_stream(self, result_callback: Callable):
        """
        ìŠ¤íŠ¸ë¦¬ë° ì¸ì‹ ì‹œì‘
        
        Args:
            result_callback: async def callback(text: str, is_final: bool)
        """
        if self._running:
            logger.warning("STT stream already running")
            return
        
        self._running = True
        self.result_callback = result_callback
        self.audio_queue = asyncio.Queue(maxsize=100)
        
        self._streaming_task = asyncio.create_task(self._streaming_recognize())
        logger.info("STT streaming started")
    
    async def stop_stream(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì¸ì‹ ì¤‘ì§€"""
        self._running = False
        
        if self.audio_queue:
            await self.audio_queue.put(None)  # ì¢…ë£Œ ì‹ í˜¸
        
        if self._streaming_task:
            try:
                await asyncio.wait_for(self._streaming_task, timeout=5.0)
            except asyncio.TimeoutError:
                self._streaming_task.cancel()
        
        logger.info("STT streaming stopped")
    
    async def send_audio(self, audio_data: bytes):
        """
        ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ STTë¡œ ì „ì†¡
        
        Args:
            audio_data: 16-bit PCM audio bytes
        """
        if not self._running or not self.audio_queue:
            logger.warning("STT not running, audio dropped")
            return
        
        try:
            self.audio_queue.put_nowait(audio_data)
        except asyncio.QueueFull:
            logger.warning("STT audio queue full, frame dropped")
    
    async def _streaming_recognize(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì¸ì‹ ë©”ì¸ ë£¨í”„"""
        try:
            # ìš”ì²­ ìƒì„±ê¸°
            requests = self._request_generator()
            
            # gRPC ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            responses = self.client.streaming_recognize(
                self.streaming_config,
                requests
            )
            
            # ì‘ë‹µ ì²˜ë¦¬
            for response in responses:
                if not response.results:
                    continue
                
                result = response.results[0]
                if not result.alternatives:
                    continue
                
                transcript = result.alternatives[0].transcript
                is_final = result.is_final
                
                # ì½œë°± í˜¸ì¶œ
                if self.result_callback:
                    await self.result_callback(transcript, is_final)
                
                logger.debug("STT result", 
                           text=transcript,
                           is_final=is_final)
                
        except Exception as e:
            logger.error("STT streaming error", error=str(e))
        finally:
            self._running = False
    
    def _request_generator(self):
        """STT ìš”ì²­ ìƒì„±ê¸° (ë™ê¸° generator)"""
        # ì²« ë²ˆì§¸ ìš”ì²­: ì„¤ì •
        yield speech.StreamingRecognizeRequest(
            streaming_config=self.streaming_config
        )
        
        # ì´í›„ ìš”ì²­: ì˜¤ë””ì˜¤ ë°ì´í„°
        while self._running:
            try:
                # asyncio.Queueë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‚¬ìš© (blocking)
                audio_data = self.audio_queue.get_nowait() if self.audio_queue else None
                
                if audio_data is None:
                    break
                
                yield speech.StreamingRecognizeRequest(
                    audio_content=audio_data
                )
                
            except asyncio.QueueEmpty:
                # íê°€ ë¹„ì—ˆìœ¼ë©´ ì§§ì€ ëŒ€ê¸°
                import time
                time.sleep(0.01)
                continue
            except Exception as e:
                logger.error("Request generator error", error=str(e))
                break


# ì‚¬ìš© ì˜ˆì‹œ
async def example_usage():
    """STTClient ì‚¬ìš© ì˜ˆì‹œ"""
    
    async def on_stt_result(text: str, is_final: bool):
        """STT ê²°ê³¼ ì½œë°±"""
        print(f"{'[FINAL]' if is_final else '[INTERIM]'} {text}")
        
        if is_final:
            # ìµœì¢… ê²°ê³¼ â†’ AI Orchestratorë¡œ ì „ë‹¬
            await orchestrator.on_stt_result(text, is_final)
    
    config = {
        "model": "telephony",
        "language_code": "ko-KR",
        "sample_rate": 16000,
        "enable_enhanced": True
    }
    
    stt = STTClient(config)
    await stt.start_stream(on_stt_result)
    
    # ì˜¤ë””ì˜¤ ì „ì†¡
    while True:
        audio_frame = await audio_buffer.get_frame()
        if audio_frame:
            await stt.send_audio(audio_frame)
    
    await stt.stop_stream()
```

---

## 4. TTS Client (Google gRPC) ğŸ†•

### 4.1 ì™„ì „í•œ êµ¬í˜„

íŒŒì¼ ìœ„ì¹˜: `src/ai_voicebot/ai_pipeline/tts_client.py`

```python
from google.cloud import texttospeech
import asyncio
from typing import AsyncGenerator, Optional
import structlog

logger = structlog.get_logger(__name__)


class TTSClient:
    """
    Google Cloud Text-to-Speech gRPC Client
    
    í…ìŠ¤íŠ¸ â†’ ìŒì„± ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: dict):
        """
        Args:
            config: TTS ì„¤ì •
                - voice_name: "ko-KR-Neural2-A"
                - speaking_rate: 1.0
                - pitch: 0.0
        """
        self.config = config
        self.client = texttospeech.TextToSpeechClient()
        
        # ìŒì„± ì„¤ì •
        self.voice = texttospeech.VoiceSelectionParams(
            language_code=config.get("language_code", "ko-KR"),
            name=config.get("voice_name", "ko-KR-Neural2-A"),
            ssml_gender=texttospeech.SsmlVoiceGender.FEMALE
        )
        
        # ì˜¤ë””ì˜¤ ì„¤ì •
        self.audio_config = texttospeech.AudioConfig(
            audio_encoding=texttospeech.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            speaking_rate=config.get("speaking_rate", 1.0),
            pitch=config.get("pitch", 0.0),
        )
        
        self._is_generating = False
        self._stop_flag = False
        
        logger.info("TTSClient initialized", 
                   voice=config.get("voice_name"))
    
    async def synthesize_stream(
        self, 
        text: str
    ) -> AsyncGenerator[bytes, None]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜ (ìŠ¤íŠ¸ë¦¬ë°)
        
        Args:
            text: ë³€í™˜í•  í…ìŠ¤íŠ¸
            
        Yields:
            ì˜¤ë””ì˜¤ ì²­í¬ (bytes)
        """
        if self._is_generating:
            logger.warning("TTS already generating")
            return
        
        self._is_generating = True
        self._stop_flag = False
        
        try:
            # TTS ìš”ì²­
            synthesis_input = texttospeech.SynthesisInput(text=text)
            
            # ë™ê¸° APIë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.client.synthesize_speech(
                    input=synthesis_input,
                    voice=self.voice,
                    audio_config=self.audio_config
                )
            )
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            audio_data = response.audio_content
            chunk_size = 4096  # 4KB chunks
            
            for i in range(0, len(audio_data), chunk_size):
                # ì¤‘ì§€ í”Œë˜ê·¸ í™•ì¸ (Barge-in)
                if self._stop_flag:
                    logger.info("TTS stopped (barge-in)")
                    break
                
                chunk = audio_data[i:i + chunk_size]
                yield chunk
                
                # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                await asyncio.sleep(0.01)
            
            logger.debug("TTS synthesis completed", text_length=len(text))
            
        except Exception as e:
            logger.error("TTS synthesis error", error=str(e))
        finally:
            self._is_generating = False
            self._stop_flag = False
    
    async def synthesize(self, text: str) -> bytes:
        """
        í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜ (ì „ì²´)
        
        Args:
            text: ë³€í™˜í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì „ì²´ ì˜¤ë””ì˜¤ ë°ì´í„° (bytes)
        """
        try:
            synthesis_input = texttospeech.SynthesisInput(text=text)
            
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.client.synthesize_speech(
                    input=synthesis_input,
                    voice=self.voice,
                    audio_config=self.audio_config
                )
            )
            
            return response.audio_content
            
        except Exception as e:
            logger.error("TTS synthesis error", error=str(e))
            return b''
    
    def stop(self):
        """TTS ìƒì„± ì¤‘ì§€ (Barge-inìš©)"""
        if self._is_generating:
            self._stop_flag = True
            logger.info("TTS stop requested")
    
    def is_generating(self) -> bool:
        """í˜„ì¬ ìƒì„± ì¤‘ì¸ì§€ í™•ì¸"""
        return self._is_generating


# ì‚¬ìš© ì˜ˆì‹œ
async def example_usage():
    """TTSClient ì‚¬ìš© ì˜ˆì‹œ"""
    config = {
        "voice_name": "ko-KR-Neural2-A",
        "speaking_rate": 1.0,
        "pitch": 0.0
    }
    
    tts = TTSClient(config)
    
    # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
    text = "ì•ˆë…•í•˜ì„¸ìš”, AI ë¹„ì„œì…ë‹ˆë‹¤."
    
    async for audio_chunk in tts.synthesize_stream(text):
        # RTPë¡œ ì „ì†¡
        await rtp_relay.send_audio(audio_chunk)
        
        # Barge-in ì²´í¬
        if vad.is_barge_in():
            tts.stop()
            break
    
    # ë˜ëŠ” ì „ì²´ ìƒì„±
    audio_data = await tts.synthesize(text)
    await rtp_relay.send_audio(audio_data)
```

---

ì´ ë¬¸ì„œëŠ” ê³„ì† ì´ì–´ì§‘ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ 4ê°œ ì»´í¬ë„ŒíŠ¸(LLM Client, RAG Engine, Call Recorder, Knowledge Extractor)ëŠ” ë‹¤ìŒ íŒŒì¼ë¡œ ë¶„ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.

**ë‹¤ìŒ ì‘ì—…:**
- `docs/ai-implementation-guide-part2.md` ìƒì„±
- ë‚˜ë¨¸ì§€ 4ê°œ ì»´í¬ë„ŒíŠ¸ ìƒì„¸ êµ¬í˜„
- í†µí•© ì˜ˆì‹œ ë° E2E í…ŒìŠ¤íŠ¸

ê³„ì† ì§„í–‰í• ê¹Œìš”?

