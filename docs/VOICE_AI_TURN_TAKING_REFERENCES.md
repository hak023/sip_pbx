# Voice AI ëŒ€í™” ì£¼ë„ê¶Œ ë° ë°œí™” ì¸ì‹ - GitHub ì°¸ê³  ìë£Œ

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [í•µì‹¬ GitHub í”„ë¡œì íŠ¸](#í•µì‹¬-github-í”„ë¡œì íŠ¸)
3. [Turn Detection (ë°œí™” ì¢…ë£Œ ê°ì§€)](#turn-detection-ë°œí™”-ì¢…ë£Œ-ê°ì§€)
4. [Barge-in & Interruption (ëŒ€í™” ì£¼ë„ê¶Œ)](#barge-in--interruption-ëŒ€í™”-ì£¼ë„ê¶Œ)
5. [Context Management & RAG](#context-management--rag)
6. [í”„ë ˆì„ì›Œí¬ ë° ë„êµ¬](#í”„ë ˆì„ì›Œí¬-ë°-ë„êµ¬)
7. [êµ¬í˜„ ê¶Œì¥ì‚¬í•­](#êµ¬í˜„-ê¶Œì¥ì‚¬í•­)

---

## ê°œìš”

### ì‚¬ìš©ìì˜ ê³ ë¯¼ì‚¬í•­

1. **ë°œí™” ì¢…ë£Œ ê°ì§€ (Turn Detection)**
   - ì‚¬ëŒì´ ë°œí™”í•˜ëŠ” ê²ƒì— ëŒ€í•œ ì¸ì‹
   - ì–´ëŠ ì •ë„ ë¬µìŒì´ë©´ ë°œí™”ê°€ ëë‚œë‹¤ê³  ì¸ì‹
   - ì „ì²´ ëŒ€í™” ë§¥ë½ íŒŒì•… í•„ìš”

2. **ëŒ€í™” ì£¼ë„ê¶Œ (Turn-Taking / Barge-in)**
   - TTS ì¤‘ ì‚¬ìš©ìê°€ ë§í•œë‹¤ê³  ë¬´ì¡°ê±´ ëŠìœ¼ë©´ ì•ˆë¨
   - "ì ì ˆí•œ" ëŒ€í™” ì£¼ë„ê¶Œ ê´€ë¦¬ í•„ìš”
   - ì‚¬ëŒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„

3. **ë§¥ë½ ê´€ë¦¬ (Context Management)**
   - ë§í•œ ê²ƒë§Œ í”„ë¡¬í”„íŒ…í•˜ë©´ ì•ˆë¨
   - ì „ì²´ ëŒ€í™” ë§¥ë½ íŒŒì•…
   - VectorDB RAG í™œìš©

---

## í•µì‹¬ GitHub í”„ë¡œì íŠ¸

### ğŸ† 1. Pipecat AI Framework (ê°€ì¥ ì¶”ì²œ)

**Repository**: [pipecat-ai/pipecat](https://github.com/pipecat-ai/pipecat)
- â­ **3,500+ stars**
- ğŸ¯ **ìš©ë„**: ì‹¤ì‹œê°„ ìŒì„± AI ì—ì´ì „íŠ¸ êµ¬ì¶•ì„ ìœ„í•œ ì™„ì „í•œ í”„ë ˆì„ì›Œí¬
- ğŸ”§ **ì–¸ì–´**: Python
- ğŸ“¦ **ì„¤ì¹˜**: `pip install pipecat-ai`

#### ì£¼ìš” ê¸°ëŠ¥
- âœ… **Turn Detection**: Smart Turn v3.2 ëª¨ë¸ í†µí•©
- âœ… **Barge-in/Interruption**: ë‹¤ì–‘í•œ interruption ì „ëµ
- âœ… **VAD**: Silero VAD (ë¡œì»¬, ë¹ ë¦„)
- âœ… **Context Management**: ëŒ€í™” ë§¥ë½ ê´€ë¦¬
- âœ… **Multi-modal**: ìŒì„± + í…ìŠ¤íŠ¸ + ë¹„ë””ì˜¤
- âœ… **Production-ready**: ì‹¤ì œ í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥

#### ë¬¸ì„œ
- [Speech Input & Turn Detection](https://docs.pipecat.ai/guides/learn/speech-input)
- [Interruption Strategies](https://docs.pipecat.ai/server/utilities/turn-management/interruption-strategies)
- [User Turn Strategies](https://docs.pipecat.ai/server/utilities/turn-management/user-turn-strategies)

#### ì ìš© ê°€ëŠ¥ì„±
- **ë§¤ìš° ë†’ìŒ** - ìš°ë¦¬ ì‹œìŠ¤í…œê³¼ ê±°ì˜ ì™„ë²½í•˜ê²Œ ë§¤ì¹­
- ì´ë¯¸ STT/TTS/LLM/RAG í†µí•©ë˜ì–´ ìˆìŒ
- Python ê¸°ë°˜ìœ¼ë¡œ ìš°ë¦¬ ì½”ë“œë² ì´ìŠ¤ì™€ í˜¸í™˜ì„± ìš°ìˆ˜

---

### ğŸ¥ˆ 2. Smart Turn v3.2

**Repository**: [pipecat-ai/smart-turn](https://github.com/pipecat-ai/smart-turn)
- â­ **1,267 stars**
- ğŸ¯ **ìš©ë„**: ë°œí™” ì¢…ë£Œ ê°ì§€ë¥¼ ìœ„í•œ AI ëª¨ë¸
- ğŸ”§ **ì–¸ì–´**: Python
- ğŸ“¦ **ì„¤ì¹˜**: Pipecat ë‚´ì¥ ë˜ëŠ” ë…ë¦½ ì‚¬ìš©

#### ì£¼ìš” íŠ¹ì§•
- âœ… **23ê°œ ì–¸ì–´ ì§€ì›** (í•œêµ­ì–´ í¬í•¨ ğŸ‡°ğŸ‡·)
- âœ… **ë¹ ë¥¸ ì¶”ë¡ **: CPUì—ì„œ 10ms, í´ë¼ìš°ë“œ ì¸ìŠ¤í„´ìŠ¤ 100ms ì´í•˜
- âœ… **Audio-native**: PCM ì˜¤ë””ì˜¤ ì§ì ‘ ì²˜ë¦¬ (prosody ì¸ì‹)
- âœ… **ê²½ëŸ‰**: CPU ë²„ì „ 8MB (int8 quantized), GPU ë²„ì „ 32MB (fp32)
- âœ… **ì˜¤í”ˆì†ŒìŠ¤**: BSD 2-clause ë¼ì´ì„ ìŠ¤

#### ì‘ë™ ì›ë¦¬
```
ì‚¬ìš©ì ìŒì„± ì…ë ¥
    â†“
Silero VAD (ìŒì„±/ë¬µìŒ ê°ì§€)
    â†“
Smart Turn v3.2 (ë°œí™” ì™„ë£Œ ì—¬ë¶€ íŒë‹¨)
    â†“
- Grammar (ë¬¸ë²•ì  ì™„ê²°ì„±)
- Tone (ì–µì–‘/ìŒì¡°)
- Pace (ë§í•˜ê¸° ì†ë„)
    â†“
"ë°œí™” ì™„ë£Œ" or "ê³„ì† ë§í•˜ëŠ” ì¤‘"
```

#### ì‚¬ìš© ì˜ˆì‹œ
```python
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.turns.user_stop import TurnAnalyzerUserTurnStopStrategy

# Smart Turn í™œì„±í™”
stop_strategy = TurnAnalyzerUserTurnStopStrategy(
    turn_analyzer=LocalSmartTurnAnalyzerV3()
)

# VAD ì„¤ì • (Smart Turn ì‚¬ìš©ì‹œ stop_secs ë‚®ì¶¤)
vad_params = VADParams(
    start_secs=0.2,
    stop_secs=0.2,  # Smart Turnì´ ë¹ ë¥´ê²Œ ë¶„ì„í•˜ë„ë¡
)
```

#### ì ìš© ê°€ëŠ¥ì„±
- **ë†’ìŒ** - ìš°ë¦¬ ì‹œìŠ¤í…œì— ì§ì ‘ í†µí•© ê°€ëŠ¥
- ê¸°ì¡´ `BargeInController`ë¥¼ Smart Turnìœ¼ë¡œ ê°•í™”
- `check_silence()` ë¡œì§ì„ Smart Turn ê¸°ë°˜ìœ¼ë¡œ êµì²´

---

### ğŸ¥‰ 3. Vogent Turn

**Repository**: [vogent/vogent-turn](https://github.com/vogent/vogent-turn)
- â­ **42 stars**
- ğŸ¯ **ìš©ë„**: Multimodal turn detection (ì˜¤ë””ì˜¤ + í…ìŠ¤íŠ¸)
- ğŸ”§ **ì–¸ì–´**: Python
- ğŸ“¦ **ì„¤ì¹˜**: `pip install vogent-turn`

#### ì£¼ìš” íŠ¹ì§•
- âœ… **Multimodal**: Whisper (ì˜¤ë””ì˜¤) + SmolLM (í…ìŠ¤íŠ¸)
- âœ… **ë¹ ë¦„**: `torch.compile` ìµœì í™”
- âœ… **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹**: ì´ì „ ëŒ€í™” ë‚´ìš© ê³ ë ¤
- âœ… **Production-ready**: ë°°ì¹˜ ì²˜ë¦¬, ëª¨ë¸ ìºì‹±

#### Architecture
```
Audio (16kHz) â”€â”€> Whisper Encoder â”€â”€> Audio Embeddings (1500D)
                                              â†“
                                      Audio Projector
                                              â†“
Text Context â”€â”€> SmolLM Tokenizer â”€â”€> Text Embeddings
                                              â†“
                    [Audio + Text] â”€â”€> SmolLM (80M params)
                                              â†“
                                   Classification Head
                                              â†“
                                   [Turn Complete / Incomplete]
```

#### ì‚¬ìš© ì˜ˆì‹œ
```python
from vogent_turn import TurnDetector
import soundfile as sf

detector = TurnDetector(compile_model=True, warmup=True)

audio, sr = sf.read("speech.wav")

# ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨
result = detector.predict(
    audio,
    prev_line="ì „í™”ë²ˆí˜¸ê°€ ì–´ë–»ê²Œ ë˜ì„¸ìš”?",  # ì´ì „ ë°œí™”
    curr_line="ì œ ë²ˆí˜¸ëŠ” 010",              # í˜„ì¬ ë°œí™”
    sample_rate=sr,
    return_probs=True,
)

print(f"ë°œí™” ì™„ë£Œ: {result['is_endpoint']}")
print(f"í™•ì‹ ë„: {result['prob_endpoint']:.1%}")
```

#### ì ìš© ê°€ëŠ¥ì„±
- **ì¤‘ê°„** - í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•œ ê²½ìš° ìœ ìš©
- Smart Turnë³´ë‹¤ ë¬´ê²ì§€ë§Œ ë” ì •í™•í•  ìˆ˜ ìˆìŒ
- ìš°ë¦¬ ì‹œìŠ¤í…œì˜ LLM í”„ë¡¬í”„íŠ¸ì™€ ì—°ë™ ê°€ëŠ¥

---

### 4. Crosstalk

**Repository**: [tarzain/crosstalk](https://github.com/tarzain/crosstalk)
- â­ **30 stars**
- ğŸ¯ **ìš©ë„**: 2-way interruptible voice interactions
- ğŸ”§ **ì–¸ì–´**: JavaScript (React)

#### ì£¼ìš” ê°œë…
```
ì „í†µì ì¸ Turn-based ì‹œìŠ¤í…œì˜ ë¬¸ì œ:
âŒ AIê°€ ë§í•˜ëŠ” ë™ì•ˆ ì‚¬ìš©ì ìŒì„± ì¸ì‹ ì•ˆë¨
âŒ ì‚¬ëŒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ëŠì„ ìˆ˜ ì—†ìŒ
âŒ ëŒ€ê¸° ì‹œê°„ ê¸¸ì–´ì§

Crosstalk ë°©ì‹:
âœ… AIì™€ ì‚¬ìš©ì ìŒì„±ì„ ë™ì‹œì— ì¸ì‹ (diarization)
âœ… ì‚¬ìš©ìê°€ ë¼ì–´ë“¤ë©´ AI ì¦‰ì‹œ ì¤‘ë‹¨
âœ… AIê°€ ê³„ì† ë§í•´ì•¼ í•˜ë©´ ìë™ìœ¼ë¡œ ì¬ê°œ
```

#### ì‘ë™ ì›ë¦¬
1. **Continuous Speech Recognition**: ì‚¬ìš©ìì™€ AI ìŒì„±ì„ ë™ì‹œì— ì¸ì‹
2. **Speaker Diarization**: ëˆ„ê°€ ë§í•˜ëŠ”ì§€ êµ¬ë¶„
3. **Prediction-based Turn**: LLMì´ ë‹¤ìŒ í™”ì ì˜ˆì¸¡
   - ì˜ˆì¸¡ì´ "AI" â†’ AI ê³„ì† ë§í•¨
   - ì˜ˆì¸¡ì´ "User" â†’ AI ì¤‘ë‹¨, ì‚¬ìš©ìì—ê²Œ ì£¼ë„ê¶Œ

#### ì ìš© ê°€ëŠ¥ì„±
- **ë‚®ìŒ** - JavaScript ê¸°ë°˜, ì»¨ì…‰ ì°¸ê³ ìš©
- Diarization ì•„ì´ë””ì–´ëŠ” ì¢‹ì§€ë§Œ Pythonìœ¼ë¡œ ì¬êµ¬í˜„ í•„ìš”
- Real-time speech recognition + diarizationì´ ë³µì¡í•¨

---

## Turn Detection (ë°œí™” ì¢…ë£Œ ê°ì§€)

### ë¬¸ì œ ì •ì˜

```
ì‚¬ìš©ì: "ì €... ìŒ... ë‚´ì¼ ë‚ ì”¨ê°€..."
ì‹œìŠ¤í…œ: [ì—¬ê¸°ì„œ ëŠìœ¼ë©´ ì•ˆë¨! ì•„ì§ ë§í•˜ëŠ” ì¤‘]

ì‚¬ìš©ì: "ë‚´ì¼ ë‚ ì”¨ê°€ ì–´ë–¤ê°€ìš”?"
ì‹œìŠ¤í…œ: [ì—¬ê¸°ì„œëŠ” ëŠì–´ì•¼ í•¨, ë°œí™” ì™„ë£Œ]
```

### í•´ê²° ë°©ë²• ë¹„êµ

#### 1. VAD Only (í˜„ì¬ ìš°ë¦¬ ë°©ì‹)
```python
# ì¥ì 
âœ… ë¹ ë¦„ (1ms ë¯¸ë§Œ)
âœ… ê²½ëŸ‰ (CPU)
âœ… êµ¬í˜„ ê°„ë‹¨

# ë‹¨ì 
âŒ ë¬µìŒë§Œ ê°ì§€ (2ì´ˆ ì¹¨ë¬µ = ë?)
âŒ "ìŒ...", "ì–´..." ê°™ì€ filler words ì²˜ë¦¬ ëª»í•¨
âŒ ë¬¸ë²•ì  ì™„ê²°ì„± íŒë‹¨ ëª»í•¨
```

#### 2. VAD + Smart Turn (ì¶”ì²œ)
```python
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3

# ì¥ì 
âœ… ì–¸ì–´í•™ì  cue ì¸ì‹ (ë¬¸ë²•, ì–µì–‘, ì†ë„)
âœ… 23ê°œ ì–¸ì–´ ì§€ì› (í•œêµ­ì–´ í¬í•¨)
âœ… ë¹ ë¦„ (10-100ms)
âœ… ë†’ì€ ì •í™•ë„

# ë‹¨ì 
âš ï¸ VADë³´ë‹¤ ë¬´ê±°ì›€ (í•˜ì§€ë§Œ ì¶©ë¶„íˆ ë¹ ë¦„)
âš ï¸ ì¶”ê°€ ëª¨ë¸ ë¡œë”© í•„ìš” (8MB)
```

#### 3. VAD + Vogent Turn (ê³ ê¸‰)
```python
from vogent_turn import TurnDetector

# ì¥ì 
âœ… Multimodal (ì˜¤ë””ì˜¤ + í…ìŠ¤íŠ¸)
âœ… ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤
âœ… ë§¤ìš° ë†’ì€ ì •í™•ë„

# ë‹¨ì 
âš ï¸ ë¬´ê±°ì›€ (80M params)
âš ï¸ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í•„ìš”
```

### êµ¬í˜„ ê¶Œì¥ì‚¬í•­

**Phase 1: VAD + ê³ ì • ì¹¨ë¬µ ì‹œê°„ (í˜„ì¬ ìš°ë¦¬ ë°©ì‹)**
```python
# src/ai_voicebot/orchestrator/barge_in_controller.py
SILENCE_THRESHOLD_MS = 2000  # 2ì´ˆ ì¹¨ë¬µ
```

**Phase 2: Smart Turn í†µí•© (ê¶Œì¥)**
```python
# 1. ì„¤ì¹˜
pip install pipecat-ai

# 2. Smart Turn í™œì„±í™”
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3

self.turn_analyzer = LocalSmartTurnAnalyzerV3()

# 3. VAD ì¹¨ë¬µ ê°ì§€ í›„ Smart Turn ì‹¤í–‰
async def check_silence(self):
    if vad_detected_silence:  # 0.2ì´ˆ ì¹¨ë¬µ
        audio_chunk = self.get_recent_audio()  # ìµœê·¼ 8ì´ˆ
        is_complete = await self.turn_analyzer.predict(audio_chunk)
        
        if is_complete:
            return self.get_and_reset_utterance()
```

**Phase 3: Vogent Turn (ì„ íƒì‚¬í•­ - ë” ë†’ì€ ì •í™•ë„ í•„ìš”ì‹œ)**
```python
from vogent_turn import TurnDetector

self.turn_detector = TurnDetector(compile_model=True)

result = self.turn_detector.predict(
    audio,
    prev_line=self.last_ai_response,
    curr_line=self.current_user_text,
    return_probs=True
)
```

---

## Barge-in & Interruption (ëŒ€í™” ì£¼ë„ê¶Œ)

### ë¬¸ì œ ì •ì˜

```
ìƒí™© 1: AIê°€ ê¸´ ì„¤ëª… ì¤‘
AI: "ë‚ ì”¨ ì˜ˆë³´ëŠ” ì˜¤ì „ì—ëŠ” ë§‘ê³  ì˜¤í›„ì—ëŠ” íë¦¬ë©°..."
ì‚¬ìš©ì: "ì ê¹ë§Œ!" [â† ì—¬ê¸°ì„œ ëŠì–´ì•¼ í•¨]
ì‹œìŠ¤í…œ: [AI ì¦‰ì‹œ ì¤‘ë‹¨, ì‚¬ìš©ìì—ê²Œ ì£¼ë„ê¶Œ]

ìƒí™© 2: Backchannel (ë§ì¥êµ¬)
AI: "ë‚ ì”¨ ì˜ˆë³´ëŠ” ì˜¤ì „ì—ëŠ” ë§‘ê³  ì˜¤í›„ì—ëŠ” íë¦¬ë©°..."
ì‚¬ìš©ì: "ìŒ..." [â† ì—¬ê¸°ì„œëŠ” ëŠìœ¼ë©´ ì•ˆë¨! ë§ì¥êµ¬ì¼ ë¿]
ì‹œìŠ¤í…œ: [AI ê³„ì† ë§í•¨]

ìƒí™© 3: ì ê·¹ì  interruption
AI: "ë‚ ì”¨ ì˜ˆë³´ëŠ” ì˜¤ì „ì—ëŠ”..."
ì‚¬ìš©ì: "ë‚´ì¼ ë‚ ì”¨ë§Œ ì•Œë ¤ì¤˜!" [â† ëª…í™•í•œ ìš”ì²­, ëŠì–´ì•¼ í•¨]
ì‹œìŠ¤í…œ: [AI ì¦‰ì‹œ ì¤‘ë‹¨, ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬]
```

### Pipecatì˜ Interruption ì „ëµ

#### 1. MinWordsInterruptionStrategy (ê¸°ë³¸)
```python
from pipecat.audio.interruptions.min_words_interruption_strategy import MinWordsInterruptionStrategy

# 3ë‹¨ì–´ ì´ìƒ ë§í•´ì•¼ interruption
strategy = MinWordsInterruptionStrategy(min_words=3)

# ì˜ˆì‹œ:
# "ìŒ" â†’ âŒ ë¬´ì‹œ (1ë‹¨ì–´)
# "ë„¤ ê·¸ë˜ìš”" â†’ âŒ ë¬´ì‹œ (2ë‹¨ì–´)
# "ì ê¹ë§Œìš” ê·¸ê±´ ì•„ë‹Œë°ìš”" â†’ âœ… Interrupt (5ë‹¨ì–´)
```

#### 2. Custom Volume-based Strategy (ê³ ê¸‰)
```python
class VolumeInterruptionStrategy(BaseInterruptionStrategy):
    """ìŒëŸ‰ ê¸°ë°˜ interruption"""
    
    def __init__(self, min_volume: float = 0.8):
        self.min_volume = min_volume
        self.audio_buffer = []
    
    async def append_audio(self, audio, sample_rate):
        self.audio_buffer.append(audio)
    
    async def should_interrupt(self) -> bool:
        if not self.audio_buffer:
            return False
        
        # í‰ê·  ìŒëŸ‰ ê³„ì‚°
        avg_volume = np.mean([np.abs(a).mean() for a in self.audio_buffer])
        return avg_volume > self.min_volume

# ì‚¬ìš©:
# - ì‘ì€ ì†Œë¦¬ ("ìŒ...") â†’ âŒ ë¬´ì‹œ
# - í° ì†Œë¦¬ ("ì ê¹ë§Œìš”!") â†’ âœ… Interrupt
```

#### 3. Semantic-based Strategy (ìµœê³ ê¸‰)
```python
class SemanticInterruptionStrategy(BaseInterruptionStrategy):
    """ì˜ë¯¸ ê¸°ë°˜ interruption (LLM ì‚¬ìš©)"""
    
    def __init__(self, llm_client):
        self.llm = llm_client
    
    async def append_text(self, text):
        self.user_text = text
    
    async def should_interrupt(self) -> bool:
        # LLMì—ê²Œ ë¬¼ì–´ë´„
        prompt = f"""
        AIê°€ ë§í•˜ëŠ” ì¤‘ì— ì‚¬ìš©ìê°€ "{self.user_text}"ë¼ê³  ë§í–ˆìŠµë‹ˆë‹¤.
        ì´ê²ƒì´ ë‹¨ìˆœ ë§ì¥êµ¬ì¸ê°€ìš”, ì•„ë‹ˆë©´ ëŒ€í™”ë¥¼ ëŠê³  ì‹¶ì€ ì˜ë„ì¸ê°€ìš”?
        
        ë‹µë³€: "ë§ì¥êµ¬" ë˜ëŠ” "interruption"
        """
        
        response = await self.llm.generate(prompt)
        return "interruption" in response.lower()

# ì‚¬ìš©:
# "ë„¤ë„¤" â†’ LLM íŒë‹¨ â†’ "ë§ì¥êµ¬" â†’ âŒ ë¬´ì‹œ
# "ì ê¹ë§Œìš”" â†’ LLM íŒë‹¨ â†’ "interruption" â†’ âœ… Interrupt
# "ë‚´ì¼ ë‚ ì”¨ëŠ”?" â†’ LLM íŒë‹¨ â†’ "interruption" â†’ âœ… Interrupt
```

### êµ¬í˜„ ê¶Œì¥ì‚¬í•­

**Phase 1: ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ (ê°„ë‹¨, íš¨ê³¼ì )**
```python
# src/ai_voicebot/orchestrator/barge_in_controller.py

class BargeInController:
    def __init__(self, min_words_for_interrupt: int = 3):
        self.min_words = min_words_for_interrupt
        self.is_tts_playing = False
    
    def should_process_speech(self, text: str) -> bool:
        """TTS ì¬ìƒ ì¤‘ ì‚¬ìš©ì ë°œí™” ì²˜ë¦¬ ì—¬ë¶€ íŒë‹¨"""
        if not self.is_tts_playing:
            return True  # TTS ì•ˆí•˜ë©´ ë¬´ì¡°ê±´ ì²˜ë¦¬
        
        # TTS ì¤‘ì´ë©´ ë‹¨ì–´ ìˆ˜ ì²´í¬
        word_count = len(text.split())
        if word_count >= self.min_words:
            self.stop_tts()  # TTS ì¤‘ë‹¨
            return True
        
        return False  # ë§ì¥êµ¬ë¡œ ê°„ì£¼, ë¬´ì‹œ
```

**Phase 2: ìŒëŸ‰ + ë‹¨ì–´ ìˆ˜ (ë” ì •êµ)**
```python
class AdvancedBargeInController:
    def should_process_speech(self, text: str, audio: np.ndarray) -> bool:
        if not self.is_tts_playing:
            return True
        
        word_count = len(text.split())
        avg_volume = np.abs(audio).mean()
        
        # ìŒëŸ‰ ë†’ê³  ë‹¨ì–´ ë§ìœ¼ë©´ interrupt
        if avg_volume > 0.5 and word_count >= 3:
            self.stop_tts()
            return True
        
        # ìŒëŸ‰ ë§¤ìš° ë†’ìœ¼ë©´ ë‹¨ì–´ ì ì–´ë„ interrupt
        if avg_volume > 0.8:
            self.stop_tts()
            return True
        
        return False
```

**Phase 3: LLM ê¸°ë°˜ (ìµœê³ ê¸‰, ëŠë¦¼)**
```python
async def should_process_speech(self, text: str) -> bool:
    if not self.is_tts_playing:
        return True
    
    # ë¹ ë¥¸ íœ´ë¦¬ìŠ¤í‹± ì²´í¬ ë¨¼ì €
    if len(text.split()) < 2:
        return False  # 1ë‹¨ì–´ëŠ” ë¬´ì¡°ê±´ ë¬´ì‹œ
    
    # LLMì—ê²Œ íŒë‹¨ ìš”ì²­
    prompt = f"""
    ë‹¹ì‹ ì€ ì „í™” ìƒë‹´ AIì…ë‹ˆë‹¤. ì§€ê¸ˆ ê³ ê°ì—ê²Œ ì„¤ëª… ì¤‘ì¸ë°,
    ê³ ê°ì´ "{text}"ë¼ê³  ë§í–ˆìŠµë‹ˆë‹¤.
    
    ì´ê²ƒì´ ë‹¨ìˆœ ë§ì¥êµ¬(ì˜ˆ: "ë„¤", "ìŒ", "ê·¸ë ‡êµ°ìš”")ì¸ì§€,
    ì•„ë‹ˆë©´ ì‹¤ì œë¡œ ë§ì„ ëŠê³  ì‹¶ì€ ê²ƒì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.
    
    ë‹µë³€: "ë§ì¥êµ¬" ë˜ëŠ” "interruption"ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    """
    
    response = await self.llm.generate_fast(prompt, max_tokens=10)
    
    if "interruption" in response.lower():
        self.stop_tts()
        return True
    
    return False
```

---

## Context Management & RAG

### ë¬¸ì œ ì •ì˜

```
âŒ ë‚˜ìœ í”„ë¡¬í”„íŒ…:
User: "ë‚´ì¼ ë‚ ì”¨ ì•Œë ¤ì¤˜"
LLM Prompt: "ë‚´ì¼ ë‚ ì”¨ ì•Œë ¤ì¤˜"

âœ… ì¢‹ì€ í”„ë¡¬í”„íŒ…:
User: "ë‚´ì¼ ë‚ ì”¨ ì•Œë ¤ì¤˜"
LLM Prompt: """
ëŒ€í™” ê¸°ë¡:
[1] ì‹œìŠ¤í…œ: ì•ˆë…•í•˜ì„¸ìš”. ê¸°ìƒì²­ AI ë¹„ì„œì…ë‹ˆë‹¤.
[2] ì‚¬ìš©ì: ê±°ê¸° ë­ í•˜ëŠ” ê³³ì´ì—ìš”?
[3] ì‹œìŠ¤í…œ: ê¸°ìƒì²­ì€ ë‚ ì”¨ ì˜ˆë³´ë¥¼ ì œê³µí•˜ëŠ” ê³³ì…ë‹ˆë‹¤.
[4] ì‚¬ìš©ì: ë‚´ì¼ ë‚ ì”¨ ì•Œë ¤ì¤˜

ê´€ë ¨ ì§€ì‹ (VectorDB):
- ê¸°ìƒì²­ì€ ë‚ ì”¨ ì˜ˆë³´ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- ì „í™”ë²ˆí˜¸: 131
- ì›¹ì‚¬ì´íŠ¸: www.kma.go.kr

ì‚¬ìš©ì ì§ˆë¬¸: ë‚´ì¼ ë‚ ì”¨ ì•Œë ¤ì¤˜
"""
```

### Pipecatì˜ Context Management

```python
from pipecat.processors.aggregators.llm_response_universal import (
    LLMContextAggregatorPair,
    LLMUserAggregatorParams,
)

# Context ìƒì„±
context = OpenAILLMContext(
    messages=[
        {"role": "system", "content": system_prompt},
    ]
)

# Aggregator ìƒì„± (ëŒ€í™” ê¸°ë¡ ìë™ ê´€ë¦¬)
user_aggregator, assistant_aggregator = LLMContextAggregatorPair(
    context,
    user_params=LLMUserAggregatorParams(
        vad_analyzer=vad_analyzer,
    ),
)

# ìë™ìœ¼ë¡œ contextì— ë©”ì‹œì§€ ì¶”ê°€ë¨
# [User] â†’ user_aggregator â†’ context.add_message(role="user", ...)
# [LLM] â†’ assistant_aggregator â†’ context.add_message(role="assistant", ...)
```

### RAG Integration íŒ¨í„´

#### 1. LangChain + ChromaDB (í˜„ì¬ ìš°ë¦¬ ì‹œìŠ¤í…œ)
```python
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

class RAGEngine:
    def __init__(self):
        self.vector_db = Chroma(...)
        self.embedder = HuggingFaceEmbeddings(...)
    
    def retrieve_context(self, query: str, top_k: int = 3):
        """VectorDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        docs = self.vector_db.similarity_search(query, k=top_k)
        return "\n\n".join([doc.page_content for doc in docs])
    
    def build_prompt(self, user_query: str, conversation_history: List[Dict]):
        """ì „ì²´ ë§¥ë½ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # 1. RAGë¡œ ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
        rag_context = self.retrieve_context(user_query)
        
        # 2. ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ…
        history_text = "\n".join([
            f"[{msg['role']}] {msg['content']}"
            for msg in conversation_history
        ])
        
        # 3. í†µí•© í”„ë¡¬í”„íŠ¸
        prompt = f"""
ë‹¹ì‹ ì€ ê¸°ìƒì²­ì˜ ì¹œì ˆí•œ AI ìƒë‹´ì›ì…ë‹ˆë‹¤.

=== ëŒ€í™” ê¸°ë¡ ===
{history_text}

=== ê´€ë ¨ ì§€ì‹ (ë‚´ë¶€ ë¬¸ì„œ) ===
{rag_context}

=== í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ ===
{user_query}

ìœ„ ëŒ€í™” ê¸°ë¡ê³¼ ì§€ì‹ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""
        return prompt
```

#### 2. Conversational Memory (Session-based)
```python
from langchain.memory import ConversationBufferMemory

class ConversationalRAGEngine:
    def __init__(self):
        self.vector_db = Chroma(...)
        # Sessionë³„ memory
        self.memories: Dict[str, ConversationBufferMemory] = {}
    
    def get_or_create_memory(self, call_id: str):
        if call_id not in self.memories:
            self.memories[call_id] = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
            )
        return self.memories[call_id]
    
    async def generate_response(self, call_id: str, user_query: str):
        memory = self.get_or_create_memory(call_id)
        
        # 1. RAG ê²€ìƒ‰
        rag_docs = self.vector_db.similarity_search(user_query)
        
        # 2. LLM í˜¸ì¶œ (memory ìë™ í¬í•¨)
        response = await self.llm_chain.ainvoke({
            "question": user_query,
            "context": rag_docs,
            "chat_history": memory.load_memory_variables({})["chat_history"],
        })
        
        # 3. Memory ì—…ë°ì´íŠ¸
        memory.save_context(
            {"input": user_query},
            {"output": response}
        )
        
        return response
```

#### 3. Agentic RAG (ê³ ê¸‰)
```python
from langgraph.graph import StateGraph
from typing import TypedDict, Annotated

class AgentState(TypedDict):
    messages: List[Dict]
    user_query: str
    rag_context: str
    next_action: str

def retrieve_node(state: AgentState):
    """VectorDB ê²€ìƒ‰ ë…¸ë“œ"""
    query = state["user_query"]
    docs = vector_db.similarity_search(query)
    state["rag_context"] = "\n\n".join([d.page_content for d in docs])
    state["next_action"] = "generate"
    return state

def generate_node(state: AgentState):
    """LLM ìƒì„± ë…¸ë“œ"""
    prompt = build_prompt_with_context(
        state["user_query"],
        state["messages"],
        state["rag_context"]
    )
    response = llm.generate(prompt)
    state["messages"].append({"role": "assistant", "content": response})
    return state

# Graph êµ¬ì„±
workflow = StateGraph(AgentState)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)
workflow.add_edge("retrieve", "generate")
workflow.set_entry_point("retrieve")

app = workflow.compile()
```

### ìš°ë¦¬ ì‹œìŠ¤í…œ ê°œì„  ë°©ì•ˆ

**í˜„ì¬ (`src/ai_voicebot/orchestrator.py`)**:
```python
async def generate_and_speak_response(self, user_text: str):
    # âŒ ë¬¸ì œ: ë‹¨ìˆœ í”„ë¡¬í”„íŒ…
    response = await self.llm.generate_response(user_text)
    await self.speak(response)
```

**ê°œì„ ì•ˆ 1: RAG + ëŒ€í™” ê¸°ë¡**:
```python
async def generate_and_speak_response(self, user_text: str):
    # 1. Organization context ê°€ì ¸ì˜¤ê¸°
    org_context = self.org_manager.get_full_context_for_llm()
    
    # 2. RAG ê²€ìƒ‰
    rag_context = await self.rag.query(user_text, top_k=3)
    
    # 3. ëŒ€í™” ê¸°ë¡ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸
    conversation_history = self.get_conversation_history()
    
    prompt = f"""
{org_context}

=== ëŒ€í™” ê¸°ë¡ ===
{conversation_history}

=== ê´€ë ¨ ì§€ì‹ ===
{rag_context}

=== ì‚¬ìš©ì ì§ˆë¬¸ ===
{user_text}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""
    
    response = await self.llm.generate_response(prompt)
    await self.speak(response)
```

**ê°œì„ ì•ˆ 2: Memory + RAG (ê¶Œì¥)**:
```python
from langchain.memory import ConversationBufferWindowMemory

class AIOrchestrator:
    def __init__(self):
        # ìµœê·¼ 5í„´ë§Œ ê¸°ì–µ
        self.memory = ConversationBufferWindowMemory(k=5)
        self.rag = RAGEngine(...)
    
    async def generate_and_speak_response(self, user_text: str):
        # 1. RAG ê²€ìƒ‰
        rag_docs = await self.rag.retrieve(user_text)
        
        # 2. Memoryì—ì„œ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        history = self.memory.load_memory_variables({})
        
        # 3. LLM í˜¸ì¶œ
        response = await self.llm.generate(
            user_query=user_text,
            chat_history=history,
            rag_context=rag_docs,
        )
        
        # 4. Memory ì—…ë°ì´íŠ¸
        self.memory.save_context(
            {"input": user_text},
            {"output": response}
        )
        
        # 5. TTS
        await self.speak(response)
```

---

## í”„ë ˆì„ì›Œí¬ ë° ë„êµ¬

### 1. Pipecat AI (ê°•ë ¥ ì¶”ì²œ)

**ì¥ì **:
- âœ… All-in-one ì†”ë£¨ì…˜ (VAD, STT, LLM, TTS, Turn Detection)
- âœ… Production-ready
- âœ… Python ê¸°ë°˜ (ìš°ë¦¬ ì‹œìŠ¤í…œê³¼ í˜¸í™˜)
- âœ… í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°
- âœ… ì˜ ë¬¸ì„œí™”ë¨

**ì„¤ì¹˜**:
```bash
pip install pipecat-ai
```

**ê¸°ë³¸ ì‚¬ìš©**:
```python
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair

# VAD + Smart Turn
vad = SileroVADAnalyzer(params=VADParams(
    start_secs=0.2,
    stop_secs=0.2,
))

turn_detector = LocalSmartTurnAnalyzerV3()

# Context aggregator (ëŒ€í™” ê¸°ë¡ ìë™ ê´€ë¦¬)
user_agg, assistant_agg = LLMContextAggregatorPair(context)

# Pipeline êµ¬ì„±
pipeline = Pipeline([
    transport.input(),
    vad,
    stt,
    user_agg,
    llm,
    tts,
    transport.output(),
])
```

### 2. OpenAI Realtime API (í´ë¼ìš°ë“œ)

**Repository**: [openai/openai-realtime-agents](https://github.com/openai/openai-realtime-agents)

**ì¥ì **:
- âœ… ì™„ì „ managed (STT, LLM, TTS í†µí•©)
- âœ… ë‚®ì€ ë ˆì´í„´ì‹œ
- âœ… Automatic turn detection
- âœ… Barge-in ì§€ì›

**ë‹¨ì **:
- âŒ ë¹„ìš© ë†’ìŒ
- âŒ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì œí•œ
- âŒ ì˜¨í”„ë ˆë¯¸ìŠ¤ ë¶ˆê°€

### 3. LangChain + LangGraph (RAG/Memory)

**ì¥ì **:
- âœ… RAG êµ¬í˜„ ì‰¬ì›€
- âœ… Memory management
- âœ… Agent êµ¬ì¶• ê°€ëŠ¥

**ì„¤ì¹˜**:
```bash
pip install langchain langgraph chromadb
```

---

## êµ¬í˜„ ê¶Œì¥ì‚¬í•­

### ë‹¨ê³„ë³„ ê°œì„  ë¡œë“œë§µ

#### Phase 1: ê¸°ë³¸ ê°œì„  (1-2ì£¼)

**1.1. Turn Detection ê°•í™”**
```python
# AS-IS: ë‹¨ìˆœ 2ì´ˆ ì¹¨ë¬µ
if silence_duration > 2.0:
    return utterance

# TO-BE: Smart Turn í†µí•©
pip install pipecat-ai

from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3

self.turn_analyzer = LocalSmartTurnAnalyzerV3()

async def check_silence(self):
    if vad_silence_detected:
        audio = self.get_recent_audio(max_seconds=8)
        is_complete = await self.turn_analyzer.predict(audio)
        if is_complete:
            return self.get_and_reset_utterance()
```

**1.2. Barge-in ì „ëµ**
```python
# AS-IS: ë¬´ì¡°ê±´ ë¬´ì‹œ
if self.is_tts_playing:
    return False

# TO-BE: ë‹¨ì–´ ìˆ˜ ê¸°ë°˜
MIN_WORDS_FOR_INTERRUPT = 3

def should_process_speech(self, text: str) -> bool:
    if not self.is_tts_playing:
        return True
    
    word_count = len(text.split())
    if word_count >= MIN_WORDS_FOR_INTERRUPT:
        self.stop_tts()
        return True
    
    return False
```

**1.3. Context Management**
```python
# AS-IS
response = await self.llm.generate_response(user_text)

# TO-BE
conversation_history = self.get_conversation_history()
rag_context = await self.rag.query(user_text)

prompt = self.build_contextual_prompt(
    user_text,
    conversation_history,
    rag_context
)

response = await self.llm.generate_response(prompt)
```

#### Phase 2: ê³ ê¸‰ ê¸°ëŠ¥ (2-4ì£¼)

**2.1. Multimodal Turn Detection (Vogent Turn)**
```bash
pip install vogent-turn
```

```python
from vogent_turn import TurnDetector

self.turn_detector = TurnDetector(compile_model=True)

result = self.turn_detector.predict(
    audio,
    prev_line=self.last_ai_utterance,
    curr_line=self.current_user_text,
    return_probs=True
)
```

**2.2. Semantic Interruption**
```python
async def should_interrupt(self, text: str) -> bool:
    # Fast heuristic first
    if len(text.split()) < 2:
        return False
    
    # LLM-based semantic analysis
    prompt = f"""
ì‚¬ìš©ìê°€ "{text}"ë¼ê³  ë§í–ˆìŠµë‹ˆë‹¤.
ì´ê²ƒì´ ë§ì¥êµ¬ì¸ê°€ìš”, interruptionì¸ê°€ìš”?
ë‹µë³€: "ë§ì¥êµ¬" ë˜ëŠ” "interruption"
"""
    
    response = await self.llm.generate_fast(prompt)
    return "interruption" in response.lower()
```

**2.3. Agentic RAG**
```python
from langgraph.graph import StateGraph

# Multi-step reasoning
workflow = StateGraph(AgentState)
workflow.add_node("classify_intent", classify_node)
workflow.add_node("retrieve_docs", retrieve_node)
workflow.add_node("generate_response", generate_node)
workflow.add_conditional_edges(
    "classify_intent",
    route_by_intent,
    {
        "simple": "generate_response",
        "complex": "retrieve_docs",
    }
)
```

#### Phase 3: Production ìµœì í™” (4-6ì£¼)

**3.1. Pipecat ì „ì²´ í†µí•©**
- ê¸°ì¡´ ì½”ë“œë¥¼ Pipecat íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
- Unified frameworkë¡œ ê´€ë¦¬

**3.2. ì„±ëŠ¥ ìµœì í™”**
- Smart Turn ìºì‹±
- LLM response streaming
- ë³‘ë ¬ RAG ê²€ìƒ‰

**3.3. ëª¨ë‹ˆí„°ë§ & ë¡œê¹…**
- Turn detection ì •í™•ë„ ì¶”ì 
- Interruption íŒ¨í„´ ë¶„ì„
- ëŒ€í™” í’ˆì§ˆ ë©”íŠ¸ë¦­

---

## ê²°ë¡  ë° ì¶”ì²œ

### ğŸ¯ ìµœì¢… ì¶”ì²œ ìŠ¤íƒ

1. **Turn Detection**: **Smart Turn v3.2** (Pipecat)
   - í•œêµ­ì–´ ì§€ì› âœ…
   - ë¹ ë¦„ (10-100ms) âœ…
   - ë†’ì€ ì •í™•ë„ âœ…

2. **Barge-in**: **MinWordsInterruptionStrategy** (Phase 1) â†’ **Semantic** (Phase 2)
   - ê°„ë‹¨í•˜ê³  íš¨ê³¼ì 
   - ë§ì¥êµ¬ í•„í„°ë§ ê°€ëŠ¥

3. **Context**: **LangChain Memory** + **RAG**
   - Session ê¸°ë°˜ memory
   - ChromaDB RAG
   - ì „ì²´ ë§¥ë½ ìœ ì§€

4. **Framework**: **Pipecat AI**
   - í†µí•© ì†”ë£¨ì…˜
   - Production-ready
   - í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°

### ğŸ“š ì°¸ê³  ìë£Œ

- **Pipecat Documentation**: https://docs.pipecat.ai
- **Smart Turn**: https://github.com/pipecat-ai/smart-turn
- **Vogent Turn**: https://github.com/vogent/vogent-turn
- **LangGraph**: https://github.com/langchain-ai/langgraph
- **Crosstalk ë…¼ë¬¸**: https://github.com/tarzain/crosstalk

### ğŸš€ ì‹œì‘í•˜ê¸°

```bash
# 1. Pipecat ì„¤ì¹˜
pip install pipecat-ai

# 2. Smart Turn í…ŒìŠ¤íŠ¸
python -m pipecat.audio.turn.smart_turn.test

# 3. ìš°ë¦¬ ì‹œìŠ¤í…œì— í†µí•©
# - src/ai_voicebot/orchestrator/turn_detector.py (ìƒˆ íŒŒì¼)
# - src/ai_voicebot/orchestrator/barge_in_controller.py (ìˆ˜ì •)
```

---

**ì‘ì„±ì¼**: 2026-02-11  
**ë²„ì „**: v1.0
