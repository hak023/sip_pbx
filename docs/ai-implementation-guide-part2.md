# AI ì»´í¬ë„ŒíŠ¸ êµ¬í˜„ ê°€ì´ë“œ Part 2

ì´ ë¬¸ì„œëŠ” Part 1ì˜ ì—°ì†ì…ë‹ˆë‹¤.

**Part 1 ì»´í¬ë„ŒíŠ¸:** Audio Buffer, VAD, STT Client, TTS Client âœ…
**Part 2 ì»´í¬ë„ŒíŠ¸:** LLM Client, RAG Engine, Call Recorder, Knowledge Extractor

---

## 5. LLM Client (Gemini) ğŸ†•

### 5.1 ì™„ì „í•œ êµ¬í˜„

íŒŒì¼ ìœ„ì¹˜: `src/ai_voicebot/ai_pipeline/llm_client.py`

```python
import google.generativeai as genai
import asyncio
from typing import List, Dict, Optional
import structlog

logger = structlog.get_logger(__name__)


class LLMClient:
    """
    Google Gemini LLM Client
    
    ëŒ€í™” ìƒì„± ë° ì§€ì‹ ìœ ìš©ì„± íŒë‹¨ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: dict, api_key: str):
        """
        Args:
            config: LLM ì„¤ì •
                - model: "gemini-pro"
                - temperature: 0.7
                - max_tokens: 200
            api_key: Google API í‚¤
        """
        self.config = config
        
        # Gemini ì„¤ì •
        genai.configure(api_key=api_key)
        
        self.model = genai.GenerativeModel(
            model_name=config.get("model", "gemini-pro")
        )
        
        self.generation_config = genai.types.GenerationConfig(
            temperature=config.get("temperature", 0.7),
            top_p=config.get("top_p", 0.8),
            top_k=config.get("top_k", 40),
            max_output_tokens=config.get("max_tokens", 200),
        )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.conversation_history: List[Dict[str, str]] = []
        
        logger.info("LLMClient initialized", 
                   model=config.get("model"))
    
    async def generate_response(
        self, 
        user_text: str, 
        context_docs: List[str],
        system_prompt: Optional[str] = None
    ) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            user_text: ì‚¬ìš©ì ì§ˆë¬¸
            context_docs: RAG ê²€ìƒ‰ ê²°ê³¼ (ê´€ë ¨ ë¬¸ì„œ)
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            
        Returns:
            ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
        """
        try:
            # í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
            prompt = self._build_conversation_prompt(
                user_text, 
                context_docs, 
                system_prompt
            )
            
            # Gemini API í˜¸ì¶œ (ë¹„ë™ê¸°)
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content(
                    prompt,
                    generation_config=self.generation_config
                )
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            answer = response.text.strip()
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.conversation_history.append({
                "role": "user",
                "content": user_text
            })
            self.conversation_history.append({
                "role": "assistant",
                "content": answer
            })
            
            # íˆìŠ¤í† ë¦¬ ì œí•œ (ìµœê·¼ 10í„´)
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[-20:]
            
            logger.info("LLM response generated",
                       user_text_length=len(user_text),
                       response_length=len(answer))
            
            return answer
            
        except Exception as e:
            logger.error("LLM generation error", error=str(e))
            return "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _build_conversation_prompt(
        self, 
        user_text: str, 
        context_docs: List[str],
        system_prompt: Optional[str] = None
    ) -> str:
        """ëŒ€í™” í”„ë¡¬í”„íŠ¸ ì¡°ë¦½"""
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        if not system_prompt:
            system_prompt = (
                "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì •í™•í•œ AI ë¹„ì„œì…ë‹ˆë‹¤. "
                "ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , "
                "ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”. "
                "ë‹µë³€ì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”."
            )
        
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ
        context_str = ""
        if context_docs:
            context_str = "\n\n**ì°¸ê³  ì •ë³´:**\n" + "\n".join([
                f"- {doc}" for doc in context_docs
            ])
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        history_str = ""
        if self.conversation_history:
            recent_history = self.conversation_history[-10:]  # ìµœê·¼ 5í„´
            history_str = "\n\n**ì´ì „ ëŒ€í™”:**\n" + "\n".join([
                f"{'ì‚¬ìš©ì' if msg['role'] == 'user' else 'AI'}: {msg['content']}"
                for msg in recent_history
            ])
        
        # ì „ì²´ í”„ë¡¬í”„íŠ¸
        prompt = f"""{system_prompt}
{context_str}
{history_str}

**í˜„ì¬ ì§ˆë¬¸:**
ì‚¬ìš©ì: {user_text}

AI:"""
        
        return prompt
    
    async def judge_usefulness(
        self, 
        transcript: str, 
        speaker: str = "callee"
    ) -> Dict[str, any]:
        """
        í†µí™” ë‚´ìš©ì˜ ìœ ìš©ì„± íŒë‹¨ (ì§€ì‹ ì¶”ì¶œìš©)
        
        Args:
            transcript: í†µí™” ì „ì²´ í…ìŠ¤íŠ¸
            speaker: í™”ì (caller/callee)
            
        Returns:
            {
                "is_useful": bool,
                "confidence": float,
                "reason": str,
                "extracted_info": List[Dict]
            }
        """
        try:
            prompt = f"""ë‹¤ìŒ í†µí™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ í–¥í›„ AI ë¹„ì„œê°€ í™œìš©í•  ìˆ˜ ìˆëŠ” 
ìœ ìš©í•œ ì •ë³´ê°€ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

**ìœ ìš©í•œ ì •ë³´ ì˜ˆì‹œ:**
- ì•½ì† ì¼ì • (ì‹œê°„, ì¥ì†Œ)
- ì—°ë½ì²˜ ì •ë³´
- ì—…ë¬´ ì§€ì‹œì‚¬í•­
- ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€
- ê°œì¸ ì„ í˜¸ë„ (ì¢‹ì•„í•˜ëŠ” ìŒì‹, ì·¨ë¯¸ ë“±)

**í†µí™” ë‚´ìš© ({speaker}):**
{transcript}

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
  "is_useful": true/false,
  "confidence": 0.0-1.0,
  "reason": "íŒë‹¨ ì´ìœ ",
  "extracted_info": [
    {{
      "text": "ì¶”ì¶œí•  í…ìŠ¤íŠ¸",
      "category": "ì•½ì†|ì •ë³´|ì§€ì‹œ|ì„ í˜¸ë„|ê¸°íƒ€",
      "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]
    }}
  ]
}}

JSON:"""
            
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.3,  # ë” ê²°ì •ë¡ ì 
                        max_output_tokens=500
                    )
                )
            )
            
            # JSON íŒŒì‹±
            import json
            result_text = response.text.strip()
            
            # JSON ì¶”ì¶œ (```json ... ``` ì œê±°)
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(result_text)
            
            logger.info("Usefulness judgment completed",
                       is_useful=result.get("is_useful"),
                       confidence=result.get("confidence"))
            
            return result
            
        except Exception as e:
            logger.error("Usefulness judgment error", error=str(e))
            return {
                "is_useful": False,
                "confidence": 0.0,
                "reason": f"Error: {str(e)}",
                "extracted_info": []
            }
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history.clear()
        logger.info("LLM conversation history cleared")
    
    def get_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.conversation_history.copy()


# ì‚¬ìš© ì˜ˆì‹œ
async def example_usage():
    """LLMClient ì‚¬ìš© ì˜ˆì‹œ"""
    import os
    
    config = {
        "model": "gemini-pro",
        "temperature": 0.7,
        "max_tokens": 200
    }
    
    api_key = os.getenv("GEMINI_API_KEY")
    llm = LLMClient(config, api_key)
    
    # ë‹µë³€ ìƒì„±
    context_docs = [
        "ë‹¤ìŒ ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 10ì‹œì— íšŒì˜ê°€ ìˆìŠµë‹ˆë‹¤.",
        "íšŒì˜ ì¥ì†ŒëŠ” ë³¸ì‚¬ 3ì¸µ íšŒì˜ì‹¤ì…ë‹ˆë‹¤."
    ]
    
    answer = await llm.generate_response(
        user_text="ë‹¤ìŒ ì£¼ íšŒì˜ ì‹œê°„ì´ ì–¸ì œì¸ê°€ìš”?",
        context_docs=context_docs
    )
    print(f"AI: {answer}")
    
    # ìœ ìš©ì„± íŒë‹¨
    transcript = """
    ë°œì‹ ì: ë‹¤ìŒ ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 10ì‹œì— íšŒì˜ ìˆì£ ?
    ì°©ì‹ ì: ë„¤, ë§ìŠµë‹ˆë‹¤. ë³¸ì‚¬ 3ì¸µ íšŒì˜ì‹¤ì—ì„œ ëµ™ê² ìŠµë‹ˆë‹¤.
    """
    
    judgment = await llm.judge_usefulness(transcript, speaker="callee")
    if judgment["is_useful"]:
        print(f"ìœ ìš©í•œ ì •ë³´: {judgment['extracted_info']}")
```

---

## 6. RAG Engine ğŸ†•

### 6.1 ì™„ì „í•œ êµ¬í˜„

íŒŒì¼ ìœ„ì¹˜: `src/ai_voicebot/ai_pipeline/rag_engine.py`

```python
from typing import List, Dict, Optional
from dataclasses import dataclass
import asyncio
import structlog

logger = structlog.get_logger(__name__)


@dataclass
class Document:
    """ê²€ìƒ‰ëœ ë¬¸ì„œ"""
    id: str
    text: str
    score: float
    metadata: Dict


class RAGEngine:
    """
    RAG (Retrieval-Augmented Generation) Engine
    
    Vector DB ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ì¬ìˆœìœ„í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self, 
        vector_db,  # VectorDB ì¸ìŠ¤í„´ìŠ¤
        embedder,   # TextEmbedder ì¸ìŠ¤í„´ìŠ¤
        top_k: int = 3,
        similarity_threshold: float = 0.7,
        reranking_enabled: bool = False
    ):
        """
        Args:
            vector_db: Vector DB í´ë¼ì´ì–¸íŠ¸
            embedder: Text Embedder ì¸ìŠ¤í„´ìŠ¤
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            reranking_enabled: ì¬ìˆœìœ„í™” í™œì„±í™”
        """
        self.vector_db = vector_db
        self.embedder = embedder
        self.top_k = top_k
        self.similarity_threshold = similarity_threshold
        self.reranking_enabled = reranking_enabled
        
        logger.info("RAGEngine initialized", 
                   top_k=top_k,
                   threshold=similarity_threshold)
    
    async def search(
        self, 
        query: str, 
        owner_filter: Optional[str] = None
    ) -> List[Document]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì§ˆë¬¸
            owner_filter: ì‚¬ìš©ì ID í•„í„° (ì°©ì‹ ì ì „ìš© ì§€ì‹)
            
        Returns:
            ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ top_kê°œ)
        """
        try:
            # 1. ì§ˆë¬¸ ì„ë² ë”©
            query_embedding = await self.embedder.embed(query)
            
            # 2. Vector DB ê²€ìƒ‰
            search_results = await self.vector_db.search(
                vector=query_embedding,
                top_k=self.top_k * 2,  # ì¬ìˆœìœ„í™”ë¥¼ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
                filter={"owner": owner_filter} if owner_filter else None
            )
            
            # 3. Document ê°ì²´ ë³€í™˜
            documents = [
                Document(
                    id=result["id"],
                    text=result["text"],
                    score=result["score"],
                    metadata=result.get("metadata", {})
                )
                for result in search_results
            ]
            
            # 4. ìœ ì‚¬ë„ í•„í„°ë§
            documents = [
                doc for doc in documents
                if doc.score >= self.similarity_threshold
            ]
            
            # 5. ì¬ìˆœìœ„í™” (ì„ íƒ)
            if self.reranking_enabled and documents:
                documents = await self._rerank(query, documents)
            
            # 6. Top-K ë°˜í™˜
            documents = documents[:self.top_k]
            
            logger.info("RAG search completed",
                       query_length=len(query),
                       results_count=len(documents))
            
            return documents
            
        except Exception as e:
            logger.error("RAG search error", error=str(e))
            return []
    
    async def _rerank(
        self, 
        query: str, 
        documents: List[Document]
    ) -> List[Document]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”
        
        ë‹¨ìˆœ ë²¡í„° ìœ ì‚¬ë„ê°€ ì•„ë‹Œ ì‹¤ì œ ê´€ë ¨ì„± ê¸°ë°˜ ì¬ìˆœìœ„í™”
        (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ê¸¸ì´ì™€ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ êµ¬í˜„)
        """
        try:
            # ì§ˆë¬¸ì˜ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            query_words = set(query.lower().split())
            
            # ê° ë¬¸ì„œì˜ ì¬ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
            for doc in documents:
                doc_words = set(doc.text.lower().split())
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨
                overlap = len(query_words & doc_words)
                keyword_score = overlap / len(query_words) if query_words else 0
                
                # ë¬¸ì„œ ê¸¸ì´ íŒ¨ë„í‹° (ë„ˆë¬´ ê¸¸ë©´ ê°ì )
                length_score = 1.0 if len(doc.text) < 300 else 0.8
                
                # ìµœì¢… ì ìˆ˜ (ì›ë˜ ì ìˆ˜ 70% + í‚¤ì›Œë“œ 20% + ê¸¸ì´ 10%)
                doc.score = (
                    doc.score * 0.7 +
                    keyword_score * 0.2 +
                    length_score * 0.1
                )
            
            # ì¬ì •ë ¬
            documents.sort(key=lambda d: d.score, reverse=True)
            
            logger.debug("Reranking completed", count=len(documents))
            return documents
            
        except Exception as e:
            logger.error("Reranking error", error=str(e))
            return documents
    
    async def search_with_expansion(
        self, 
        query: str, 
        owner_filter: Optional[str] = None
    ) -> List[Document]:
        """
        ì¿¼ë¦¬ í™•ì¥ì„ ì‚¬ìš©í•œ ê²€ìƒ‰ (ê³ ê¸‰)
        
        ì›ë³¸ ì¿¼ë¦¬ + í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ë” ë§ì€ ê²°ê³¼ í™•ë³´
        """
        # ì›ë³¸ ê²€ìƒ‰
        original_results = await self.search(query, owner_filter)
        
        # ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´, ê´€ë ¨ì–´)
        expanded_query = await self._expand_query(query)
        
        if expanded_query != query:
            # í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
            expanded_results = await self.search(expanded_query, owner_filter)
            
            # ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
            seen_ids = {doc.id for doc in original_results}
            for doc in expanded_results:
                if doc.id not in seen_ids:
                    original_results.append(doc)
                    seen_ids.add(doc.id)
            
            # ì¬ì •ë ¬
            original_results.sort(key=lambda d: d.score, reverse=True)
            original_results = original_results[:self.top_k]
        
        return original_results
    
    async def _expand_query(self, query: str) -> str:
        """
        ì¿¼ë¦¬ í™•ì¥ (ê°„ë‹¨í•œ ë™ì˜ì–´ ì¹˜í™˜)
        
        ì‹¤ì œë¡œëŠ” LLMì„ ì‚¬ìš©í•˜ê±°ë‚˜ í•œêµ­ì–´ ë™ì˜ì–´ ì‚¬ì „ í™œìš© ê°€ëŠ¥
        """
        # ê°„ë‹¨í•œ ë™ì˜ì–´ ë§¤í•‘
        synonyms = {
            "íšŒì˜": ["ë¯¸íŒ…", "íšŒì˜", "ëª¨ì„"],
            "ì‹œê°„": ["ì‹œê°„", "ì‹œê°", "íƒ€ì„"],
            "ì¥ì†Œ": ["ì¥ì†Œ", "ìœ„ì¹˜", "ê³³"],
        }
        
        expanded = query
        for word, syns in synonyms.items():
            if word in query:
                # ì²« ë²ˆì§¸ ë™ì˜ì–´ë¡œ ì¹˜í™˜
                expanded = query.replace(word, syns[0])
                break
        
        return expanded


# ì‚¬ìš© ì˜ˆì‹œ
async def example_usage():
    """RAGEngine ì‚¬ìš© ì˜ˆì‹œ"""
    from src.ai_voicebot.knowledge.vector_db import ChromaDBClient
    from src.ai_voicebot.knowledge.embedder import TextEmbedder
    
    # ì´ˆê¸°í™”
    vector_db = ChromaDBClient()
    embedder = TextEmbedder()
    
    rag = RAGEngine(
        vector_db=vector_db,
        embedder=embedder,
        top_k=3,
        similarity_threshold=0.7
    )
    
    # ê²€ìƒ‰
    query = "ë‹¤ìŒ ì£¼ íšŒì˜ ì‹œê°„ì´ ì–¸ì œì¸ê°€ìš”?"
    documents = await rag.search(
        query=query,
        owner_filter="user_1004"  # ì°©ì‹ ì ì „ìš©
    )
    
    # ê²°ê³¼ ì¶œë ¥
    for i, doc in enumerate(documents, 1):
        print(f"{i}. (ì ìˆ˜: {doc.score:.2f}) {doc.text}")
    
    # LLMì— ì „ë‹¬
    context_docs = [doc.text for doc in documents]
    answer = await llm.generate_response(query, context_docs)
```

---

## 7. Call Recorder ğŸ†•

### 7.1 ì™„ì „í•œ êµ¬í˜„

íŒŒì¼ ìœ„ì¹˜: `src/ai_voicebot/recording/recorder.py`

```python
import asyncio
import wave
import os
from pathlib import Path
from typing import Optional
from datetime import datetime
import json
import structlog

logger = structlog.get_logger(__name__)


class CallRecorder:
    """
    í†µí™” ë…¹ìŒ ë° ì €ì¥
    
    - ì–‘ë°©í–¥ RTP ìŠ¤íŠ¸ë¦¼ ë…¹ìŒ
    - í™”ì ë¶„ë¦¬ (caller/callee ë³„ë„ WAV)
    - ë¯¹ì‹± (ë‹¨ì¼ WAV)
    - ë©”íƒ€ë°ì´í„° ì €ì¥
    """
    
    def __init__(
        self,
        output_dir: str = "./recordings",
        sample_rate: int = 16000,
        channels: int = 1,
        sample_width: int = 2  # 16-bit
    ):
        """
        Args:
            output_dir: ë…¹ìŒ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
            sample_rate: ìƒ˜í”Œë ˆì´íŠ¸ (Hz)
            channels: ì±„ë„ ìˆ˜ (1=mono)
            sample_width: ìƒ˜í”Œ ë„ˆë¹„ (bytes, 2=16-bit)
        """
        self.output_dir = Path(output_dir)
        self.sample_rate = sample_rate
        self.channels = channels
        self.sample_width = sample_width
        
        # ë…¹ìŒ ë²„í¼
        self.caller_buffer: list[bytes] = []
        self.callee_buffer: list[bytes] = []
        self.mixed_buffer: list[bytes] = []
        
        # ë…¹ìŒ ìƒíƒœ
        self.is_recording = False
        self.call_id: Optional[str] = None
        self.start_time: Optional[datetime] = None
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("CallRecorder initialized", 
                   output_dir=str(self.output_dir))
    
    def start_recording(self, call_id: str):
        """ë…¹ìŒ ì‹œì‘"""
        if self.is_recording:
            logger.warning("Already recording", call_id=self.call_id)
            return
        
        self.is_recording = True
        self.call_id = call_id
        self.start_time = datetime.now()
        
        # ë²„í¼ ì´ˆê¸°í™”
        self.caller_buffer.clear()
        self.callee_buffer.clear()
        self.mixed_buffer.clear()
        
        logger.info("Recording started", call_id=call_id)
    
    def add_caller_audio(self, audio_data: bytes):
        """ë°œì‹ ì ì˜¤ë””ì˜¤ ì¶”ê°€"""
        if not self.is_recording:
            return
        
        self.caller_buffer.append(audio_data)
        
        # ë¯¹ì‹± ë²„í¼ì—ë„ ì¶”ê°€ (caller ì±„ë„)
        self._add_to_mixed(audio_data, is_caller=True)
    
    def add_callee_audio(self, audio_data: bytes):
        """ì°©ì‹ ì ì˜¤ë””ì˜¤ ì¶”ê°€"""
        if not self.is_recording:
            return
        
        self.callee_buffer.append(audio_data)
        
        # ë¯¹ì‹± ë²„í¼ì—ë„ ì¶”ê°€ (callee ì±„ë„)
        self._add_to_mixed(audio_data, is_caller=False)
    
    def _add_to_mixed(self, audio_data: bytes, is_caller: bool):
        """ë¯¹ì‹± ë²„í¼ì— ì˜¤ë””ì˜¤ ì¶”ê°€"""
        # ê°„ë‹¨í•œ ë¯¹ì‹±: ê·¸ëƒ¥ append (ì‹¤ì œë¡œëŠ” ì‹œê°„ ë™ê¸°í™” í•„ìš”)
        # TODO: íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ë™ê¸°í™”
        self.mixed_buffer.append(audio_data)
    
    async def stop_recording(self) -> dict:
        """
        ë…¹ìŒ ì¤‘ì§€ ë° íŒŒì¼ ì €ì¥
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ì •ë³´ dict
        """
        if not self.is_recording:
            logger.warning("Not recording")
            return {}
        
        self.is_recording = False
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (call_idë³„)
        call_dir = self.output_dir / self.call_id
        call_dir.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ê²½ë¡œ
        caller_path = call_dir / "caller.wav"
        callee_path = call_dir / "callee.wav"
        mixed_path = call_dir / "mixed.wav"
        metadata_path = call_dir / "metadata.json"
        
        # WAV íŒŒì¼ ì €ì¥
        await asyncio.gather(
            self._save_wav(caller_path, self.caller_buffer),
            self._save_wav(callee_path, self.callee_buffer),
            self._save_wav(mixed_path, self.mixed_buffer)
        )
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "call_id": self.call_id,
            "start_time": self.start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": duration,
            "sample_rate": self.sample_rate,
            "channels": self.channels,
            "files": {
                "caller": str(caller_path),
                "callee": str(callee_path),
                "mixed": str(mixed_path)
            }
        }
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        logger.info("Recording saved",
                   call_id=self.call_id,
                   duration=duration,
                   caller_frames=len(self.caller_buffer),
                   callee_frames=len(self.callee_buffer))
        
        # ë²„í¼ ì •ë¦¬
        self.caller_buffer.clear()
        self.callee_buffer.clear()
        self.mixed_buffer.clear()
        
        return metadata
    
    async def _save_wav(self, filepath: Path, audio_buffer: list[bytes]):
        """WAV íŒŒì¼ ì €ì¥"""
        try:
            # ë¹„ë™ê¸° íŒŒì¼ ì“°ê¸°
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                self._write_wav_file,
                filepath,
                audio_buffer
            )
            
            logger.debug("WAV file saved", path=str(filepath))
            
        except Exception as e:
            logger.error("WAV save error", path=str(filepath), error=str(e))
    
    def _write_wav_file(self, filepath: Path, audio_buffer: list[bytes]):
        """WAV íŒŒì¼ ì“°ê¸° (ë™ê¸°)"""
        with wave.open(str(filepath), 'wb') as wav_file:
            wav_file.setnchannels(self.channels)
            wav_file.setsampwidth(self.sample_width)
            wav_file.setframerate(self.sample_rate)
            
            # ëª¨ë“  ì˜¤ë””ì˜¤ ë°ì´í„° ì“°ê¸°
            for audio_data in audio_buffer:
                wav_file.writeframes(audio_data)


# ì‚¬ìš© ì˜ˆì‹œ
async def example_usage():
    """CallRecorder ì‚¬ìš© ì˜ˆì‹œ"""
    recorder = CallRecorder(output_dir="./recordings")
    
    # ë…¹ìŒ ì‹œì‘
    recorder.start_recording(call_id="call_123")
    
    # í†µí™” ì¤‘ ì˜¤ë””ì˜¤ ì¶”ê°€
    while in_call:
        # RTP íŒ¨í‚· ìˆ˜ì‹ 
        caller_audio = await receive_caller_rtp()
        callee_audio = await receive_callee_rtp()
        
        recorder.add_caller_audio(caller_audio)
        recorder.add_callee_audio(callee_audio)
    
    # ë…¹ìŒ ì¤‘ì§€ ë° ì €ì¥
    metadata = await recorder.stop_recording()
    print(f"Saved: {metadata['files']}")
```

---

## 8. Knowledge Extractor ğŸ†•

### 8.1 ì™„ì „í•œ êµ¬í˜„

íŒŒì¼ ìœ„ì¹˜: `src/ai_voicebot/knowledge/knowledge_extractor.py`

```python
from typing import List, Dict
import asyncio
from pathlib import Path
import json
import structlog

logger = structlog.get_logger(__name__)


class KnowledgeExtractor:
    """
    í†µí™” ë…¹ìŒì—ì„œ ìœ ìš©í•œ ì§€ì‹ì„ ì¶”ì¶œí•˜ì—¬ Vector DBì— ì €ì¥
    
    ì›Œí¬í”Œë¡œìš°:
    1. ë…¹ìŒ íŒŒì¼ ë¡œë“œ
    2. ì „ì‚¬ í…ìŠ¤íŠ¸ ë¡œë“œ
    3. LLM ìœ ìš©ì„± íŒë‹¨
    4. í…ìŠ¤íŠ¸ ì²­í‚¹
    5. ì„ë² ë”© ìƒì„±
    6. Vector DB ì €ì¥
    """
    
    def __init__(
        self,
        llm_client,      # LLMClient ì¸ìŠ¤í„´ìŠ¤
        embedder,        # TextEmbedder ì¸ìŠ¤í„´ìŠ¤
        vector_db,       # VectorDB ì¸ìŠ¤í„´ìŠ¤
        min_confidence: float = 0.7,
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        """
        Args:
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸
            embedder: í…ìŠ¤íŠ¸ ì„ë² ë”
            vector_db: Vector DB í´ë¼ì´ì–¸íŠ¸
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ (ìœ ìš©ì„± íŒë‹¨)
            chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì)
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë© (ë¬¸ì)
        """
        self.llm = llm_client
        self.embedder = embedder
        self.vector_db = vector_db
        self.min_confidence = min_confidence
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        logger.info("KnowledgeExtractor initialized",
                   min_confidence=min_confidence)
    
    async def extract_from_call(
        self, 
        call_id: str,
        transcript_path: str,
        owner_id: str,
        speaker: str = "callee"
    ) -> Dict:
        """
        í†µí™”ì—ì„œ ì§€ì‹ ì¶”ì¶œ
        
        Args:
            call_id: í†µí™” ID
            transcript_path: ì „ì‚¬ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            owner_id: ì†Œìœ ì ID (ì°©ì‹ ì ID)
            speaker: ì¶”ì¶œ ëŒ€ìƒ í™”ì (caller/callee)
            
        Returns:
            {
                "success": bool,
                "extracted_count": int,
                "confidence": float
            }
        """
        try:
            # 1. ì „ì‚¬ í…ìŠ¤íŠ¸ ë¡œë“œ
            transcript = await self._load_transcript(transcript_path)
            if not transcript:
                logger.warning("Empty transcript", call_id=call_id)
                return {"success": False, "extracted_count": 0}
            
            # 2. í™”ì í•„í„°ë§ (callee ë°œí™”ë§Œ)
            speaker_text = self._filter_by_speaker(transcript, speaker)
            if not speaker_text:
                logger.info("No text from target speaker", 
                          call_id=call_id, 
                          speaker=speaker)
                return {"success": False, "extracted_count": 0}
            
            # 3. LLM ìœ ìš©ì„± íŒë‹¨
            judgment = await self.llm.judge_usefulness(
                transcript=speaker_text,
                speaker=speaker
            )
            
            if not judgment["is_useful"]:
                logger.info("Not useful content", 
                          call_id=call_id,
                          reason=judgment["reason"])
                return {"success": True, "extracted_count": 0}
            
            if judgment["confidence"] < self.min_confidence:
                logger.info("Low confidence", 
                          call_id=call_id,
                          confidence=judgment["confidence"])
                return {"success": True, "extracted_count": 0}
            
            # 4. ìœ ìš©í•œ ì •ë³´ ì¶”ì¶œ
            extracted_info = judgment.get("extracted_info", [])
            if not extracted_info:
                # LLMì´ êµ¬ì²´ì  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•œ ê²½ìš°, ì „ì²´ í…ìŠ¤íŠ¸ ì²­í‚¹
                extracted_info = [
                    {
                        "text": speaker_text,
                        "category": "ê¸°íƒ€",
                        "keywords": []
                    }
                ]
            
            # 5. ì²­í‚¹ ë° ì„ë² ë”©
            stored_count = 0
            for idx, info in enumerate(extracted_info):
                text = info["text"]
                chunks = self._chunk_text(text)
                
                for chunk_idx, chunk in enumerate(chunks):
                    # ì„ë² ë”© ìƒì„±
                    embedding = await self.embedder.embed(chunk)
                    
                    # Vector DB ì €ì¥
                    doc_id = f"{call_id}_chunk_{idx}_{chunk_idx}"
                    metadata = {
                        "call_id": call_id,
                        "owner": owner_id,
                        "speaker": speaker,
                        "category": info.get("category", "ê¸°íƒ€"),
                        "keywords": info.get("keywords", []),
                        "chunk_index": chunk_idx,
                        "confidence": judgment["confidence"]
                    }
                    
                    await self.vector_db.upsert(
                        doc_id=doc_id,
                        embedding=embedding,
                        text=chunk,
                        metadata=metadata
                    )
                    
                    stored_count += 1
            
            logger.info("Knowledge extracted and stored",
                       call_id=call_id,
                       chunks_stored=stored_count,
                       confidence=judgment["confidence"])
            
            return {
                "success": True,
                "extracted_count": stored_count,
                "confidence": judgment["confidence"]
            }
            
        except Exception as e:
            logger.error("Knowledge extraction error", 
                        call_id=call_id, 
                        error=str(e))
            return {"success": False, "extracted_count": 0}
    
    async def _load_transcript(self, path: str) -> str:
        """ì „ì‚¬ í…ìŠ¤íŠ¸ ë¡œë“œ"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error("Transcript load error", path=path, error=str(e))
            return ""
    
    def _filter_by_speaker(self, transcript: str, speaker: str) -> str:
        """í™”ìë³„ ë°œí™” í•„í„°ë§"""
        # ê°„ë‹¨í•œ íŒŒì‹± (í˜•ì‹: "í™”ì: í…ìŠ¤íŠ¸")
        lines = transcript.split('\n')
        speaker_lines = []
        
        speaker_label = "ì°©ì‹ ì" if speaker == "callee" else "ë°œì‹ ì"
        
        for line in lines:
            if line.startswith(f"{speaker_label}:"):
                text = line.split(':', 1)[1].strip()
                speaker_lines.append(text)
        
        return ' '.join(speaker_lines)
    
    def _chunk_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì²­í‚¹ (ì˜¤ë²„ë© í¬í•¨)"""
        if len(text) <= self.chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end]
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ)
            if end < len(text):
                last_period = max(
                    chunk.rfind('.'),
                    chunk.rfind('!'),
                    chunk.rfind('?')
                )
                if last_period > 0:
                    chunk = chunk[:last_period + 1]
                    end = start + last_period + 1
            
            chunks.append(chunk.strip())
            
            # ë‹¤ìŒ ì‹œì‘ì  (ì˜¤ë²„ë© ì ìš©)
            start = end - self.chunk_overlap
        
        return chunks


# ì‚¬ìš© ì˜ˆì‹œ
async def example_usage():
    """KnowledgeExtractor ì‚¬ìš© ì˜ˆì‹œ"""
    from src.ai_voicebot.ai_pipeline.llm_client import LLMClient
    from src.ai_voicebot.knowledge.embedder import TextEmbedder
    from src.ai_voicebot.knowledge.vector_db import ChromaDBClient
    
    # ì´ˆê¸°í™”
    llm = LLMClient(config, api_key)
    embedder = TextEmbedder()
    vector_db = ChromaDBClient()
    
    extractor = KnowledgeExtractor(
        llm_client=llm,
        embedder=embedder,
        vector_db=vector_db,
        min_confidence=0.7
    )
    
    # í†µí™”ì—ì„œ ì§€ì‹ ì¶”ì¶œ
    result = await extractor.extract_from_call(
        call_id="call_123",
        transcript_path="./recordings/call_123/transcript.txt",
        owner_id="user_1004",
        speaker="callee"
    )
    
    if result["success"]:
        print(f"Extracted {result['extracted_count']} knowledge chunks")
```

---

## 9. í†µí•© ì˜ˆì‹œ

### 9.1 ì „ì²´ íë¦„ í†µí•©

íŒŒì¼ ìœ„ì¹˜: `src/ai_voicebot/orchestrator.py` ìˆ˜ì •

```python
# AI Orchestratorì—ì„œ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ í†µí•© ì‚¬ìš©

from .audio_buffer import AudioBuffer
from .vad_detector import VADDetector
from .ai_pipeline.stt_client import STTClient
from .ai_pipeline.tts_client import TTSClient
from .ai_pipeline.llm_client import LLMClient
from .ai_pipeline.rag_engine import RAGEngine
from .recording.recorder import CallRecorder
from .knowledge.knowledge_extractor import KnowledgeExtractor

class AIOrchestrator:
    def __init__(self, config):
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.audio_buffer = AudioBuffer(config.audio_buffer)
        self.vad = VADDetector(config.vad)
        self.stt = STTClient(config.stt)
        self.tts = TTSClient(config.tts)
        self.llm = LLMClient(config.llm, api_key)
        self.rag = RAGEngine(vector_db, embedder, config.rag)
        self.recorder = CallRecorder(config.recording)
        self.extractor = KnowledgeExtractor(
            self.llm, embedder, vector_db, config.knowledge
        )
    
    async def handle_call(self, call_id, caller_info):
        # ë…¹ìŒ ì‹œì‘
        self.recorder.start_recording(call_id)
        
        # ì˜¤ë””ì˜¤ ë²„í¼ ì‹œì‘
        await self.audio_buffer.start()
        
        # STT ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
        await self.stt.start_stream(self.on_stt_result)
        
        # ì¸ì‚¬ë§ ì¬ìƒ
        await self.play_greeting()
    
    async def on_audio_packet(self, rtp_packet):
        # ë…¹ìŒ
        if rtp_packet.direction == "caller":
            self.recorder.add_caller_audio(rtp_packet.payload)
        
        # ë²„í¼ë§
        await self.audio_buffer.add_packet(rtp_packet)
        
        # í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
        frame = await self.audio_buffer.get_frame()
        if frame:
            # VAD ê²€ì‚¬
            is_speech = self.vad.detect(frame)
            
            # Barge-in í™•ì¸
            if self.vad.is_barge_in() and self.is_speaking:
                await self.stop_speaking()
            
            # STTë¡œ ì „ì†¡
            await self.stt.send_audio(frame)
    
    async def generate_and_speak_response(self, user_text):
        # RAG ê²€ìƒ‰
        documents = await self.rag.search(user_text, owner_filter=self.callee_id)
        context_docs = [doc.text for doc in documents]
        
        # LLM ë‹µë³€ ìƒì„±
        response_text = await self.llm.generate_response(user_text, context_docs)
        
        # TTS ì¬ìƒ
        await self.speak(response_text)
    
    async def stop_call(self):
        # STT ì¤‘ì§€
        await self.stt.stop_stream()
        
        # ì˜¤ë””ì˜¤ ë²„í¼ ì¤‘ì§€
        await self.audio_buffer.stop()
        
        # ë…¹ìŒ ì €ì¥
        metadata = await self.recorder.stop_recording()
        
        # ì§€ì‹ ì¶”ì¶œ (ë¹„ë™ê¸°, ë°±ê·¸ë¼ìš´ë“œ)
        asyncio.create_task(
            self.extractor.extract_from_call(
                call_id=self.call_id,
                transcript_path=metadata["transcript_path"],
                owner_id=self.callee_id
            )
        )
```

---

## 10. í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ

### 10.1 í†µí•© í…ŒìŠ¤íŠ¸

```python
# tests/integration/test_ai_workflow.py

@pytest.mark.asyncio
async def test_full_ai_conversation_flow():
    """ì „ì²´ AI ëŒ€í™” íë¦„ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    # 1. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    config = load_test_config()
    orchestrator = AIOrchestrator(config)
    
    # 2. í†µí™” ì‹œì‘
    await orchestrator.handle_call(
        call_id="test_call_001",
        caller_info={"caller": "1004", "callee": "1008"}
    )
    
    # 3. ì˜¤ë””ì˜¤ ì „ì†¡ ì‹œë®¬ë ˆì´ì…˜
    test_audio = load_test_audio("test_question.wav")
    await orchestrator.on_audio_packet(test_audio)
    
    # 4. ì‘ë‹µ ëŒ€ê¸°
    await asyncio.sleep(5)
    
    # 5. ë…¹ìŒ í™•ì¸
    recordings = list(Path("./recordings/test_call_001").glob("*.wav"))
    assert len(recordings) == 3  # caller, callee, mixed
    
    # 6. í†µí™” ì¢…ë£Œ
    await orchestrator.stop_call()
```

---

**êµ¬í˜„ ê°€ì´ë“œ ì™„ë£Œ! ğŸ‰**

ì´ì œ ëª¨ë“  8ê°œ ì»´í¬ë„ŒíŠ¸ì˜ ìƒì„¸ êµ¬í˜„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

