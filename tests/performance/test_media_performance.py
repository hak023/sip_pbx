"""Media Performance ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

ë¯¸ë””ì–´ ì²˜ë¦¬ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
"""

import pytest
import time
import asyncio
from datetime import datetime

from src.media.performance import PerformanceMeasurement, PerformanceReport
from src.media.port_pool import PortPoolManager
from src.media.session_manager import MediaSessionManager
from src.media.rtp_packet import RTPPacket, RTPHeader
from src.config.models import PortPoolConfig
from src.common.logger import setup_logging


@pytest.fixture(scope="module", autouse=True)
def setup_test_logging():
    """í…ŒìŠ¤íŠ¸ìš© ë¡œê¹… ì„¤ì •"""
    setup_logging(level="INFO", format_type="text")


@pytest.fixture
def performance_measurement():
    """ì„±ëŠ¥ ì¸¡ì •ê¸°"""
    return PerformanceMeasurement(mode="bypass")


@pytest.fixture
def port_pool():
    """í¬íŠ¸ í’€"""
    config = PortPoolConfig(start=20000, end=30000)
    return PortPoolManager(config=config)


@pytest.fixture
def media_session_manager(port_pool):
    """ë¯¸ë””ì–´ ì„¸ì…˜ ê´€ë¦¬ì"""
    return MediaSessionManager(port_pool=port_pool)


def create_test_rtp_packet(
    sequence: int = 1,
    timestamp: int = 160,
    payload_size: int = 160,
) -> RTPPacket:
    """í…ŒìŠ¤íŠ¸ìš© RTP íŒ¨í‚· ìƒì„±
    
    Args:
        sequence: ì‹œí€€ìŠ¤ ë²ˆí˜¸
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        payload_size: í˜ì´ë¡œë“œ í¬ê¸°
        
    Returns:
        RTP íŒ¨í‚·
    """
    header = RTPHeader(
        version=2,
        padding=False,
        extension=False,
        csrc_count=0,
        marker=False,
        payload_type=0,  # PCMU
        sequence_number=sequence,
        timestamp=timestamp,
        ssrc=12345,
    )
    
    payload = b'\x00' * payload_size
    
    return RTPPacket(header=header, payload=payload)


class TestPerformanceMeasurement:
    """ì„±ëŠ¥ ì¸¡ì • ê¸°ë³¸ í…ŒìŠ¤íŠ¸"""
    
    def test_measurement_creation(self, performance_measurement):
        """ì¸¡ì •ê¸° ìƒì„±"""
        assert performance_measurement is not None
        assert performance_measurement.mode == "bypass"
    
    def test_start_stop(self, performance_measurement):
        """ì‹œì‘/ì¤‘ì§€"""
        performance_measurement.start()
        time.sleep(0.1)
        performance_measurement.stop()
        
        duration = performance_measurement.get_duration()
        assert duration >= 0.1
    
    def test_record_latency(self, performance_measurement):
        """ì§€ì—° ì‹œê°„ ê¸°ë¡"""
        performance_measurement.start()
        
        performance_measurement.record_latency(
            latency_ms=5.5,
            packet_size=160,
            direction="caller_to_callee"
        )
        
        assert len(performance_measurement.measurements) == 1
        assert performance_measurement.total_packets == 1
        assert performance_measurement.total_bytes == 160
    
    def test_record_packet_loss(self, performance_measurement):
        """íŒ¨í‚· ì†ì‹¤ ê¸°ë¡"""
        performance_measurement.record_packet_loss(5)
        
        assert performance_measurement.packets_lost == 5
    
    def test_get_stats(self, performance_measurement):
        """í†µê³„ ì¡°íšŒ"""
        performance_measurement.start()
        
        # ì—¬ëŸ¬ ì¸¡ì • ê¸°ë¡
        for i in range(10):
            performance_measurement.record_latency(
                latency_ms=5.0 + i * 0.5,
                packet_size=160,
            )
        
        time.sleep(0.1)
        performance_measurement.stop()
        
        stats = performance_measurement.get_stats(concurrent_calls=1)
        
        assert stats.avg_latency_ms > 0
        assert stats.total_packets == 10
        assert stats.packets_per_second > 0
        assert stats.test_duration_seconds > 0
    
    def test_reset(self, performance_measurement):
        """ì¸¡ì • ì´ˆê¸°í™”"""
        performance_measurement.start()
        performance_measurement.record_latency(5.0, 160)
        performance_measurement.reset()
        
        assert len(performance_measurement.measurements) == 0
        assert performance_measurement.total_packets == 0


class TestPerformanceReport:
    """ì„±ëŠ¥ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸"""
    
    def test_generate_report(self, performance_measurement):
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        performance_measurement.start()
        
        for i in range(100):
            performance_measurement.record_latency(
                latency_ms=3.0 + i * 0.01,
                packet_size=160,
            )
        
        time.sleep(0.1)
        performance_measurement.stop()
        
        stats = performance_measurement.get_stats(concurrent_calls=1)
        report = PerformanceReport.generate_report(stats)
        
        assert "Media Performance Test Report" in report
        assert "Latency Metrics" in report
        assert "Throughput Metrics" in report
        assert "Packet Loss" in report


@pytest.mark.benchmark
class TestMediaPerformanceBenchmark:
    """ë¯¸ë””ì–´ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def test_rtp_packet_parsing_performance(self):
        """RTP íŒ¨í‚· íŒŒì‹± ì„±ëŠ¥"""
        measurement = PerformanceMeasurement(mode="bypass")
        measurement.start()
        
        # 1000ê°œ íŒ¨í‚· íŒŒì‹±
        for i in range(1000):
            start = time.perf_counter()
            
            packet = create_test_rtp_packet(sequence=i)
            
            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            
            measurement.record_latency(latency_ms, 160)
        
        measurement.stop()
        
        stats = measurement.get_stats(concurrent_calls=1)
        
        print(f"\nğŸ” RTP Packet Parsing Performance:")
        print(f"   Average: {stats.avg_latency_ms:.4f} ms")
        print(f"   P99: {stats.p99_latency_ms:.4f} ms")
        print(f"   Throughput: {stats.packets_per_second:.2f} packets/sec")
        
        # íŒŒì‹±ì€ ë§¤ìš° ë¹¨ë¼ì•¼ í•¨
        assert stats.avg_latency_ms < 1.0  # 1ms ì´í•˜
    
    def test_session_creation_performance(self, media_session_manager):
        """ì„¸ì…˜ ìƒì„± ì„±ëŠ¥"""
        measurement = PerformanceMeasurement(mode="bypass")
        measurement.start()
        
        sdp = "v=0\r\no=- 1 1 IN IP4 192.168.1.100\r\ns=-\r\nc=IN IP4 192.168.1.100\r\nt=0 0\r\nm=audio 30000 RTP/AVP 0\r\n"
        
        # 100ê°œ ì„¸ì…˜ ìƒì„±
        for i in range(100):
            start = time.perf_counter()
            
            session = media_session_manager.create_session(f"call-perf-{i}", sdp)
            
            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            
            measurement.record_latency(latency_ms, 0)
        
        measurement.stop()
        
        stats = measurement.get_stats(concurrent_calls=100)
        
        print(f"\nğŸ” Session Creation Performance:")
        print(f"   Average: {stats.avg_latency_ms:.4f} ms")
        print(f"   P99: {stats.p99_latency_ms:.4f} ms")
        print(f"   Sessions/sec: {stats.packets_per_second:.2f}")
        
        # ì„¸ì…˜ ìƒì„±ì€ ë¹¨ë¼ì•¼ í•¨
        assert stats.avg_latency_ms < 50.0  # 50ms ì´í•˜
    
    @pytest.mark.skip(reason="ê¸´ ì‹¤í–‰ ì‹œê°„ì´ í•„ìš”í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸")
    def test_concurrent_calls_simulation(self, media_session_manager):
        """ë™ì‹œ í†µí™” ì‹œë®¬ë ˆì´ì…˜ (100 calls)"""
        measurement = PerformanceMeasurement(mode="bypass")
        measurement.start()
        
        sdp = "v=0\r\no=- 1 1 IN IP4 192.168.1.100\r\ns=-\r\nc=IN IP4 192.168.1.100\r\nt=0 0\r\nm=audio 30000 RTP/AVP 0\r\n"
        
        # 100ê°œ ì„¸ì…˜ ìƒì„±
        sessions = []
        for i in range(100):
            session = media_session_manager.create_session(f"call-load-{i}", sdp)
            sessions.append(session)
        
        # 10ì´ˆ ë™ì•ˆ RTP íŒ¨í‚· ì‹œë®¬ë ˆì´ì…˜ (ê° ì„¸ì…˜ì—ì„œ ì´ˆë‹¹ 50 íŒ¨í‚·)
        duration = 10.0
        packets_per_second = 50
        interval = 1.0 / packets_per_second
        
        start_time = time.time()
        packet_count = 0
        
        while time.time() - start_time < duration:
            for session in sessions:
                start = time.perf_counter()
                
                # RTP íŒ¨í‚· ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                packet = create_test_rtp_packet(sequence=packet_count)
                session.update_rtp_received(from_caller=True)
                
                end = time.perf_counter()
                latency_ms = (end - start) * 1000
                
                measurement.record_latency(latency_ms, 160)
                packet_count += 1
            
            time.sleep(interval)
        
        measurement.stop()
        
        stats = measurement.get_stats(concurrent_calls=100)
        
        print(f"\nğŸ” 100 Concurrent Calls Simulation:")
        print(f"   Duration: {stats.test_duration_seconds:.2f}s")
        print(f"   Total Packets: {stats.total_packets:,}")
        print(f"   Average Latency: {stats.avg_latency_ms:.2f} ms")
        print(f"   P99 Latency: {stats.p99_latency_ms:.2f} ms")
        print(f"   CPU: {stats.cpu_percent:.2f}%")
        print(f"   Memory: {stats.memory_mb:.2f} MB")
        
        # ì„±ëŠ¥ ëª©í‘œ
        assert stats.avg_latency_ms < 5.0
        assert stats.p99_latency_ms < 10.0
        assert stats.cpu_percent < 50.0
        assert stats.memory_mb < 2048.0


@pytest.mark.benchmark
class TestBypassModePerformance:
    """Bypass ëª¨ë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def test_bypass_mode_latency(self):
        """Bypass ëª¨ë“œ ì§€ì—° ì‹œê°„"""
        measurement = PerformanceMeasurement(mode="bypass")
        measurement.start()
        
        # 1000ê°œ íŒ¨í‚·ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
        for i in range(1000):
            start = time.perf_counter()
            
            # Bypass ëª¨ë“œ: ë‹¨ìˆœ relay ì‹œë®¬ë ˆì´ì…˜
            packet = create_test_rtp_packet(sequence=i)
            # ì‹¤ì œë¡œëŠ” UDP ì†Œì¼“ ì „ì†¡ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
            time.sleep(0.001)  # 1ms ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜
            
            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            
            measurement.record_latency(latency_ms, 160)
        
        measurement.stop()
        
        stats = measurement.get_stats(concurrent_calls=1)
        
        print(f"\nğŸ” Bypass Mode Performance:")
        print(f"   Average: {stats.avg_latency_ms:.2f} ms")
        print(f"   P99: {stats.p99_latency_ms:.2f} ms")
        print(f"   Throughput: {stats.packets_per_second:.2f} pps")
        
        print(f"\n   Performance Goals:")
        print(f"   âœ… Avg <= 5ms: {stats.avg_latency_ms:.2f} ms ({'PASS' if stats.avg_latency_ms <= 5.0 else 'FAIL'})")
        print(f"   âœ… P99 <= 10ms: {stats.p99_latency_ms:.2f} ms ({'PASS' if stats.p99_latency_ms <= 10.0 else 'FAIL'})")
        
        # Bypass ëª¨ë“œ ëª©í‘œ
        assert stats.avg_latency_ms < 5.0
        assert stats.p99_latency_ms < 10.0


@pytest.mark.benchmark
class TestReflectingModePerformance:
    """Reflecting ëª¨ë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_reflecting_mode_latency(self):
        """Reflecting ëª¨ë“œ ì§€ì—° ì‹œê°„"""
        measurement = PerformanceMeasurement(mode="reflecting")
        measurement.start()
        
        # ë¶„ì„ í ì‹œë®¬ë ˆì´ì…˜
        analysis_queue = asyncio.Queue(maxsize=100)
        
        # 1000ê°œ íŒ¨í‚·ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
        for i in range(1000):
            start = time.perf_counter()
            
            # Reflecting ëª¨ë“œ: relay + enqueue
            packet = create_test_rtp_packet(sequence=i)
            
            # Relay
            time.sleep(0.001)  # 1ms ë„¤íŠ¸ì›Œí¬ ì§€ì—°
            
            # Enqueue for analysis (non-blocking)
            try:
                analysis_queue.put_nowait(packet)
            except asyncio.QueueFull:
                measurement.record_packet_loss()
            
            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            
            measurement.record_latency(latency_ms, 160)
        
        measurement.stop()
        
        stats = measurement.get_stats(concurrent_calls=1)
        
        print(f"\nğŸ” Reflecting Mode Performance:")
        print(f"   Average: {stats.avg_latency_ms:.2f} ms")
        print(f"   P99: {stats.p99_latency_ms:.2f} ms")
        print(f"   Throughput: {stats.packets_per_second:.2f} pps")
        print(f"   Queue Size: {analysis_queue.qsize()}")
        
        print(f"\n   Performance Goals:")
        print(f"   âœ… Avg <= 15ms: {stats.avg_latency_ms:.2f} ms ({'PASS' if stats.avg_latency_ms <= 15.0 else 'FAIL'})")
        print(f"   âœ… P99 <= 30ms: {stats.p99_latency_ms:.2f} ms ({'PASS' if stats.p99_latency_ms <= 30.0 else 'FAIL'})")
        
        # Reflecting ëª¨ë“œ ëª©í‘œ
        assert stats.avg_latency_ms < 15.0
        assert stats.p99_latency_ms < 30.0


@pytest.mark.benchmark
class TestResourceUsage:
    """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  í…ŒìŠ¤íŠ¸"""
    
    def test_memory_usage_per_session(self, media_session_manager):
        """ì„¸ì…˜ë‹¹ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        import psutil
        process = psutil.Process()
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬
        initial_memory = process.memory_info().rss / (1024 * 1024)
        
        sdp = "v=0\r\no=- 1 1 IN IP4 192.168.1.100\r\ns=-\r\nc=IN IP4 192.168.1.100\r\nt=0 0\r\nm=audio 30000 RTP/AVP 0\r\n"
        
        # 100ê°œ ì„¸ì…˜ ìƒì„±
        for i in range(100):
            media_session_manager.create_session(f"call-mem-{i}", sdp)
        
        # ìµœì¢… ë©”ëª¨ë¦¬
        final_memory = process.memory_info().rss / (1024 * 1024)
        memory_per_session = (final_memory - initial_memory) / 100
        
        print(f"\nğŸ” Memory Usage:")
        print(f"   Initial: {initial_memory:.2f} MB")
        print(f"   Final (100 sessions): {final_memory:.2f} MB")
        print(f"   Per Session: {memory_per_session:.4f} MB")
        print(f"   Projected (1000 sessions): {memory_per_session * 1000:.2f} MB")
        
        # 100 ì„¸ì…˜ì—ì„œ 2GB ì´í•˜ (ë§¤ìš° ì¶©ë¶„í•œ ì—¬ìœ )
        assert final_memory < 2048.0

